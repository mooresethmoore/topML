{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import tcripser\n",
    "import gudhi,gudhi.hera,gudhi.wasserstein,persim\n",
    "import ase\n",
    "from ase.io import cube\n",
    "from ase.io import cif\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from typing import *\n",
    "import collections\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "inDir=\"Z:/data/diverse_metals\"\n",
    "df=pd.read_csv(f\"{inDir}/post-combustion-vsa-2-clean.csv\",index_col=0)\n",
    "phDF=pd.read_csv(f\"{inDir}/phDF_tThresh0_B1.csv\",index_col=0)\n",
    "imgIndexOrd=list(phDF.index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "newThresh=[-8,25]\n",
    "pBounds=[-25,50]\n",
    "\n",
    "truncCols=[i for i in phDF.columns[4:] if newThresh[0]<=int(i.split(\"_\")[0])<=newThresh[1] and newThresh[0]<=int(i.split(\"_\")[1])<=newThresh[1]]\n",
    "\n",
    "phDFSub=phDF[list(phDF.columns[:4])+list(truncCols)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CubePHDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, property_excel_dir: str, phDF_dir: str,cifs: List[str] = None ,newThresh: List[int]=None,elimDiag=True ):\n",
    "\n",
    "        if property_excel_dir[-3:]==\"csv\": #better for match: case if python > 3.10\n",
    "            propertyDF: pd.DataFrame = pd.read_csv(property_excel_dir,index_col=0)\n",
    "        elif property_excel_dir[-4:]==\"xlsx\":\n",
    "            propertyDF: pd.DataFrame = pd.read_excel(property_excel_dir,index_col=0)\n",
    "        if phDF_dir[-3:]==\"csv\":\n",
    "            phDF: pd.DataFrame = pd.read_csv(phDF_dir,index_col=0) #indexIsMOFs\n",
    "        elif phDF_dir[-4:]==\"xlsx\":\n",
    "            phDF: pd.DataFrame = pd.read_excel(phDF_dir,index_col=0) #indexIsMOFs\n",
    "        if cifs: ## You can either provide the general phDF + property_excel files, and a list of subset cifs\n",
    "            assert False not in {c in (set(phDF.index) & set(propertyDF.index)) for c in cifs}\n",
    "            self.cifs=cifs\n",
    "        else: ### or you can leave cifs as none and just take all overlapping MOFS in both input data files\n",
    "            self.cifs=list(set(phDF.index) & set(propertyDF.index)) #\n",
    "        self.property=torch.from_numpy((propertyDF.loc[self.cifs]).to_numpy(dtype=np.float32))\n",
    "        self.propCols=list(propertyDF.columns)\n",
    "\n",
    "        phDF=phDF.loc[self.cifs]\n",
    "\n",
    "        phVals={int(s[:s.find(\"_\")]) for s in phDF.columns if \"_\" in s}# assuming integer resolution for now\n",
    "        # this requires the columns to have the format b_d\n",
    "        phBounds=[min(phVals),max(phVals)]\n",
    "        self.phThresh=phBounds\n",
    "        if newThresh and len(newThresh)==2 and newThresh[0]>=self.phThresh[0] and  newThresh[1]<=self.phThresh[1] :\n",
    "            self.subsetPHImg(phDF,newThresh)\n",
    "        else:\n",
    "            self.subsetPHImg(phDF,self.phThresh)\n",
    "\n",
    "        if elimDiag:\n",
    "            self.elimDiag()\n",
    "\n",
    "\n",
    "\n",
    "    def subsetPHImg(self,phDF,newThresh: List[int]=[-8, 25],offset=4):\n",
    "\n",
    "        truncCols=[i for i in phDF.columns[offset:] if newThresh[0]<=int(i.split(\"_\")[0])<=newThresh[1] and newThresh[0]<=int(i.split(\"_\")[1])<=newThresh[1]]\n",
    "        phDFSub=phDF[list(phDF.columns[:offset])+list(truncCols)]\n",
    "        life=newThresh[1]-newThresh[0]\n",
    "\n",
    "        phImg=np.zeros((len(self.cifs),life+1,life+1))#,dtype='uint8')\n",
    "\n",
    "        for m in range(len(self.cifs)):\n",
    "            for b in np.arange(newThresh[0],newThresh[1]+1):\n",
    "                for d in np.arange(b,newThresh[1]+1):\n",
    "                    phImg[m,newThresh[1]-d,b-newThresh[0]]=phDFSub[f\"{b}_{d}\"].loc[self.cifs[m]]\n",
    "        self.phThresh=newThresh\n",
    "        self.phImg=torch.from_numpy(phImg).type(torch.float32)\n",
    "\n",
    "\n",
    "    def elimDiag(self): #allow persist thresh in future but really fast implementation for one pixel\n",
    "        sqlen=len(self.phImg[0])\n",
    "        imgClone=self.phImg.clone()\n",
    "        yrange=torch.arange(sqlen)\n",
    "        imgClone[:,yrange.__reversed__(),yrange]=torch.tensor(0).to(imgClone)\n",
    "        self.phImg=imgClone\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"cif\":self.cifs[index],\n",
    "                \"image\": self.phImg[index],\n",
    "                \"workcap\": self.property[index][0].view(1,),\n",
    "                \"sel\": self.property[index][1].view(1,),\n",
    "                }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mCubePHDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproperty_excel_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43minDir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/post-combustion-vsa-2-clean.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mphDF_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43minDir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/phDF_tThresh0_B1.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mCubePHDataset.__init__\u001B[1;34m(self, property_excel_dir, phDF_dir, cifs, newThresh, elimDiag)\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubsetPHImg(phDF,newThresh)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 21\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubsetPHImg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mphDF\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mphThresh\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m elimDiag:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39melimDiag()\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mCubePHDataset.subsetPHImg\u001B[1;34m(self, phDF, newThresh, offset)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcifs)):\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39marange(newThresh[\u001B[38;5;241m0\u001B[39m],newThresh[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 38\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnewThresh\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     39\u001B[0m             phImg[m,newThresh[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m-\u001B[39md,b\u001B[38;5;241m-\u001B[39mnewThresh[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m=\u001B[39mphDFSub[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mb\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00md\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcifs[m]]\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphThresh\u001B[38;5;241m=\u001B[39mnewThresh\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "dataset = CubePHDataset(property_excel_dir=f\"{inDir}/post-combustion-vsa-2-clean.csv\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,25])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "MLinDir=f\"{inDir}/dataSplitML\"\n",
    "optVar=\"sel\"\n",
    "\n",
    "trainDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/trainCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "{'cif': 'DB12-DADLIG_freeONLY',\n 'image': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n 'workcap': tensor([1.1375]),\n 'sel': tensor([278.5536])}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[5000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1a349f2a370>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainDataset[5000]['image'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 32])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[5000]['image'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    #print(batch)\n",
    "    #pd, image, cif = batch['pd'], batch['image'], batch['cif']\n",
    "    elem = batch[0]\n",
    "    assert isinstance(elem, collections.abc.Mapping)\n",
    "    #print(type(batch))\n",
    "\n",
    "    mapped_dict = {key: torch.stack([d[key] for d in batch], dim=0) for key in elem if key in ['pd', 'image', 'workcap', 'sel'] }\n",
    "    mapped_dict.update({key: [d[key] for d in batch] for key in elem if key not in ['pd', 'image', 'workcap', 'sel']})\n",
    "    #print(mapped_dict.keys())\n",
    "\n",
    "    image, cif, workcap, sel = mapped_dict['image'], mapped_dict['cif'], mapped_dict['workcap'], mapped_dict['sel']\n",
    "\n",
    "    return dict(image=image[:,None,...], cif=cif, workcap=workcap, sel=sel)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "MLinDir=f\"{inDir}/dataSplitML\"\n",
    "optVar=\"sel\"\n",
    "\n",
    "trainDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/trainCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainDataset, batch_size=2048, pin_memory=True, num_workers=0, shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "testDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/testCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "test_dataloader= torch.utils.data.DataLoader(testDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n",
    "\n",
    "validDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/valCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "valid_dataloader= torch.utils.data.DataLoader(validDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "{'cif': ['DB0-m3_o16_o43_f0_fsc.sym.17',\n  'DB5-hypotheticalMOF_35577_0_0_2_14_2_10',\n  'DB0-m3_o10_o16_f0_pcu.sym.3',\n  'DB15-pcu_N123_E156_opt',\n  'DB0-m23_o151_o153_f0_pcu',\n  'DB14-vapfup_P1_H',\n  'DB0-m3_o16_o22_f0_pcu.sym.122',\n  'DB0-m2_o10_o25_f0_pcu.sym.93',\n  'DB0-m3_o10_o29_f0_pcu.sym.40',\n  'DB12-JEJKEP_clean',\n  'DB12-EXOZEX_freeONLY',\n  'DB5-hypotheticalMOF_5072577_1_0_2_19_5_6',\n  'DB0-m2_o11_o12_f0_pcu.sym.73',\n  'DB7-ddmof_21169',\n  'DB0-m3_o14_o27_f0_pcu.sym.104',\n  'DB12-EPEXII_freeONLY',\n  'DB12-HALVAU_clean',\n  'DB0-m3_o24_o520_f0_fsc.sym.40',\n  'DB0-m3_o12_o25_f0_pcu.sym.46',\n  'DB12-HEMTEZ_clean',\n  'DB0-m3_o12_o26_f0_pcu.sym.76',\n  'DB1-Zn2O8N2-AZO_A-irmof10_A_No325',\n  'DB0-m3_o12_o23_f0_pcu.sym.129',\n  'DB0-m2_o12_o20_f0_pcu.sym.24',\n  'DB12-EBATUY_freeONLY',\n  'DB15-pcu_N123_E158_opt',\n  'DB0-m3_o8_o21_f0_pcu.sym.26',\n  'DB12-WOJJOV_SL',\n  'DB5-hypotheticalMOF_3002082_0_0_2_34_34_7',\n  'DB5-hypotheticalMOF_7548_0_0_0_11_4_6',\n  'DB12-BEPNUH_freeONLY',\n  'DB12-KULMIP_clean',\n  'DB15-pcu_N123_E175_opt',\n  'DB0-m2_o11_o21_f0_pcu.sym.96',\n  'DB12-TADCAE_clean',\n  'DB0-m23_o20_o21_f0_pcu',\n  'DB0-m2_o12_o22_f0_pcu.sym.6',\n  'DB12-LAGNIS_clean',\n  'DB0-m3_o24_o47_f0_fsc.sym.18',\n  'DB0-m3_o12_o17_f0_pcu.sym.33',\n  'DB5-hypotheticalMOF_5039254_2_0_1_15_25_6',\n  'DB12-MAWVAK_clean',\n  'DB0-m3_o13_o20_f0_pcu.sym.22',\n  'DB0-m3_o11_o17_f0_pcu.sym.20',\n  'DB0-m3_o12_o25_f0_pcu.sym.10',\n  'DB0-m3_o7_o19_f0_pcu.sym.79',\n  'DB5-hypotheticalMOF_3629_1_1_0_24_2_10',\n  'DB14-icizol_P1_H',\n  'DB0-m2_o8_o20_f0_pcu.sym.102',\n  'DB15-pcu_N65_E166_opt',\n  'DB12-UKUCAF_freeONLY',\n  'DB0-m1_o40_soc_0.116666666667_NMe2',\n  'DB0-m3_o96_o22_f0_fsc.sym.59',\n  'DB0-m2_o22_o25_f0_pcu.sym.1',\n  'DB12-TURDOB_clean',\n  'DB12-YOTSAD_clean',\n  'DB0-m3_o8_o16_f0_pcu.sym.24',\n  'DB12-MOQSIV_freeONLY',\n  'DB6-cuf_2596',\n  'DB12-PUGGOP_clean',\n  'DB15-pcu_N123_E137_opt',\n  'DB0-m2_o7_o22_f0_pcu.sym.34',\n  'DB5-hypotheticalMOF_32834_0_0_2_0_26_7',\n  'DB12-HOXSUK_clean',\n  'DB0-m3_o11_o24_f0_pcu.sym.74',\n  'DB0-m3_o14_o23_f0_pcu.sym.64',\n  'DB0-m3_o10_o23_f0_pcu.sym.63',\n  'DB0-m3_o152_o23_f0_fsc.sym.93',\n  'DB12-DOYBUQ01_clean',\n  'DB0-m3_o4_o45_f0_fsc.sym.18',\n  'DB12-BETGUE_clean',\n  'DB7-ddmof_21329',\n  'DB12-XINWUO_clean',\n  'DB0-m3_o10_o16_f0_pcu.sym.2',\n  'DB7-ddmof_20995',\n  'DB12-VIRVEY_clean',\n  'DB5-hypotheticalMOF_5074530_0_0_2_23_11_14',\n  'DB0-m3_o10_o25_f0_pcu.sym.16',\n  'DB0-m3_o14_o15_f0_pcu.sym.18',\n  'DB0-m2_o7_o17_f0_pcu.sym.29',\n  'DB5-hypotheticalMOF_5047746_0_0_1_19_14_4',\n  'DB0-m2_o14_o29_f0_pcu.sym.53',\n  'DB0-m3_o43_o22_f0_fsc.sym.20',\n  'DB12-DACXOV_clean',\n  'DB0-m3_o24_o151_f0_fsc.sym.20',\n  'DB5-hypotheticalMOF_5039966_3_3_1_15_15_6',\n  'DB12-REYMOZ_freeONLY',\n  'DB12-MAKXAZ_clean',\n  'DB15-kag_N65_E182_opt',\n  'DB0-m2_o1_o22_f0_pcu.sym.1',\n  'DB7-ddmof_21548',\n  'DB0-m3_o1480_o6_f0_fsc.sym.88',\n  'DB0-m3_o8_o28_f0_pcu.sym.100',\n  'DB5-hypotheticalMOF_10980_2_2_0_6_10_0',\n  'DB12-DEYXIQ_freeONLY',\n  'DB0-m3_o1480_o1_f0_fsc.sym.16',\n  'DB12-IROROY_clean',\n  'DB15-pcu_N37_E223_opt',\n  'DB12-IPIJEY_freeONLY',\n  'DB12-QUSSII_freeONLY',\n  'DB5-hypotheticalMOF_5018937_3_2_0_19_18_8',\n  'DB5-hypotheticalMOF_5069987_0_0_2_3_1_5',\n  'DB0-m23_o25_o153_f0_pcu',\n  'DB0-m2_o12_o20_f0_pcu.sym.87',\n  'DB0-m2_o24_o28_f0_pcu.sym.86',\n  'DB5-hypotheticalMOF_5074397_0_0_2_23_1_6',\n  'DB12-LASWIN_clean',\n  'DB5-hypotheticalMOF_5024150_0_0_0_27_10_8',\n  'DB0-m2_o11_o29_f0_pcu.sym.13',\n  'DB5-hypotheticalMOF_5062346_2_0_2_18_11_5',\n  'DB0-m23_o19_o23_f0_pcu',\n  'DB0-m2_o8_o22_f0_pcu.sym.112',\n  'DB0-m3_o1_o27_f0_pcu.sym.7',\n  'DB5-hypotheticalMOF_5066862_0_0_2_8_11_7',\n  'DB0-m2_o24_o29_f0_pcu.sym.58',\n  'DB0-m2_o24_o29_f0_pcu.sym.32',\n  'DB0-m3_o16_o151_f0_fsc.sym.53',\n  'DB1-ZIFZn-BDC_A-irmof6_A_No292',\n  'DB0-m3_o440_o22_f0_fsc.sym.17',\n  'DB12-VEDFAN_clean',\n  'DB12-ZEZQEC_clean',\n  'DB0-m3_o12_o14_f0_pcu.sym.115',\n  'DB0-m3_o16_o20_f0_pcu.sym.65',\n  'DB0-m15_o11_o139_f0_fsc',\n  'DB0-m1_o32_o1_rna',\n  'DB12-BEXRUT_freeONLY',\n  'DB12-ALUKAU_clean',\n  'DB0-m2_o12_o21_f0_pcu.sym.95',\n  'DB6-cuf_4393',\n  'DB6-cuf_7405',\n  'DB12-BIYSUZ_clean',\n  'DB12-TONLOZ_clean',\n  'DB12-BEXSII_clean',\n  'DB12-XAFXOT_clean',\n  'DB0-m3_o11_o19_f0_pcu.sym.119',\n  'DB12-UXEQOE_clean',\n  'DB5-hypotheticalMOF_5067066_0_0_2_8_14_4',\n  'DB5-hypotheticalMOF_3000997_1_1_1_34_34_2',\n  'DB0-m3_o24_o27_f0_pcu.sym.30',\n  'DB5-hypotheticalMOF_35409_0_0_2_14_9_6',\n  'DB1-Zn2O8N2-irmof14_A-irmof7_A_No251',\n  'DB0-m3_o10_o18_f0_pcu.sym.14',\n  'DB0-m23_o14_o29_f0_pcu',\n  'DB5-hypotheticalMOF_5068376_0_0_2_20_11_4',\n  'DB12-KEGZOL02_freeONLY',\n  'DB0-m3_o18_o1490_f0_fsc.sym.33',\n  'DB5-hypotheticalMOF_5007141_0_0_0_15_4_5',\n  'DB0-m3_o82_o159_f0_fsc.sym.10',\n  'DB6-cuf_4501',\n  'DB12-NEVVAM01_freeONLY',\n  'DB5-hypotheticalMOF_5063024_0_0_2_18_14_6',\n  'DB0-m3_o8_o21_f0_pcu.sym.20',\n  'DB0-m3_o12_o24_f0_pcu.sym.110',\n  'DB0-m3_o152_o17_f0_fsc.sym.10',\n  'DB7-ddmof_22870',\n  'DB1-Cu2O8N2-irmof6_A-irmof8_A_No80',\n  'DB0-m3_o24_o42_f0_fsc.sym.29',\n  'DB0-m3_o24_o440_f0_fsc.sym.13',\n  'DB12-REWSUJ_clean',\n  'DB13-cds-Syn039803',\n  'DB1-Cu2O8N2-irmof6_A-irmof8_A_No40',\n  'DB5-hypotheticalMOF_1001177_0_0_3_10_10_12',\n  'DB0-m3_o520_o156_f0_fsc.sym.81',\n  'DB1-Cu2O8N2-irmof16_A-pmof1_A_No24',\n  'DB0-m2_o12_o13_f0_pcu.sym.64',\n  'DB0-m2_o22_o25_f0_pcu',\n  'DB0-m2_o1_o18_f0_pcu.sym.12',\n  'DB0-m3_o18_o146_f0_fsc.sym.19',\n  'DB0-m3_o16_o22_f0_pcu.sym.26',\n  'DB12-CATART03_clean',\n  'DB0-m3_o22_o47_f0_fsc.sym.28',\n  'DB12-CAMROA_clean',\n  'DB12-OCUGEZ01_freeONLY',\n  'DB15-sxc_N65_E175_opt',\n  'DB0-m9_o11_o12_f0_sra.sym.25',\n  'DB0-m2_o11_o27_f0_pcu.sym.82',\n  'DB5-hypotheticalMOF_5035795_3_3_1_18_17_0',\n  'DB5-hypotheticalMOF_5046209_1_1_1_19_9_6',\n  'DB15-pcu_N123_E218_unopt',\n  'DB12-VAKBIU_clean',\n  'DB12-SIWSUO_clean',\n  'DB12-LACGED_clean',\n  'DB0-m2_o11_o13_f0_pcu.sym.111',\n  'DB0-m2_o18_o19_f0_pcu.sym.41',\n  'DB5-hypotheticalMOF_6002768_0_0_3_3_17_12',\n  'DB12-XEHSIN_freeONLY',\n  'DB0-m3_o90_o22_f0_fsc.sym.19',\n  'DB12-VAJQOP_clean',\n  'DB12-HEDCAW_clean',\n  'DB12-WOXKOL_clean',\n  'DB0-m2_o10_o23_f0_pcu.sym.7',\n  'DB5-hypotheticalMOF_5066852_1_0_2_8_11_3',\n  'DB7-ddmof_232',\n  'DB12-PIKBUH_clean',\n  'DB5-hypotheticalMOF_5079875_0_0_2_28_16_8',\n  'DB0-m3_o14_o25_f0_pcu.sym.28',\n  'DB0-m9_o8_o10_f0_sra.sym.31',\n  'DB0-m18_o4_o20_f0_pcu',\n  'DB0-m3_o11_o20_f0_pcu.sym.11',\n  'DB12-QUJFUX_clean',\n  'DB15-qtz_N76_E44_opt',\n  'DB0-m2_o7_o14_f0_pcu.sym.23',\n  'DB5-hypotheticalMOF_5074552_0_0_2_23_11_8',\n  'DB5-hypotheticalMOF_5075481_0_0_2_23_3_10',\n  'DB15-cds_N29_E101_opt',\n  'DB5-hypotheticalMOF_3001075_1_0_1_34_34_11',\n  'DB12-LIXDAZ_clean',\n  'DB15-qtz-x_N65_E108_opt',\n  'DB0-m3_o1_o44_f0_fsc.sym.12',\n  'DB1-Zn2O8N2-ADC_A-AZO_A_No224',\n  'DB5-hypotheticalMOF_5072101_1_1_2_19_10_5',\n  'DB0-m2_o22_o25_f0_pcu.sym.91',\n  'DB0-m3_o8_o23_f0_pcu.sym.90',\n  'DB0-m3_o24_o28_f0_pcu.sym.74',\n  'DB0-m3_o48_o155_f0_fsc.sym.11',\n  'DB0-m3_o10_o27_f0_pcu.sym.68',\n  'DB0-m2_o12_o25_f0_pcu.sym.70',\n  'DB5-hypotheticalMOF_5059985_0_0_2_12_10_11',\n  'DB12-MACHIJ_freeONLY',\n  'DB0-m3_o17_o114_f0_fsc.sym.53',\n  'DB12-FUWYAY_clean',\n  'DB12-HUNCUQ_clean',\n  'DB12-NAQQED_clean',\n  'DB12-GAMTIY_clean',\n  'DB7-ddmof_3855',\n  'DB5-hypotheticalMOF_5035479_0_0_1_12_14_12',\n  'DB12-RUGKOV_clean',\n  'DB0-m3_o11_o21_f0_pcu.sym.119',\n  'DB0-m2_o10_o20_f0_pcu.sym.11',\n  'DB5-hypotheticalMOF_3002045_1_0_2_34_34_0',\n  'DB0-m3_o16_o1530_f0_fsc.sym.18',\n  'DB12-ZEDRAD_clean',\n  'DB12-OMIPAD_freeONLY',\n  'DB7-ddmof_16210',\n  'DB0-m9_o4_o12_f0_sra.sym.58',\n  'DB0-m3_o2_o19_f0_pcu.sym.18',\n  'DB15-pcu_N123_E141_opt',\n  'DB0-m3_o10_o48_f0_fsc',\n  'DB0-m3_o22_o58_dme',\n  'DB12-TISPAO_clean',\n  'DB5-hypotheticalMOF_5037133_3_3_1_18_10_8',\n  'DB0-m3_o12_o22_f0_pcu.sym.25',\n  'DB5-hypotheticalMOF_5041466_1_0_1_8_21_3',\n  'DB0-m2_o11_o12_f0_pcu.sym.18',\n  'DB0-m3_o11_o20_f0_pcu.sym.54',\n  'DB0-m3_o14_o151_f0_fsc.sym.43',\n  'DB0-m2_o22_o22_f0_pcu.sym.33',\n  'DB0-m3_o3_o21_f0_pcu.sym.13',\n  'DB12-WIKXAQ_clean',\n  'DB0-m2_o18_o23_f0_pcu.sym.63',\n  'DB7-ddmof_21701',\n  'DB12-LULXAS_clean',\n  'DB0-m2_o11_o20_f0_pcu.sym.81',\n  'DB12-WIHWAN_freeONLY',\n  'DB7-ddmof_15327',\n  'DB0-m2_o8_o10_f0_pcu.sym.42',\n  'DB5-hypotheticalMOF_5036826_1_0_1_18_24_8',\n  'DB0-m3_o14_o25_f0_pcu.sym.113',\n  'DB12-LIBQIY_clean',\n  'DB12-SEQTEP_clean',\n  'DB5-hypotheticalMOF_32860_0_0_2_0_7_7',\n  'DB12-HIHGUC_clean',\n  'DB12-ZAVQAQ_clean',\n  'DB12-PELQII_clean',\n  'DB0-m3_o145_o22_f0_fsc.sym.4',\n  'DB0-m2_o10_o19_f0_pcu.sym.30',\n  'DB7-ddmof_23072',\n  'DB12-EGAGEA_clean',\n  'DB12-DUJRAD_clean',\n  'DB0-m3_o22_uoa',\n  'DB0-m3_o12_o16_f0_pcu.sym.68',\n  'DB5-hypotheticalMOF_19519_2_1_1_24_25_2',\n  'DB12-ncomms7350-s2_freeONLY',\n  'DB0-m3_o11_o20_f0_pcu.sym.63',\n  'DB7-ddmof_21708',\n  'DB0-m3_o8_o23_f0_pcu.sym.155',\n  'DB0-m3_o10_o12_f0_pcu.sym.62',\n  'DB0-m3_o2_o5_f0_pcu.sym.41',\n  'DB12-LEMVOP_freeONLY',\n  'DB12-PUWCOZ_freeONLY',\n  'DB0-m2_o13_o22_f0_pcu.sym.58',\n  'DB0-m5_o12_o24_bcu_0.4_NH2',\n  'DB0-m3_o24_etb-e',\n  'DB12-GOLYUD_clean',\n  'DB0-m2_o11_o29_f0_pcu.sym.93',\n  'DB0-m18_o19_o147_f0_pcu',\n  'DB12-CIDKUX_freeONLY',\n  'DB0-m2_o14_o18_f0_pcu.sym.8',\n  'DB0-m2_o11_o17_f0_pcu.sym.128',\n  'DB0-m2_o11_o26_f0_pcu.sym.76',\n  'DB12-OTIPUE_freeONLY',\n  'DB5-hypotheticalMOF_33144_0_0_2_0_24_11',\n  'DB7-ddmof_21371',\n  'DB0-m2_o24_o28_f0_nbo.sym.1',\n  'DB12-KECRAL10_clean',\n  'DB5-hypotheticalMOF_18725_2_1_1_25_21_2',\n  'DB0-m3_o12_o27_f0_pcu.sym.23',\n  'DB12-CODKOX_clean',\n  'DB7-ddmof_23529',\n  'DB12-QISNEN_clean',\n  'DB0-m2_o11_o11_f0_pcu.sym.32',\n  'DB5-hypotheticalMOF_5037105_2_0_1_18_10_6',\n  'DB5-hypotheticalMOF_5040102_1_0_1_15_13_9',\n  'DB12-CAZFOA_clean',\n  'DB12-TARVOX_clean',\n  'DB15-cds_N87_E94_opt',\n  'DB0-m3_o160_o52_f0_fsc.sym.78',\n  'DB5-hypotheticalMOF_5058835_0_0_2_12_9_14',\n  'DB0-m2_o22_o26_f0_pcu.sym.36',\n  'DB0-m2_o12_o22_f0_pcu.sym.57',\n  'DB0-m3_o10_o17_f0_pcu.sym.6',\n  'DB12-MOYNIY_clean',\n  'DB7-ddmof_23729',\n  'DB12-ALUKIC_freeONLY',\n  'DB0-m3_o11_o19_f0_pcu.sym.96',\n  'DB0-m3_o1480_o155_f0_fsc.sym.50',\n  'DB0-m2_o11_o20_f0_pcu.sym.56',\n  'DB0-m3_o8_o470_fsc_0.285714285714_SO3H',\n  'DB5-hypotheticalMOF_17327_0_0_1_7_7_5',\n  'DB0-m3_o540_o22_f0_fsc.sym.43',\n  'DB0-m3_o99_o22_f0_fsc.sym.43',\n  'DB12-YAVSOG_clean',\n  'DB12-APAYIB_clean',\n  'DB0-m2_o24_o28_f0_pcu.sym.92',\n  'DB0-m2_o11_o27_f0_pcu.sym.36',\n  'DB12-ASALOV_clean',\n  'DB0-m2_o22_o25_f0_pcu.sym.9',\n  'DB12-FODCEI_freeONLY',\n  'DB6-cuf_8268',\n  'DB0-m3_o16_o520_f0_fsc.sym.9',\n  'DB12-PIQFIG_freeONLY',\n  'DB12-LADQIT_clean',\n  'DB0-m3_o12_o24_f0_pcu.sym.38',\n  'DB15-sxc_N65_E41_opt',\n  'DB0-m9_o3_o20_f0_sra.sym.24',\n  'DB5-hypotheticalMOF_5039722_0_0_1_15_11_7',\n  'DB5-hypotheticalMOF_28962_0_0_2_25_17_14',\n  'DB0-m2_o8_o16_f0_pcu.sym.92',\n  'DB0-m3_o540_o22_f0_fsc.sym.20',\n  'DB12-PAMGIV_clean',\n  'DB5-hypotheticalMOF_5072650_0_0_2_19_14_8',\n  'DB12-LUJPUC_clean',\n  'DB0-m3_o14_o19_f0_pcu.sym.48',\n  'DB5-hypotheticalMOF_12887_1_1_0_14_2_7',\n  'DB12-ETESIG_clean',\n  'DB5-hypotheticalMOF_5043369_1_0_1_20_11_3',\n  'DB7-ddmof_7177',\n  'DB12-YUZRES_clean',\n  'DB0-m2_o17_o29_f0_pcu.sym.63',\n  'DB12-JOVXEZ_clean',\n  'DB7-ddmof_21096',\n  'DB12-IPIHIA_clean',\n  'DB0-m3_o1_o8_f0_pcu.sym.110',\n  'DB0-m3_o10_o25_f0_pcu.sym.23',\n  'DB12-BELYIC_clean',\n  'DB0-m3_o16_o45_f0_fsc.sym.4',\n  'DB0-m3_o7_o21_f0_pcu.sym.80',\n  'DB0-m9_o2_o11_f0_sra.sym.43',\n  'DB12-FONPUV_freeONLY',\n  'DB0-m2_o11_o12_f0_pcu.sym.35',\n  'DB0-m3_o16_o25_f0_pcu.sym.88',\n  'DB7-ddmof_23029',\n  'DB12-LONVAN_clean',\n  'DB0-m3_o16_o27_f0_pcu.sym.65',\n  'DB0-m3_o22_o27_f0_pcu.sym.31',\n  'DB0-m3_o11_o20_f0_pcu.sym.60',\n  'DB0-m2_o11_o12_f0_pcu.sym.36',\n  'DB12-OQOXAV_clean',\n  'DB5-hypotheticalMOF_5046108_0_0_1_3_3_10',\n  'DB12-TUTHIA_clean',\n  'DB15-qzd_N87_E181_opt',\n  'DB0-m3_o150_o22_f0_fsc.sym.20',\n  'DB0-m3_o12_o14_f0_pcu.sym.2',\n  'DB0-m3_o12_o27_f0_pcu.sym.22',\n  'DB12-MIMKIE_clean',\n  'DB5-hypotheticalMOF_27070_0_0_1_22_11_5',\n  'DB5-hypotheticalMOF_5037916_1_1_1_18_14_5',\n  'DB0-m2_o8_o22_f0_pcu.sym.31',\n  'DB7-ddmof_5422',\n  'DB12-OFEHAK_clean',\n  'DB0-m2_o9_o17_f0_pcu.sym.27',\n  'DB12-OKITAE_clean',\n  'DB0-m3_o24_o145_f0_fsc.sym.5',\n  'DB0-m15_o24_o146_f0_fsc',\n  'DB0-m3_o24_o146_f0_fsc.sym.38',\n  'DB0-m2_o11_o17_f0_pcu.sym.46',\n  'DB0-m2_o12_o19_f0_pcu.sym.8',\n  'DB5-hypotheticalMOF_5040398_0_0_1_15_16_8',\n  'DB15-qtz-x_N65_E37_opt',\n  'DB0-m3_o153_o155_f0_fsc.sym.13',\n  'DB12-QIWPET_freeONLY',\n  'DB0-m3_o24_o52_f0_fsc.sym.64',\n  'DB12-CUYWUP_freeONLY',\n  'DB15-qtz-e_N123_E27_opt',\n  'DB0-m3_o153_o158_f0_fsc.sym.22',\n  'DB5-hypotheticalMOF_5067433_0_0_2_8_15_7',\n  'DB0-m9_o1_o22_f0_sra.sym.86',\n  'DB5-hypotheticalMOF_5049807_0_0_1_23_13_14',\n  'DB0-m3_o160_o480_f0_fsc.sym.53',\n  'DB12-ADAXEK_freeONLY',\n  'DB0-m2_o17_o22_f0_pcu.sym.56',\n  'DB0-m2_o10_o23_f0_pcu.sym.31',\n  'DB0-m2_o1_o6_f0_pcu.sym.22',\n  'DB0-m3_o24_o112_f0_fsc.sym.12',\n  'DB5-hypotheticalMOF_5037225_1_1_1_18_11_10',\n  'DB0-m2_o7_o22_f0_pcu.sym.89',\n  'DB0-m3_o24_o51_f0_fsc.sym.17',\n  'DB0-m3_o13_o21_f0_pcu.sym.122',\n  'DB12-AHOKIR01_clean',\n  'DB12-BUGSEC_freeONLY',\n  'DB0-m2_o11_o20_f0_pcu.sym.39',\n  'DB0-m2_o15_o21_f0_pcu.sym.14',\n  'DB12-XOCGUR_clean',\n  'DB12-CECVOW_clean',\n  'DB12-PAJSAX_freeONLY',\n  'DB12-QOXNIB_freeONLY',\n  'DB5-hypotheticalMOF_5060427_3_0_2_12_13_5',\n  'DB14-sevwic_P1_H',\n  'DB0-m23_o1_o153_f0_pcu',\n  'DB0-m3_o14_o25_f0_pcu',\n  'DB5-hypotheticalMOF_5079400_0_0_2_28_11_5',\n  'DB0-m3_o24_o112_f0_fsc.sym.49',\n  'DB0-m3_o24_o1530_f0_fsc.sym.65',\n  'DB15-qtz_N76_E141_opt',\n  'DB12-EMAMOW_freeONLY',\n  'DB0-m3_o151_o15_f0_fsc.sym.30',\n  'DB12-GISWEM_freeONLY',\n  'DB0-m2_o11_o11_f0_pcu.sym.115',\n  'DB0-m2_o11_o29_f0_pcu.sym.80',\n  'DB0-m3_o8_o22_f0_pcu.sym.143',\n  'DB5-hypotheticalMOF_29173_1_0_2_25_2_3',\n  'DB0-m2_o10_o27_f0_pcu.sym.37',\n  'DB0-m9_o2_o14_f0_sra.sym.8',\n  'DB12-HIHGEM_manual',\n  'DB5-hypotheticalMOF_5064852_2_1_2_15_11_8',\n  'DB12-EXOZEX_clean',\n  'DB5-hypotheticalMOF_5065337_0_0_2_15_14_4',\n  'DB13-nbo-Syn039648',\n  'DB0-m2_o17_o21_f0_pcu.sym.65',\n  'DB12-EGATEM_freeONLY',\n  'DB12-HOWQEQ_clean',\n  'DB0-m2_o11_o24_f0_pcu.sym.24',\n  'DB12-VIZTIJ_clean',\n  'DB12-BUCYAA_clean',\n  'DB0-m3_o4_o28_f0_pcu.sym.28',\n  'DB12-CISXEJ_freeONLY',\n  'DB6-cuf_7173',\n  'DB0-m3_o10_o11_f0_pcu.sym.59',\n  'DB0-m2_o22_o25_f0_pcu.sym.23',\n  'DB0-m2_o12_o12_f0_pcu.sym.4',\n  'DB5-hypotheticalMOF_5041776_0_0_1_8_10_11',\n  'DB0-m2_o11_o11_f0_pcu.sym.21',\n  'DB12-CAKXIX_clean',\n  'DB5-hypotheticalMOF_27971_0_0_2_2_9_14',\n  'DB12-GAGXAP_clean',\n  'DB0-m2_o16_o29_f0_pcu.sym.13',\n  'DB0-m18_o25_o139_f0_fsc',\n  'DB0-m2_o12_o24_f0_pcu.sym.49',\n  'DB0-m3_o12_o17_f0_pcu.sym.36',\n  'DB0-m3_o24_o26_f0_pcu.sym.21',\n  'DB5-hypotheticalMOF_6606_2_2_0_10_10_6',\n  'DB1-Cu2O8N2-fum_A-pmof1_A_No7',\n  'DB0-m9_o2_o14_f0_sra.sym.11',\n  'DB0-m2_o24_o25_f0_pcu.sym.110',\n  'DB12-WEMFEB_freeONLY',\n  'DB0-m3_o16_o1520_f0_fsc.sym.16',\n  'DB0-m2_o14_o22_f0_pcu.sym.3',\n  'DB12-PUGFII_clean',\n  'DB1-Cu2O8N2-irmof6_A-irmof8_A_No104',\n  'DB12-AFOQUI_clean',\n  'DB0-m3_o24_o450_f0_fsc.sym.14',\n  'DB12-AWIBAL_freeONLY',\n  'DB0-m3_o2_o6_f0_pcu.sym.52',\n  'DB5-hypotheticalMOF_1002707_0_0_3_5_1_13',\n  'DB0-m3_o8_o24_f0_pcu.sym.51',\n  'DB5-hypotheticalMOF_6000570_0_0_3_12_13_8',\n  'DB12-RAVCOI_clean',\n  'DB12-XUMCAK_clean',\n  'DB0-m3_o13_o151_f0_fsc.sym.22',\n  'DB0-m23_o1_o8_f0_pcu',\n  'DB12-QQQGTA01_clean',\n  'DB0-m2_o12_o43_f0_fsc',\n  'DB15-pcu_N139_E36_opt',\n  'DB0-m3_o13_o23_f0_pcu.sym.154',\n  'DB12-LOSBOM_clean',\n  'DB5-hypotheticalMOF_5045396_0_0_1_3_5_5',\n  'DB5-hypotheticalMOF_5063172_0_0_2_18_22_7',\n  'DB0-m3_o8_o24_f0_pcu.sym.107',\n  'DB0-m23_o25_o27_f0_pcu',\n  'DB0-m2_o11_o14_f0_pcu.sym.14',\n  'DB0-m2_o8_o18_f0_pcu.sym.25',\n  'DB0-m3_o12_o17_f0_pcu.sym.37',\n  'DB0-m2_o10_o16_f0_pcu.sym.35',\n  'DB0-m9_o12_o2_f0_sra.sym.25',\n  'DB0-m2_o11_o12_f0_pcu.sym.10',\n  'DB12-BUGSIG_clean',\n  'DB12-LATPIG_clean',\n  'DB12-NIYZIG_freeONLY',\n  'DB0-m3_o12_o21_f0_pcu.sym.44',\n  'DB12-ISIKIF_clean',\n  'DB12-XANMIJ_freeONLY',\n  'DB0-m2_o10_o16_f0_pcu.sym.11',\n  'DB7-ddmof_16477',\n  'DB12-LACJAC_clean',\n  'DB0-m3_o12_o22_f0_pcu.sym.67',\n  'DB12-MOCJEU_clean',\n  'DB12-IJANAJ_clean',\n  'DB0-m2_o18_o24_f0_pcu.sym.14',\n  'DB12-QADKAH_clean',\n  'DB12-UCEXIK_clean',\n  'DB5-hypotheticalMOF_6002146_0_0_3_8_11_11',\n  'DB12-MOPQIT_freeONLY'],\n 'image': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         ...,\n \n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n \n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n 'workcap': tensor([[1.6175e+00],\n         [3.1101e+00],\n         [1.8102e+00],\n         [1.3986e+00],\n         [1.3362e+00],\n         [1.1039e-01],\n         [2.2248e+00],\n         [2.5121e+00],\n         [3.7412e+00],\n         [1.7594e+00],\n         [1.8339e+00],\n         [2.6345e+00],\n         [6.5510e-01],\n         [2.7896e-01],\n         [6.1165e-01],\n         [1.4988e+00],\n         [2.6175e+00],\n         [1.1398e+00],\n         [3.7567e-01],\n         [2.3025e+00],\n         [6.4942e-01],\n         [2.5943e-01],\n         [4.3778e-01],\n         [1.4940e-01],\n         [2.2832e+00],\n         [3.1393e+00],\n         [1.7823e-01],\n         [2.6182e+00],\n         [1.6238e-01],\n         [2.5848e+00],\n         [2.1413e-01],\n         [3.7343e-01],\n         [2.2598e+00],\n         [6.8711e-01],\n         [4.6871e-02],\n         [1.3997e-01],\n         [2.0542e+00],\n         [2.6944e+00],\n         [1.4936e+00],\n         [4.8443e-01],\n         [2.3509e-01],\n         [2.2524e+00],\n         [1.8566e-01],\n         [4.0137e-01],\n         [5.7771e-01],\n         [2.4725e-01],\n         [1.9865e+00],\n         [1.0075e+00],\n         [3.0169e-01],\n         [3.6477e+00],\n         [2.7395e+00],\n         [2.8699e+00],\n         [1.5062e+00],\n         [3.8324e-01],\n         [2.7480e+00],\n         [4.0903e-01],\n         [2.1918e-01],\n         [4.2451e-02],\n         [5.9700e-01],\n         [2.2720e+00],\n         [1.4339e+00],\n         [2.1582e-01],\n         [1.1669e+00],\n         [3.2460e+00],\n         [5.7266e-01],\n         [1.8195e+00],\n         [1.2840e+00],\n         [1.4967e+00],\n         [1.2868e+00],\n         [1.2214e+00],\n         [5.1361e-01],\n         [3.0725e-01],\n         [2.9137e-01],\n         [2.1330e+00],\n         [3.0241e-01],\n         [9.3751e-01],\n         [1.5321e+00],\n         [1.3694e+00],\n         [1.9859e+00],\n         [1.0282e-01],\n         [7.7118e-01],\n         [1.1283e+00],\n         [1.1360e+00],\n         [1.3047e+00],\n         [8.0767e-01],\n         [6.6891e-01],\n         [4.5763e-01],\n         [1.5071e+00],\n         [1.7396e+00],\n         [5.2011e-01],\n         [3.0282e-01],\n         [2.2264e+00],\n         [4.0650e-01],\n         [1.2229e+00],\n         [3.3185e-01],\n         [2.4975e+00],\n         [8.9695e-01],\n         [7.5340e-03],\n         [1.6351e+00],\n         [1.6108e+00],\n         [2.4593e-01],\n         [1.2853e-01],\n         [6.9448e-01],\n         [2.2462e-01],\n         [1.6841e+00],\n         [1.6988e+00],\n         [2.2778e+00],\n         [2.1371e-01],\n         [4.4093e-01],\n         [1.4664e-01],\n         [9.6743e-01],\n         [3.6597e-01],\n         [6.4018e-01],\n         [2.5756e+00],\n         [5.5637e-01],\n         [1.2066e+00],\n         [2.5073e+00],\n         [3.2473e+00],\n         [1.0347e+00],\n         [2.5354e+00],\n         [3.0420e+00],\n         [1.1701e+00],\n         [4.1279e-01],\n         [1.8228e+00],\n         [5.0190e-01],\n         [1.7504e+00],\n         [2.4985e-01],\n         [2.0886e-01],\n         [2.3487e-01],\n         [3.9002e-01],\n         [1.6271e+00],\n         [3.0268e+00],\n         [2.1274e+00],\n         [1.7548e-01],\n         [2.4151e-01],\n         [3.1715e-01],\n         [1.9828e+00],\n         [1.0130e+00],\n         [2.1903e-01],\n         [1.4499e+00],\n         [4.4811e+00],\n         [3.0204e+00],\n         [2.5728e+00],\n         [5.6924e-01],\n         [9.0659e-02],\n         [3.0450e+00],\n         [5.0960e-01],\n         [2.8509e+00],\n         [9.7003e-01],\n         [1.4361e+00],\n         [5.1602e-01],\n         [2.1008e-01],\n         [2.7246e+00],\n         [1.3209e+00],\n         [1.3924e-01],\n         [1.6214e+00],\n         [1.9006e+00],\n         [9.8260e-01],\n         [2.1701e+00],\n         [3.4469e-01],\n         [1.1825e+00],\n         [2.4325e+00],\n         [2.3717e+00],\n         [3.1781e-01],\n         [6.3883e-01],\n         [2.4589e-01],\n         [7.7644e-01],\n         [6.6615e-01],\n         [6.3092e-01],\n         [1.7543e+00],\n         [1.3369e+00],\n         [2.8732e+00],\n         [1.3437e+00],\n         [2.7202e+00],\n         [8.6913e-01],\n         [4.1492e-01],\n         [1.9344e+00],\n         [2.5244e+00],\n         [1.5097e+00],\n         [2.7448e+00],\n         [2.4319e+00],\n         [4.7563e-01],\n         [5.4343e-01],\n         [4.9757e-01],\n         [2.0757e+00],\n         [1.0010e+00],\n         [1.6225e+00],\n         [1.2030e-02],\n         [2.0433e+00],\n         [4.3452e-01],\n         [6.5480e-01],\n         [4.4763e-01],\n         [2.5525e-01],\n         [5.3219e-01],\n         [3.9566e-01],\n         [7.1204e-01],\n         [1.5748e+00],\n         [1.4523e-01],\n         [2.7166e-01],\n         [4.9997e-01],\n         [1.5225e-01],\n         [1.8003e-01],\n         [4.9344e-01],\n         [2.4568e+00],\n         [1.6118e-01],\n         [6.2807e-01],\n         [4.6543e-01],\n         [1.0407e+00],\n         [1.1760e+00],\n         [1.9964e+00],\n         [2.1845e+00],\n         [8.6082e-01],\n         [3.1067e-01],\n         [6.6427e-01],\n         [2.3006e+00],\n         [8.0243e-01],\n         [3.8959e-01],\n         [4.2870e-01],\n         [4.2041e-01],\n         [3.8793e+00],\n         [8.9726e-01],\n         [1.4531e+00],\n         [1.5531e+00],\n         [1.3520e+00],\n         [2.2159e-01],\n         [1.9437e+00],\n         [2.2945e+00],\n         [2.6205e-01],\n         [2.8653e-01],\n         [1.4432e-01],\n         [2.3014e+00],\n         [1.1780e-03],\n         [1.1487e+00],\n         [3.6607e-01],\n         [1.6148e+00],\n         [3.3710e-01],\n         [4.9516e-01],\n         [3.6258e+00],\n         [2.3075e-01],\n         [6.9313e-01],\n         [4.8862e-02],\n         [2.2738e-01],\n         [1.4611e-01],\n         [3.4400e-01],\n         [1.9033e-01],\n         [1.5529e+00],\n         [1.1291e+00],\n         [1.8146e-01],\n         [1.0256e-01],\n         [6.5312e-01],\n         [3.4377e-01],\n         [2.0792e+00],\n         [1.6965e-01],\n         [6.7921e-01],\n         [1.6947e-01],\n         [1.4179e+00],\n         [1.5492e-01],\n         [1.0256e+00],\n         [3.5824e+00],\n         [3.1274e+00],\n         [2.4506e-01],\n         [8.6304e-01],\n         [1.8303e+00],\n         [9.0206e-01],\n         [2.9532e+00],\n         [8.1591e-01],\n         [4.0826e-01],\n         [9.9223e-01],\n         [2.6632e+00],\n         [4.0097e-01],\n         [4.6225e-01],\n         [5.5110e-01],\n         [5.6394e-01],\n         [2.4584e-01],\n         [2.7142e-01],\n         [5.3167e-01],\n         [1.5774e+00],\n         [7.5931e-01],\n         [1.2234e+00],\n         [8.7497e-01],\n         [8.4966e-01],\n         [1.2576e+00],\n         [1.4843e+00],\n         [1.3768e+00],\n         [5.4438e-01],\n         [2.0972e-01],\n         [4.2168e-01],\n         [1.3547e+00],\n         [1.1540e+00],\n         [1.2533e+00],\n         [1.3619e+00],\n         [4.5609e-01],\n         [3.4612e-01],\n         [1.3622e-01],\n         [1.1085e-01],\n         [4.5374e-01],\n         [2.5945e-01],\n         [5.2121e-02],\n         [2.7654e-01],\n         [3.1437e+00],\n         [3.7018e-01],\n         [1.9841e-01],\n         [4.3336e-01],\n         [4.3369e-01],\n         [1.8205e+00],\n         [1.8219e-01],\n         [2.1084e+00],\n         [8.6013e-01],\n         [5.5418e-01],\n         [6.3092e-01],\n         [3.1823e+00],\n         [8.8805e-01],\n         [3.5963e-01],\n         [8.8554e-01],\n         [5.2069e-01],\n         [2.3676e+00],\n         [1.4998e-01],\n         [2.8271e+00],\n         [2.5796e+00],\n         [3.0553e+00],\n         [1.5182e+00],\n         [7.7303e-01],\n         [3.3765e+00],\n         [9.1449e-01],\n         [2.3768e-01],\n         [9.8204e-01],\n         [3.5328e-01],\n         [1.2523e-01],\n         [3.5024e-01],\n         [2.2726e+00],\n         [6.0544e-02],\n         [1.8751e+00],\n         [7.3005e-01],\n         [3.3862e-01],\n         [2.9476e-01],\n         [5.5432e-01],\n         [5.2101e-01],\n         [2.0612e-01],\n         [2.4074e+00],\n         [1.5715e+00],\n         [4.7104e-01],\n         [1.6814e+00],\n         [7.1189e-01],\n         [1.9766e+00],\n         [1.5260e+00],\n         [2.4666e-01],\n         [1.1535e-01],\n         [2.2303e+00],\n         [3.4066e-01],\n         [1.6324e+00],\n         [1.3106e-01],\n         [2.2732e+00],\n         [7.9554e-01],\n         [5.4531e-01],\n         [3.8145e+00],\n         [2.2600e+00],\n         [1.8829e-01],\n         [8.8053e-01],\n         [1.5730e+00],\n         [3.4945e-01],\n         [7.3501e-01],\n         [1.6535e-01],\n         [5.9223e-01],\n         [6.0988e-01],\n         [2.6218e-01],\n         [1.7584e-01],\n         [3.0992e-01],\n         [1.3934e+00],\n         [1.2527e+00],\n         [4.2477e+00],\n         [5.0755e-01],\n         [9.6966e-01],\n         [5.9444e-01],\n         [2.0642e-01],\n         [2.0898e+00],\n         [1.0680e+00],\n         [2.3509e+00],\n         [1.5820e-01],\n         [2.4185e-01],\n         [2.3850e+00],\n         [8.6446e-01],\n         [6.3590e-01],\n         [1.8269e+00],\n         [1.5865e+00],\n         [2.1818e+00],\n         [6.7045e-01],\n         [1.9022e-01],\n         [6.2670e-01],\n         [2.7042e+00],\n         [1.5115e+00],\n         [1.7550e+00],\n         [7.2283e-01],\n         [3.4871e+00],\n         [1.1333e+00],\n         [2.1130e+00],\n         [1.3597e+00],\n         [2.1875e+00],\n         [8.4344e-01],\n         [2.9774e+00],\n         [2.1192e+00],\n         [3.1349e-01],\n         [1.4781e+00],\n         [2.3289e-01],\n         [1.9767e+00],\n         [1.9951e+00],\n         [2.7703e-01],\n         [3.2577e-01],\n         [5.3775e-01],\n         [7.6679e-01],\n         [2.6416e-01],\n         [1.6620e-01],\n         [7.0970e-01],\n         [1.2368e+00],\n         [6.5401e-01],\n         [5.1015e-01],\n         [5.0159e-01],\n         [1.6086e-01],\n         [1.2287e+00],\n         [2.5741e+00],\n         [4.5947e-01],\n         [4.5373e-01],\n         [2.2868e+00],\n         [1.8861e+00],\n         [1.7797e-01],\n         [3.2579e-01],\n         [2.3803e+00],\n         [9.5740e-01],\n         [2.9515e-01],\n         [8.7950e-01],\n         [2.8870e-01],\n         [3.1678e-01],\n         [9.0715e-01],\n         [2.4998e+00],\n         [1.4408e+00],\n         [4.9539e-02],\n         [1.6130e+00],\n         [5.3908e-01],\n         [3.0324e-01],\n         [3.6823e-01],\n         [1.5253e+00],\n         [1.2159e+00],\n         [2.4291e-01],\n         [3.4728e-01],\n         [1.1018e+00],\n         [6.6174e-01],\n         [1.2813e+00],\n         [2.2179e-01],\n         [4.2186e-01],\n         [1.1723e-01],\n         [3.0839e-01],\n         [8.0545e-01],\n         [2.8245e-01],\n         [7.5187e-01],\n         [1.5616e+00],\n         [1.2524e+00],\n         [1.1307e+00],\n         [8.8429e-01],\n         [2.5352e-01],\n         [5.9373e-01],\n         [2.2394e-01],\n         [6.5368e-01],\n         [4.4392e+00],\n         [2.6298e+00],\n         [9.8615e-01],\n         [7.7316e-01],\n         [3.2225e+00],\n         [1.8909e+00],\n         [1.8278e+00],\n         [2.0592e+00],\n         [1.1381e+00],\n         [8.4689e-01],\n         [9.6796e-01],\n         [2.2729e-01],\n         [1.0901e-01],\n         [7.2349e-01],\n         [5.6158e-01],\n         [5.7116e-01],\n         [6.0482e-02],\n         [1.2932e+00],\n         [2.5113e+00],\n         [6.4603e-01],\n         [5.7610e-01],\n         [2.1804e-01],\n         [7.8296e-01],\n         [6.1782e+00],\n         [1.3836e+00],\n         [6.3282e-01],\n         [3.8325e-01],\n         [2.3816e-01],\n         [1.3397e+00],\n         [2.1967e-01],\n         [4.4704e-01],\n         [1.5884e+00],\n         [7.8900e-01],\n         [1.5216e-01],\n         [6.5426e-01],\n         [3.9430e-01],\n         [1.1957e+00],\n         [5.9736e-01],\n         [7.0843e-01],\n         [2.0067e+00],\n         [2.8997e+00],\n         [2.0224e-01],\n         [1.0235e+00],\n         [3.9950e-01],\n         [7.6430e-01],\n         [4.7614e-01],\n         [7.1730e-01],\n         [1.6634e+00],\n         [2.5938e+00],\n         [7.7643e-01],\n         [6.9666e-01]]),\n 'sel': tensor([[5.8085e+01],\n         [1.1817e+02],\n         [8.3470e+01],\n         [2.3802e+01],\n         [4.8432e+01],\n         [1.7968e+02],\n         [1.3782e+02],\n         [2.3761e+02],\n         [1.5910e+02],\n         [1.9647e+02],\n         [1.1086e+02],\n         [1.8459e+02],\n         [1.9257e+01],\n         [4.4947e+00],\n         [2.0450e+01],\n         [1.5487e+02],\n         [8.3392e+01],\n         [8.5655e+01],\n         [7.6235e+00],\n         [1.2357e+02],\n         [2.1194e+01],\n         [4.5965e+00],\n         [9.2828e+00],\n         [2.0044e+00],\n         [1.9702e+02],\n         [5.5034e+01],\n         [1.3526e+00],\n         [2.3449e+02],\n         [2.0181e+00],\n         [1.2179e+02],\n         [1.0775e+01],\n         [1.2444e+01],\n         [6.0558e+01],\n         [2.1071e+01],\n         [3.6125e+01],\n         [1.5177e+00],\n         [1.2554e+02],\n         [2.7071e+02],\n         [1.0231e+02],\n         [1.4184e+01],\n         [2.1361e+00],\n         [8.2614e+01],\n         [3.1382e+00],\n         [6.4631e+00],\n         [1.3519e+01],\n         [3.2953e+00],\n         [1.2116e+02],\n         [7.5696e+01],\n         [5.0213e+00],\n         [5.2206e+01],\n         [2.2195e+02],\n         [1.4661e+02],\n         [4.9965e+01],\n         [8.2098e+00],\n         [1.4370e+02],\n         [1.8603e+02],\n         [2.3454e+00],\n         [1.6150e+00],\n         [9.7156e+00],\n         [1.2508e+02],\n         [4.5277e+01],\n         [2.8171e+00],\n         [3.9996e+01],\n         [1.3818e+02],\n         [2.3125e+01],\n         [7.5341e+01],\n         [5.5122e+01],\n         [5.4422e+01],\n         [1.2801e+02],\n         [7.5545e+01],\n         [1.7626e+01],\n         [5.1296e+00],\n         [6.3357e+00],\n         [9.4945e+01],\n         [5.0941e+00],\n         [4.6905e+01],\n         [5.7364e+01],\n         [5.7182e+01],\n         [1.4831e+02],\n         [2.2327e+00],\n         [1.7410e+01],\n         [4.4724e+01],\n         [5.6470e+01],\n         [2.6927e+02],\n         [3.4174e+01],\n         [4.8392e+01],\n         [8.9130e+00],\n         [2.9652e+02],\n         [1.1707e+01],\n         [1.5407e+01],\n         [1.1187e+00],\n         [8.1691e+01],\n         [4.8138e+00],\n         [9.0752e+01],\n         [2.0680e+02],\n         [1.2197e+02],\n         [2.3101e+02],\n         [3.9605e+00],\n         [1.4893e+02],\n         [1.9401e+02],\n         [6.9426e+00],\n         [4.4599e+00],\n         [2.3428e+01],\n         [4.1799e+00],\n         [8.4299e+01],\n         [6.3297e+01],\n         [3.2660e+02],\n         [2.1813e+00],\n         [6.6524e+00],\n         [1.4426e+00],\n         [2.7909e+01],\n         [5.9307e+00],\n         [1.4814e+01],\n         [1.1597e+02],\n         [1.0121e+01],\n         [4.2312e+01],\n         [9.8887e+01],\n         [1.8486e+02],\n         [5.1724e+01],\n         [1.1255e+02],\n         [3.9933e+01],\n         [6.0402e+01],\n         [7.1692e+00],\n         [6.9542e+01],\n         [5.6926e+00],\n         [1.6341e+02],\n         [1.6520e+01],\n         [2.0319e+00],\n         [2.5757e+00],\n         [4.9456e+00],\n         [1.1328e+02],\n         [2.4484e+02],\n         [7.9077e+01],\n         [1.1565e+02],\n         [5.8277e+00],\n         [3.0905e+02],\n         [8.2342e+01],\n         [5.6292e+01],\n         [7.3652e+00],\n         [5.7562e+01],\n         [1.7504e+02],\n         [1.4593e+02],\n         [1.9327e+02],\n         [9.2579e+00],\n         [2.5142e+00],\n         [1.1234e+02],\n         [8.3990e+00],\n         [1.1317e+02],\n         [4.7681e+01],\n         [9.2543e+01],\n         [8.1964e+00],\n         [2.0008e+00],\n         [1.5683e+02],\n         [3.4668e+01],\n         [2.8236e+00],\n         [6.7605e+01],\n         [1.2974e+02],\n         [2.2778e+01],\n         [1.1260e+02],\n         [2.4147e+00],\n         [7.1588e+01],\n         [1.0610e+02],\n         [9.1994e+01],\n         [7.5421e+00],\n         [8.3787e+01],\n         [4.9726e+00],\n         [3.3296e+01],\n         [2.2090e+01],\n         [3.0617e+01],\n         [8.0189e+01],\n         [1.2045e+02],\n         [1.4897e+02],\n         [1.0777e+02],\n         [4.7102e+01],\n         [4.9160e+01],\n         [1.0499e+01],\n         [1.9646e+02],\n         [1.6050e+02],\n         [2.9342e+01],\n         [1.6629e+02],\n         [2.2366e+02],\n         [8.2390e+01],\n         [1.4328e+01],\n         [9.7024e+00],\n         [1.6298e+02],\n         [9.7989e+01],\n         [9.5610e+01],\n         [5.5450e-01],\n         [9.1364e+01],\n         [1.0102e+01],\n         [4.2732e+01],\n         [3.1446e+01],\n         [1.1593e+01],\n         [1.3174e+02],\n         [7.8423e+00],\n         [2.4993e+01],\n         [2.2834e+02],\n         [1.6729e+00],\n         [4.2995e+00],\n         [2.0342e+01],\n         [1.3453e+00],\n         [7.1825e+00],\n         [5.4042e+00],\n         [1.2006e+02],\n         [6.7367e-01],\n         [1.7642e+01],\n         [2.4446e+01],\n         [1.5938e+01],\n         [5.8052e+01],\n         [3.5688e+01],\n         [1.6167e+02],\n         [3.9781e+01],\n         [3.3085e+00],\n         [2.9620e+01],\n         [8.5968e+01],\n         [3.8651e+01],\n         [6.6399e+00],\n         [1.1402e+01],\n         [3.3007e+01],\n         [1.9327e+02],\n         [1.2772e+02],\n         [2.7705e+02],\n         [2.1241e+01],\n         [1.5345e+02],\n         [2.6401e+00],\n         [6.3572e+01],\n         [9.2835e+01],\n         [3.6147e+00],\n         [4.4633e+00],\n         [1.8460e+00],\n         [1.0958e+02],\n         [2.6935e+01],\n         [2.1729e+02],\n         [1.6000e+01],\n         [9.0177e+01],\n         [3.6843e+00],\n         [2.9371e+00],\n         [2.3499e+02],\n         [3.3152e+00],\n         [4.1699e+01],\n         [1.1669e+00],\n         [9.5552e+00],\n         [2.8497e+00],\n         [6.6096e+00],\n         [2.4037e+00],\n         [7.5521e+01],\n         [7.1887e+01],\n         [1.6414e+00],\n         [2.2059e+00],\n         [1.0652e+01],\n         [1.3537e+01],\n         [2.1708e+02],\n         [1.6320e+00],\n         [3.2955e+01],\n         [1.5472e+00],\n         [7.2447e+01],\n         [1.3338e+00],\n         [4.4409e+01],\n         [6.7204e+01],\n         [1.5964e+02],\n         [1.8831e+02],\n         [1.9990e+01],\n         [2.8145e+02],\n         [3.2371e+01],\n         [1.6055e+02],\n         [4.7703e+01],\n         [1.3237e+01],\n         [2.9958e+01],\n         [1.6594e+02],\n         [9.4490e+00],\n         [2.4765e+01],\n         [1.8503e+01],\n         [3.6390e+01],\n         [3.6947e+00],\n         [1.3973e+00],\n         [8.0007e+00],\n         [1.1336e+02],\n         [1.5836e+01],\n         [1.8312e+02],\n         [2.5532e+02],\n         [3.5250e+01],\n         [1.4569e+02],\n         [7.1938e+01],\n         [1.6197e+02],\n         [8.6241e+00],\n         [3.2082e+00],\n         [1.0785e+01],\n         [5.5758e+01],\n         [4.7177e+01],\n         [4.7403e+01],\n         [2.9836e+02],\n         [1.2195e+01],\n         [1.6972e+01],\n         [2.1480e+00],\n         [1.7760e+00],\n         [1.1834e+01],\n         [4.9278e+00],\n         [3.4604e+00],\n         [1.3975e+00],\n         [1.2067e+02],\n         [1.3253e+01],\n         [2.7192e+00],\n         [6.5894e+00],\n         [1.0038e+02],\n         [1.3059e+02],\n         [1.8636e+00],\n         [9.7223e+01],\n         [2.6843e+01],\n         [1.8371e+01],\n         [2.5770e+01],\n         [1.6909e+02],\n         [1.6203e+02],\n         [1.1554e+01],\n         [1.7050e+01],\n         [2.5748e+01],\n         [7.7281e+01],\n         [1.5423e+00],\n         [1.3305e+02],\n         [1.2806e+02],\n         [1.3872e+02],\n         [1.2986e+02],\n         [1.8895e+02],\n         [1.8207e+02],\n         [3.8859e+01],\n         [3.8400e+00],\n         [2.1923e+02],\n         [7.9939e+00],\n         [1.7169e+01],\n         [4.1049e+00],\n         [1.0423e+02],\n         [1.4595e+00],\n         [4.9682e+01],\n         [3.6704e+01],\n         [2.2985e+00],\n         [1.0593e+01],\n         [1.0638e+01],\n         [8.2627e+00],\n         [2.0664e+00],\n         [9.9343e+01],\n         [3.4389e+02],\n         [1.0892e+01],\n         [6.2646e+01],\n         [3.9255e+01],\n         [1.9636e+02],\n         [1.9550e+02],\n         [6.4346e+00],\n         [6.8496e+00],\n         [1.4319e+02],\n         [3.5362e+00],\n         [1.1067e+02],\n         [1.7791e+00],\n         [1.9433e+02],\n         [2.7451e+01],\n         [1.4462e+01],\n         [8.7886e+01],\n         [9.8956e+01],\n         [1.8555e+00],\n         [3.6196e+01],\n         [1.4565e+02],\n         [7.2540e+00],\n         [1.9342e+01],\n         [3.1170e+00],\n         [2.8945e+01],\n         [1.4412e+01],\n         [4.8474e+00],\n         [2.1861e+00],\n         [5.2603e+00],\n         [4.8223e+02],\n         [9.3345e+01],\n         [1.6571e+02],\n         [1.8221e+01],\n         [4.5388e+01],\n         [1.9631e+01],\n         [4.5684e+00],\n         [3.3751e+02],\n         [2.8571e+01],\n         [1.8256e+02],\n         [2.7369e+00],\n         [1.0866e+01],\n         [2.1556e+02],\n         [4.0738e+01],\n         [2.3146e+02],\n         [9.1885e+01],\n         [5.4780e+01],\n         [1.2193e+02],\n         [2.0649e+01],\n         [3.8379e+00],\n         [1.1805e+01],\n         [3.7045e+01],\n         [4.6726e+01],\n         [2.1618e+02],\n         [5.1725e+01],\n         [1.8353e+02],\n         [5.9215e+01],\n         [6.9545e+01],\n         [5.0432e+01],\n         [1.8762e+02],\n         [3.1900e+01],\n         [1.1828e+02],\n         [1.4053e+02],\n         [1.0837e+01],\n         [6.5309e+01],\n         [8.5808e+00],\n         [1.4948e+02],\n         [2.9254e+02],\n         [3.4220e+00],\n         [2.0820e+01],\n         [9.9123e+00],\n         [2.7203e+02],\n         [1.3203e+01],\n         [1.9147e+00],\n         [1.9685e+01],\n         [6.3715e+01],\n         [4.0177e+01],\n         [2.9448e+01],\n         [7.2568e+01],\n         [1.9368e+00],\n         [2.3502e+02],\n         [1.5419e+02],\n         [7.7611e+00],\n         [1.1178e+01],\n         [2.9338e+02],\n         [8.7860e+01],\n         [1.0418e+00],\n         [9.7874e+00],\n         [1.3536e+02],\n         [1.5511e+02],\n         [5.8747e+00],\n         [1.7329e+01],\n         [3.8031e+00],\n         [7.7526e+00],\n         [5.1781e+01],\n         [1.0980e+02],\n         [5.8121e+01],\n         [1.7563e+00],\n         [8.0375e+01],\n         [1.0403e+01],\n         [1.2569e+01],\n         [3.6714e+00],\n         [7.6806e+01],\n         [2.8931e+02],\n         [4.4615e+00],\n         [1.4520e+01],\n         [2.2407e+02],\n         [2.8873e+01],\n         [1.5471e+02],\n         [2.1743e+00],\n         [2.1615e+01],\n         [3.5293e+00],\n         [1.0334e+01],\n         [1.5922e+02],\n         [4.4799e+00],\n         [3.7872e+01],\n         [8.0834e+01],\n         [5.6977e+01],\n         [3.9035e+01],\n         [2.4375e+01],\n         [6.0220e+00],\n         [1.3067e+01],\n         [5.0322e+00],\n         [5.4968e+01],\n         [2.6911e+02],\n         [1.3384e+02],\n         [5.3387e+01],\n         [6.8190e+02],\n         [1.2645e+02],\n         [8.8905e+01],\n         [2.9156e+02],\n         [9.9848e+01],\n         [9.5199e+01],\n         [4.4221e+01],\n         [3.4626e+02],\n         [2.3447e+00],\n         [1.6699e+02],\n         [1.8866e+01],\n         [1.2309e+01],\n         [2.6904e+01],\n         [1.6170e+00],\n         [1.2058e+02],\n         [9.4137e+01],\n         [1.3859e+02],\n         [1.0922e+01],\n         [2.4370e+00],\n         [3.2916e+01],\n         [1.6130e+02],\n         [3.1900e+02],\n         [1.0484e+01],\n         [1.0862e+01],\n         [4.8495e+00],\n         [7.8733e+01],\n         [2.7699e+00],\n         [1.1949e+01],\n         [7.9246e+01],\n         [1.4261e+02],\n         [2.0315e+00],\n         [2.3502e+01],\n         [3.9320e+01],\n         [4.9751e+01],\n         [1.5886e+01],\n         [5.2927e+01],\n         [1.0584e+02],\n         [1.2028e+02],\n         [7.3707e-01],\n         [9.6696e+01],\n         [1.4797e+01],\n         [5.4910e+00],\n         [1.5503e+01],\n         [1.8765e+01],\n         [2.2667e+02],\n         [4.6284e+01],\n         [1.3118e+02],\n         [5.0807e+01]])}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "def collate_fn(batch):\n",
    "    elem = batch[0]\n",
    "    assert isinstance(elem, collections.abc.Mapping)\n",
    "\n",
    "    return {}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class RegressionCNN(torch.nn.Module):\n",
    "  def __init__(self, input_size=1, output_size=1, image_size=32, kernel_size=3):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size #(input channel e.g. RBG)\n",
    "    self.output_size = output_size #(num_classes)\n",
    "    self.image_size = image_size #(height, width)\n",
    "    self.kernel_size = kernel_size #(filter size)\n",
    "    self.model = self.build_model()\n",
    "\n",
    "  @property\n",
    "  def computeRF(self, ):\n",
    "    #(h = np.floor(H + 2p - d*(k-1) - 1) / s)\n",
    "    return np.floor(self.image_size - (self.kernel_size - 1) - 1)\n",
    "\n",
    "  def build_model(self):\n",
    "    # add convolutional layers\n",
    "    layers = [\n",
    "        torch.nn.Conv2d(self.input_size, 32, kernel_size=self.kernel_size), #input: (1,128,128) -> output: (32, 125, 125)\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=self.kernel_size), #input: (32, 125, 125) -> output: (64, 122, 122)\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "\n",
    "        #torch.nn.MaxPool2d(kernel_size=(2,2)), #input: (64, 122, 122) -> output: (64, 61, 61)\n",
    "        torch.nn.AdaptiveMaxPool2d(10), #input: (64, 122, 122)  -> output: (64, 10, 10)\n",
    "        #torch.nn.Dropout(0.10),\n",
    "\n",
    "        # add dense layers\n",
    "        torch.nn.Flatten(start_dim=1, end_dim=-1),\n",
    "        torch.nn.Linear(64*10*10, 128),\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "        #torch.nn.Dropout(0.25),\n",
    "        torch.nn.Linear(128, self.output_size),\n",
    "        #torch.nn.LogSoftmax(128,self.output_size),\n",
    "        torch.nn.ReLU(inplace=True)\n",
    "        ]\n",
    "    #model = torch.nn.Sequential(*layers)\n",
    "    model = torch.nn.ModuleList(layers)\n",
    "\n",
    "    # compile the model\n",
    "    return model\n",
    "\n",
    "  def forward(self, pd=None, image=None, cif=None, workcap=None, sel=None):\n",
    "    x = image\n",
    "    for layer in self.model:\n",
    "      x = layer(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "model = RegressionCNN(kernel_size=9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1453],\n        [0.2619],\n        [0.0278],\n        [0.0426],\n        [0.0747],\n        [0.0345],\n        [0.0845],\n        [0.0720],\n        [0.0669],\n        [0.0391],\n        [0.0506],\n        [0.0577],\n        [0.0654],\n        [0.2548],\n        [0.0556],\n        [0.1280],\n        [0.0420],\n        [0.0323],\n        [0.1588],\n        [0.0515],\n        [0.0789],\n        [0.0831],\n        [0.0629],\n        [0.0592],\n        [0.0659],\n        [0.0945],\n        [0.0676],\n        [0.0727],\n        [0.1047],\n        [0.0638],\n        [0.0652],\n        [0.0601],\n        [0.0494],\n        [0.2752],\n        [0.0631],\n        [0.0764],\n        [0.0542],\n        [0.0587],\n        [0.0648],\n        [0.1011],\n        [0.0549],\n        [0.0652],\n        [0.0937],\n        [0.1097],\n        [0.0629],\n        [0.0300],\n        [0.0791],\n        [0.0550],\n        [0.0548],\n        [0.0777],\n        [0.1531],\n        [0.0134],\n        [0.0545],\n        [0.1579],\n        [0.0654],\n        [0.0546],\n        [0.0627],\n        [0.0582],\n        [0.0833],\n        [0.0953],\n        [0.0878],\n        [0.0743],\n        [0.0874],\n        [0.1039],\n        [0.0456],\n        [0.0536],\n        [0.0433],\n        [0.0695],\n        [0.0816],\n        [0.0000],\n        [0.0735],\n        [0.0813],\n        [0.0876],\n        [0.0262],\n        [0.0789],\n        [0.1226],\n        [0.0406],\n        [0.0546],\n        [0.0703],\n        [0.0602],\n        [0.0687],\n        [0.0356],\n        [0.0559],\n        [0.0440],\n        [0.0726],\n        [0.0562],\n        [0.0470],\n        [0.0461],\n        [0.0456],\n        [0.0783],\n        [0.0565],\n        [0.0995],\n        [0.1404],\n        [0.0390],\n        [0.0679],\n        [0.0519],\n        [0.0568],\n        [0.0791],\n        [0.0503],\n        [0.1038],\n        [0.0133],\n        [0.0613],\n        [0.1936],\n        [0.0923],\n        [0.0957],\n        [0.0958],\n        [0.0943],\n        [0.0523],\n        [0.0740],\n        [0.0625],\n        [0.0653],\n        [0.0700],\n        [0.0843],\n        [0.1539],\n        [0.0670],\n        [0.0673],\n        [0.0748],\n        [0.0626],\n        [0.0592],\n        [0.1169],\n        [0.0539],\n        [0.0938],\n        [0.0679],\n        [0.0997],\n        [0.0345],\n        [0.0666],\n        [0.0171],\n        [0.0767],\n        [0.0009],\n        [0.0750],\n        [0.0000],\n        [0.0000],\n        [0.0429],\n        [0.0437],\n        [0.0659],\n        [0.0626],\n        [0.0691],\n        [0.0731],\n        [0.0642],\n        [0.0668],\n        [0.0771],\n        [0.0604],\n        [0.0679],\n        [0.0314],\n        [0.0584],\n        [0.0708],\n        [0.0594],\n        [0.0691],\n        [0.0482],\n        [0.0655],\n        [0.2175],\n        [0.0582],\n        [0.0381],\n        [0.0623],\n        [0.0455],\n        [0.0551],\n        [0.0588],\n        [0.0723],\n        [0.0617],\n        [0.0576],\n        [0.0629],\n        [0.0000],\n        [0.4028],\n        [0.0624],\n        [0.0582],\n        [0.0263],\n        [0.0977],\n        [0.1601],\n        [0.0241],\n        [0.3712],\n        [0.0528],\n        [0.0733],\n        [0.0414],\n        [0.0676],\n        [0.0139],\n        [0.0277],\n        [0.0624],\n        [0.0471],\n        [0.1010],\n        [0.0611],\n        [0.0623],\n        [0.0607],\n        [0.0295],\n        [0.0665],\n        [0.0501],\n        [0.0823],\n        [0.0665],\n        [0.0791],\n        [0.0816],\n        [0.0353],\n        [0.0299],\n        [0.0970],\n        [0.0441],\n        [0.1090],\n        [0.0804],\n        [0.0651],\n        [0.0508],\n        [0.1058],\n        [0.0714],\n        [0.0444],\n        [0.0740],\n        [0.2794],\n        [0.0707],\n        [0.0254],\n        [0.0862],\n        [0.1253],\n        [0.1161],\n        [0.0547],\n        [0.0753],\n        [0.0429],\n        [0.0655],\n        [0.0799],\n        [0.0333],\n        [0.0642],\n        [0.0396],\n        [0.0397],\n        [0.0684],\n        [0.2071],\n        [0.0384],\n        [0.0727],\n        [0.0570],\n        [0.0842],\n        [0.0486],\n        [0.0334],\n        [0.0962],\n        [0.0648],\n        [0.0581],\n        [0.1280],\n        [0.0022],\n        [0.0651],\n        [0.0657],\n        [0.0550],\n        [0.0710],\n        [0.0555],\n        [0.0526],\n        [0.0681],\n        [0.1058],\n        [0.0964],\n        [0.0503],\n        [0.0081],\n        [0.0575],\n        [0.0686],\n        [0.0851],\n        [0.0694],\n        [0.0864],\n        [0.0153],\n        [0.0519],\n        [0.0553],\n        [0.0544],\n        [0.0495],\n        [0.0000],\n        [0.0431],\n        [0.0699],\n        [0.0732],\n        [0.0623],\n        [0.0695],\n        [0.0875],\n        [0.0845],\n        [0.0310],\n        [0.1070],\n        [0.0555],\n        [0.0964],\n        [0.0347],\n        [0.0550],\n        [0.0446],\n        [0.0481],\n        [0.1003],\n        [0.1534],\n        [0.0712],\n        [0.0000],\n        [0.0063],\n        [0.0730],\n        [0.0604],\n        [0.0517],\n        [0.0541],\n        [0.0483],\n        [0.0487],\n        [0.2272],\n        [0.0649],\n        [0.1191],\n        [0.0488],\n        [0.0870],\n        [0.0867],\n        [0.0062],\n        [0.1606],\n        [0.0147],\n        [0.0000],\n        [0.0645],\n        [0.0277],\n        [0.0767],\n        [0.0438],\n        [0.0430],\n        [0.0996],\n        [0.0566],\n        [0.0742],\n        [0.0477],\n        [0.1567],\n        [0.0607],\n        [0.0681],\n        [0.0926],\n        [0.0633],\n        [0.0407],\n        [0.0939],\n        [0.0572],\n        [0.2281],\n        [0.0739],\n        [0.0723],\n        [0.0396],\n        [0.0716],\n        [0.0541],\n        [0.0683],\n        [0.0599],\n        [0.0794],\n        [0.0336],\n        [0.0762],\n        [0.0728],\n        [0.0530],\n        [0.0681],\n        [0.0519],\n        [0.0577],\n        [0.0847],\n        [0.0595],\n        [0.0538],\n        [0.0638],\n        [0.0637],\n        [0.0735],\n        [0.0745],\n        [0.0000],\n        [0.0825],\n        [0.0542],\n        [0.0457],\n        [0.0650],\n        [0.0352],\n        [0.1348],\n        [0.1744],\n        [0.0991],\n        [0.1162],\n        [0.0790],\n        [0.0557],\n        [0.0758],\n        [0.1795],\n        [0.0616],\n        [0.0871],\n        [0.0540],\n        [0.0980],\n        [0.0369],\n        [0.1185],\n        [0.0663],\n        [0.0569],\n        [0.0675],\n        [0.1339],\n        [0.0543],\n        [0.1249],\n        [0.1166],\n        [0.0535],\n        [0.0509],\n        [0.0785],\n        [0.0755],\n        [0.0579],\n        [0.0554],\n        [0.0458],\n        [0.0841],\n        [0.0649],\n        [0.0539],\n        [0.0649],\n        [0.0050],\n        [0.0507],\n        [0.0000],\n        [0.0591],\n        [0.0390],\n        [0.0923],\n        [0.0934],\n        [0.0661],\n        [0.1668],\n        [0.0725],\n        [0.1112],\n        [0.0693],\n        [0.0253],\n        [0.0547],\n        [0.1825],\n        [0.0437],\n        [0.1292],\n        [0.0432],\n        [0.0000],\n        [0.0723],\n        [0.0549],\n        [0.1462],\n        [0.0795],\n        [0.1520],\n        [0.0235],\n        [0.0491],\n        [0.1431],\n        [0.0622],\n        [0.0559],\n        [0.1722],\n        [0.0596],\n        [0.0655],\n        [0.0750],\n        [0.1025],\n        [0.0583],\n        [0.0677],\n        [0.0604],\n        [0.0688],\n        [0.0396],\n        [0.0454],\n        [0.0927],\n        [0.0864],\n        [0.0779],\n        [0.0663],\n        [0.0420],\n        [0.1015],\n        [0.0560],\n        [0.0847],\n        [0.0551],\n        [0.0000],\n        [0.0909],\n        [0.0249],\n        [0.0588],\n        [0.0956],\n        [0.1084],\n        [0.0757],\n        [0.0550],\n        [0.0356],\n        [0.0587],\n        [0.0660],\n        [0.0628],\n        [0.0817],\n        [0.0315],\n        [0.0712],\n        [0.0250],\n        [0.0931],\n        [0.0833],\n        [0.0132],\n        [0.0792],\n        [0.0407],\n        [0.0617],\n        [0.0485],\n        [0.0567],\n        [0.0590],\n        [0.1611],\n        [0.0890],\n        [0.0628],\n        [0.0964],\n        [0.0741],\n        [0.0615],\n        [0.0853],\n        [0.0000],\n        [0.0416],\n        [0.1888],\n        [0.0513],\n        [0.0760],\n        [0.0535],\n        [0.0403],\n        [0.0646],\n        [0.0498],\n        [0.0398],\n        [0.0000],\n        [0.0483],\n        [0.0646],\n        [0.0731],\n        [0.0633],\n        [0.0311],\n        [0.1379],\n        [0.1238],\n        [0.0953],\n        [0.0698],\n        [0.0607],\n        [0.0766],\n        [0.0815],\n        [0.0304],\n        [0.1270],\n        [0.0538],\n        [0.0578],\n        [0.0173],\n        [0.0600],\n        [0.0492],\n        [0.0306],\n        [0.1314],\n        [0.0628],\n        [0.0598],\n        [0.0661],\n        [0.0922],\n        [0.0542],\n        [0.0603],\n        [0.0000],\n        [0.1007],\n        [0.0612],\n        [0.0518],\n        [0.0558],\n        [0.0656],\n        [0.0577],\n        [0.0316],\n        [0.0706],\n        [0.0749],\n        [0.0517],\n        [0.0559],\n        [0.0562],\n        [0.0458],\n        [0.0508],\n        [0.0000],\n        [0.0442],\n        [0.1322],\n        [0.0571],\n        [0.0712],\n        [0.0504],\n        [0.0589],\n        [0.0862],\n        [0.1114],\n        [0.0812],\n        [0.1058],\n        [0.0477],\n        [0.0277]], grad_fn=<ReluBackward0>)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**next(iter(train_dataloader)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "MLinDir=f\"{inDir}/dataSplitML\"\n",
    "optVar=\"workcap\"\n",
    "\n",
    "trainDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/trainCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainDataset, batch_size=2048, pin_memory=True, num_workers=0, shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "testDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/testCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "test_dataloader= torch.utils.data.DataLoader(testDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n",
    "\n",
    "validDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/valCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "valid_dataloader= torch.utils.data.DataLoader(validDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = RegressionCNN(kernel_size=9)\n",
    "#optVar=\"workcap\"\n",
    "def main(args):\n",
    "    epochs = 1500\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    device = torch.device(0) #torch.cuda.current_device()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters()) #model.parameters() # extract Ws from the model\n",
    "    best_loss = np.inf\n",
    "    best_epoch=0\n",
    "    for e in range(epochs):\n",
    "      for batch in train_dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True) #dL/dWs = 0 vs None\n",
    "        image = batch['image'].to(device)\n",
    "        pred = model(image=image) #output (batch, 1)\n",
    "        y = batch[optVar].to(device) #output (batch, 1)\n",
    "        loss = loss_fn(pred, y) #scalar (Tensor)\n",
    "        loss.backward() #gradient dL/dWs\n",
    "        optimizer.step() #W <- W - lr * dL/dWs\n",
    "      val_loss = 0.\n",
    "      for batch in valid_dataloader:\n",
    "        model.eval()\n",
    "        image = batch['image'].to(device)\n",
    "        pred = model(image=image) #output (batch, 1)\n",
    "        y = batch[optVar].to(device) #output (batch, 1)\n",
    "        loss = loss_fn(pred, y) #scalar (Tensor)\n",
    "        val_loss += loss.item()\n",
    "      val_loss /= len(valid_dataloader)\n",
    "      if val_loss < best_loss:\n",
    "        torch.save({\"model\":model.state_dict()}, f\"{inDir}/tThresh0_CNN_-8_23_{optVar}_noDrop.pth\")\n",
    "        best_loss = val_loss\n",
    "        best_epoch = e\n",
    "    print(f\"At epoch {best_epoch} : loss {val_loss} \")\n",
    "    return model\n",
    "args=None\n",
    "model=main(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(0)\n",
    "selckpt = torch.load(f\"{inDir}/tThresh0_CNN_-8_23_{optVar}.pth\", map_location=device)\n",
    "model2=RegressionCNN(kernel_size=9)\n",
    "model2.load_state_dict(selckpt['model'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [2.1048],\n",
      "        [2.2924],\n",
      "        ...,\n",
      "        [0.6077],\n",
      "        [0.5194],\n",
      "        [0.7877]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "tensor([[2.2497],\n",
      "        [1.0169],\n",
      "        [2.0522],\n",
      "        [0.9859],\n",
      "        [0.6015],\n",
      "        [0.8402],\n",
      "        [1.5439],\n",
      "        [0.3926],\n",
      "        [1.3024],\n",
      "        [1.1030],\n",
      "        [1.4132],\n",
      "        [0.5736],\n",
      "        [1.5847],\n",
      "        [0.5346],\n",
      "        [0.9448],\n",
      "        [0.6304],\n",
      "        [0.9569],\n",
      "        [0.4713],\n",
      "        [0.5453],\n",
      "        [1.3267],\n",
      "        [0.6625],\n",
      "        [0.6541],\n",
      "        [1.2392],\n",
      "        [0.0000],\n",
      "        [1.6719],\n",
      "        [0.4587],\n",
      "        [0.5253],\n",
      "        [1.3201],\n",
      "        [0.8056],\n",
      "        [0.8660],\n",
      "        [2.1460],\n",
      "        [0.0872],\n",
      "        [0.3872],\n",
      "        [0.6807],\n",
      "        [0.9426],\n",
      "        [1.1987],\n",
      "        [1.4648],\n",
      "        [1.8099],\n",
      "        [1.6239],\n",
      "        [0.4657],\n",
      "        [0.9935],\n",
      "        [0.8689],\n",
      "        [0.9793],\n",
      "        [0.9097],\n",
      "        [1.0589],\n",
      "        [0.9997],\n",
      "        [1.3864],\n",
      "        [0.7188],\n",
      "        [0.5750],\n",
      "        [0.7217],\n",
      "        [1.0526],\n",
      "        [0.4168],\n",
      "        [0.8461],\n",
      "        [2.3930],\n",
      "        [1.7992],\n",
      "        [2.5182],\n",
      "        [0.5985],\n",
      "        [0.4986],\n",
      "        [0.4651],\n",
      "        [0.8078],\n",
      "        [1.7083],\n",
      "        [1.6152],\n",
      "        [0.6334],\n",
      "        [1.6362],\n",
      "        [0.6848],\n",
      "        [0.5261],\n",
      "        [0.6184],\n",
      "        [1.0259],\n",
      "        [1.3181],\n",
      "        [2.2475],\n",
      "        [2.2786],\n",
      "        [1.8560],\n",
      "        [0.7420],\n",
      "        [1.2566],\n",
      "        [0.6646],\n",
      "        [1.7426],\n",
      "        [0.2312],\n",
      "        [0.8115],\n",
      "        [0.6416],\n",
      "        [0.9711],\n",
      "        [0.4559],\n",
      "        [1.0960],\n",
      "        [1.1510],\n",
      "        [1.0550],\n",
      "        [0.7694],\n",
      "        [0.5813],\n",
      "        [0.6148],\n",
      "        [0.3821],\n",
      "        [0.7159],\n",
      "        [2.3377],\n",
      "        [1.9653],\n",
      "        [1.7661],\n",
      "        [0.9151],\n",
      "        [0.4936],\n",
      "        [1.0053],\n",
      "        [0.6283],\n",
      "        [2.0593],\n",
      "        [0.7886],\n",
      "        [0.8338],\n",
      "        [0.4002],\n",
      "        [2.1326],\n",
      "        [0.3495],\n",
      "        [0.4574],\n",
      "        [1.2032],\n",
      "        [1.3151],\n",
      "        [0.7980],\n",
      "        [1.5428],\n",
      "        [0.5357],\n",
      "        [1.3054],\n",
      "        [0.3666],\n",
      "        [0.5434],\n",
      "        [0.4136],\n",
      "        [0.6031],\n",
      "        [0.5271],\n",
      "        [0.5044],\n",
      "        [0.4818],\n",
      "        [0.6792],\n",
      "        [2.4375],\n",
      "        [0.9828],\n",
      "        [0.7848],\n",
      "        [0.7752],\n",
      "        [0.4493],\n",
      "        [0.5412],\n",
      "        [0.3714],\n",
      "        [0.7517],\n",
      "        [0.8328],\n",
      "        [0.3485],\n",
      "        [1.3997],\n",
      "        [0.8773],\n",
      "        [0.9434],\n",
      "        [1.4567],\n",
      "        [0.8256],\n",
      "        [0.8467],\n",
      "        [0.6179],\n",
      "        [0.2719],\n",
      "        [1.4585],\n",
      "        [0.4777],\n",
      "        [2.0474],\n",
      "        [0.9166],\n",
      "        [0.7005],\n",
      "        [1.9660],\n",
      "        [1.3402],\n",
      "        [0.5442],\n",
      "        [0.9076],\n",
      "        [1.1648],\n",
      "        [0.4980],\n",
      "        [0.6632],\n",
      "        [0.8297],\n",
      "        [0.9969],\n",
      "        [1.6718],\n",
      "        [0.9531],\n",
      "        [0.6872],\n",
      "        [1.1398],\n",
      "        [0.4875],\n",
      "        [0.5217],\n",
      "        [1.5187],\n",
      "        [2.6719],\n",
      "        [0.8916],\n",
      "        [1.4586],\n",
      "        [1.0931],\n",
      "        [0.6130],\n",
      "        [0.9107],\n",
      "        [1.3933],\n",
      "        [1.3102],\n",
      "        [1.9211],\n",
      "        [0.7306],\n",
      "        [1.5895],\n",
      "        [0.6208],\n",
      "        [0.4026],\n",
      "        [0.6652],\n",
      "        [0.5020],\n",
      "        [0.8167],\n",
      "        [0.5228],\n",
      "        [1.2608],\n",
      "        [0.6570],\n",
      "        [0.7808],\n",
      "        [0.6027],\n",
      "        [0.5697],\n",
      "        [1.4524],\n",
      "        [1.5383],\n",
      "        [1.4639],\n",
      "        [0.4649],\n",
      "        [0.6488],\n",
      "        [1.3686],\n",
      "        [1.7834],\n",
      "        [0.5740],\n",
      "        [1.5985],\n",
      "        [1.0234],\n",
      "        [1.2362],\n",
      "        [2.0982],\n",
      "        [0.6157],\n",
      "        [0.4958],\n",
      "        [0.2334],\n",
      "        [1.2725],\n",
      "        [1.4204],\n",
      "        [1.2106],\n",
      "        [0.9743],\n",
      "        [1.9001],\n",
      "        [1.0775],\n",
      "        [0.7499],\n",
      "        [0.7221],\n",
      "        [1.0670],\n",
      "        [1.7259],\n",
      "        [1.0882],\n",
      "        [0.7385],\n",
      "        [0.6306],\n",
      "        [2.1050],\n",
      "        [1.1918],\n",
      "        [0.3148],\n",
      "        [0.4302],\n",
      "        [0.3526],\n",
      "        [1.5927],\n",
      "        [0.7067],\n",
      "        [0.5338],\n",
      "        [1.7103],\n",
      "        [1.6697],\n",
      "        [0.9800],\n",
      "        [0.5283],\n",
      "        [0.9678],\n",
      "        [2.2385],\n",
      "        [0.7187],\n",
      "        [0.6821],\n",
      "        [0.7048],\n",
      "        [1.5630],\n",
      "        [0.4487],\n",
      "        [1.7861],\n",
      "        [0.5773],\n",
      "        [1.0124],\n",
      "        [0.4807],\n",
      "        [1.3386],\n",
      "        [1.5635],\n",
      "        [3.0531],\n",
      "        [1.9198],\n",
      "        [2.8797],\n",
      "        [0.6050],\n",
      "        [0.9976],\n",
      "        [1.9558],\n",
      "        [0.7549],\n",
      "        [0.8814],\n",
      "        [0.7552],\n",
      "        [0.6409],\n",
      "        [0.7193],\n",
      "        [1.5208],\n",
      "        [2.0139],\n",
      "        [2.4608],\n",
      "        [1.0044],\n",
      "        [1.1243],\n",
      "        [0.5879],\n",
      "        [0.4019],\n",
      "        [0.3392],\n",
      "        [2.3022],\n",
      "        [0.7532],\n",
      "        [1.5020],\n",
      "        [0.7592],\n",
      "        [0.6233],\n",
      "        [0.8418],\n",
      "        [1.7363],\n",
      "        [1.3084],\n",
      "        [1.2072],\n",
      "        [0.5607],\n",
      "        [0.2901],\n",
      "        [1.2297],\n",
      "        [0.8564],\n",
      "        [0.4818],\n",
      "        [0.7390],\n",
      "        [0.8974],\n",
      "        [0.6586],\n",
      "        [2.3548],\n",
      "        [1.4850],\n",
      "        [0.7442],\n",
      "        [0.9530],\n",
      "        [1.4608],\n",
      "        [1.0751],\n",
      "        [0.6765],\n",
      "        [1.2638],\n",
      "        [1.3267],\n",
      "        [0.4653],\n",
      "        [0.9729],\n",
      "        [0.0743],\n",
      "        [0.8864],\n",
      "        [1.9500],\n",
      "        [1.0410],\n",
      "        [0.4806],\n",
      "        [0.2921],\n",
      "        [0.6325],\n",
      "        [0.7110],\n",
      "        [0.4768],\n",
      "        [0.5824],\n",
      "        [3.0194],\n",
      "        [0.7198],\n",
      "        [0.8945],\n",
      "        [0.3576],\n",
      "        [0.8095],\n",
      "        [0.6764],\n",
      "        [0.6430],\n",
      "        [0.5333],\n",
      "        [1.2412],\n",
      "        [1.2806],\n",
      "        [0.6459],\n",
      "        [0.4391],\n",
      "        [0.4805],\n",
      "        [1.5159],\n",
      "        [0.7037],\n",
      "        [0.7064],\n",
      "        [0.5506],\n",
      "        [0.6713],\n",
      "        [1.3050],\n",
      "        [0.6715],\n",
      "        [0.3688],\n",
      "        [2.4551],\n",
      "        [1.4747],\n",
      "        [1.3751],\n",
      "        [1.2196],\n",
      "        [0.4110],\n",
      "        [0.8053],\n",
      "        [1.7127],\n",
      "        [1.0907],\n",
      "        [0.7025],\n",
      "        [1.0303],\n",
      "        [1.9619],\n",
      "        [1.3093],\n",
      "        [0.4867],\n",
      "        [1.2765],\n",
      "        [0.9115],\n",
      "        [0.6001],\n",
      "        [0.9741],\n",
      "        [1.6774],\n",
      "        [0.6684],\n",
      "        [1.4106],\n",
      "        [0.4305],\n",
      "        [1.0870],\n",
      "        [0.4399],\n",
      "        [0.9581],\n",
      "        [0.9874],\n",
      "        [1.1654],\n",
      "        [1.9899],\n",
      "        [0.7395],\n",
      "        [0.8522],\n",
      "        [0.4590],\n",
      "        [0.4893],\n",
      "        [1.4334],\n",
      "        [1.6178],\n",
      "        [1.1662],\n",
      "        [0.5108],\n",
      "        [0.8017],\n",
      "        [0.4787],\n",
      "        [1.7182],\n",
      "        [1.4006],\n",
      "        [0.6363],\n",
      "        [0.5701],\n",
      "        [0.4482],\n",
      "        [0.4126],\n",
      "        [0.8768],\n",
      "        [1.4040],\n",
      "        [0.6044],\n",
      "        [0.5055],\n",
      "        [2.0570],\n",
      "        [1.0971],\n",
      "        [0.9856],\n",
      "        [1.2005],\n",
      "        [0.6388],\n",
      "        [0.5816],\n",
      "        [0.8384],\n",
      "        [0.5034],\n",
      "        [0.7208],\n",
      "        [1.1546],\n",
      "        [0.7456],\n",
      "        [0.6105],\n",
      "        [0.5532],\n",
      "        [1.6263],\n",
      "        [2.3735],\n",
      "        [0.8431],\n",
      "        [0.5166],\n",
      "        [0.7112],\n",
      "        [0.3938],\n",
      "        [1.3862],\n",
      "        [1.4674],\n",
      "        [0.4476],\n",
      "        [0.5432],\n",
      "        [1.1638],\n",
      "        [1.5783],\n",
      "        [0.5693],\n",
      "        [1.7448],\n",
      "        [1.5945],\n",
      "        [1.8364],\n",
      "        [1.2099],\n",
      "        [0.6035],\n",
      "        [0.7542],\n",
      "        [0.8245],\n",
      "        [1.2206],\n",
      "        [0.8972],\n",
      "        [1.0763],\n",
      "        [0.9261],\n",
      "        [1.2308],\n",
      "        [0.8553],\n",
      "        [2.1304],\n",
      "        [1.0448],\n",
      "        [1.0436],\n",
      "        [0.5368],\n",
      "        [1.1642],\n",
      "        [0.7145],\n",
      "        [0.8080],\n",
      "        [1.3197],\n",
      "        [0.8472],\n",
      "        [1.5514],\n",
      "        [1.1837],\n",
      "        [0.6137],\n",
      "        [1.1913],\n",
      "        [0.6909],\n",
      "        [0.4036],\n",
      "        [0.7706],\n",
      "        [0.8341],\n",
      "        [0.3063],\n",
      "        [1.2241],\n",
      "        [0.3762],\n",
      "        [0.6335],\n",
      "        [0.3447],\n",
      "        [0.4795],\n",
      "        [0.4125],\n",
      "        [1.6066],\n",
      "        [0.8876],\n",
      "        [1.6317],\n",
      "        [0.6986],\n",
      "        [1.0033],\n",
      "        [0.4316],\n",
      "        [0.4754],\n",
      "        [0.5900],\n",
      "        [1.0730],\n",
      "        [0.4079],\n",
      "        [0.8195],\n",
      "        [1.5289],\n",
      "        [1.5446],\n",
      "        [0.6682],\n",
      "        [1.8405],\n",
      "        [0.4923],\n",
      "        [1.5457],\n",
      "        [0.8554],\n",
      "        [1.0192],\n",
      "        [0.7851],\n",
      "        [0.5830],\n",
      "        [0.5814],\n",
      "        [0.2939],\n",
      "        [0.5925],\n",
      "        [1.4484],\n",
      "        [0.9715],\n",
      "        [0.8360],\n",
      "        [1.1174],\n",
      "        [0.9814],\n",
      "        [0.7920],\n",
      "        [1.1462],\n",
      "        [1.5516],\n",
      "        [0.4538],\n",
      "        [1.8246],\n",
      "        [0.5179],\n",
      "        [0.6618],\n",
      "        [0.7493],\n",
      "        [0.7634],\n",
      "        [0.4336],\n",
      "        [0.9901],\n",
      "        [0.4283],\n",
      "        [0.3487],\n",
      "        [1.2424],\n",
      "        [0.8351],\n",
      "        [1.3613],\n",
      "        [0.9633],\n",
      "        [1.7156],\n",
      "        [2.4680],\n",
      "        [0.7486],\n",
      "        [1.9387],\n",
      "        [0.4964],\n",
      "        [1.6747],\n",
      "        [1.6240],\n",
      "        [2.6107],\n",
      "        [1.1320],\n",
      "        [0.4042],\n",
      "        [1.2626],\n",
      "        [0.0000],\n",
      "        [1.3647],\n",
      "        [0.6361],\n",
      "        [2.3518],\n",
      "        [1.1062],\n",
      "        [1.1821],\n",
      "        [0.4854],\n",
      "        [0.5968],\n",
      "        [2.3863],\n",
      "        [0.4371],\n",
      "        [0.6651],\n",
      "        [1.5780],\n",
      "        [0.9653],\n",
      "        [0.3742],\n",
      "        [0.8265],\n",
      "        [1.9341],\n",
      "        [0.8138],\n",
      "        [1.9147],\n",
      "        [0.4450],\n",
      "        [2.0015],\n",
      "        [1.5785],\n",
      "        [0.9414],\n",
      "        [0.8728],\n",
      "        [1.2479],\n",
      "        [0.4200],\n",
      "        [0.9446],\n",
      "        [0.8491],\n",
      "        [0.6702],\n",
      "        [0.7780],\n",
      "        [0.8592],\n",
      "        [0.6254],\n",
      "        [0.4758],\n",
      "        [1.2779],\n",
      "        [0.2651],\n",
      "        [0.5805],\n",
      "        [1.1341],\n",
      "        [1.9390],\n",
      "        [0.8607],\n",
      "        [1.0175],\n",
      "        [0.5288],\n",
      "        [0.6745],\n",
      "        [2.5849],\n",
      "        [1.3166],\n",
      "        [0.6347],\n",
      "        [1.4183],\n",
      "        [1.1682],\n",
      "        [0.6010],\n",
      "        [0.7056],\n",
      "        [0.6663],\n",
      "        [0.5761],\n",
      "        [0.4079],\n",
      "        [0.3885],\n",
      "        [0.9163],\n",
      "        [0.9149],\n",
      "        [0.8651],\n",
      "        [0.6004],\n",
      "        [2.0124],\n",
      "        [0.9475],\n",
      "        [1.1071],\n",
      "        [2.3616],\n",
      "        [0.5922],\n",
      "        [0.0000],\n",
      "        [0.6376],\n",
      "        [0.8019],\n",
      "        [1.4824],\n",
      "        [2.2022],\n",
      "        [1.8213],\n",
      "        [1.1065],\n",
      "        [0.4429],\n",
      "        [0.5720],\n",
      "        [0.6063],\n",
      "        [1.0045],\n",
      "        [0.6054],\n",
      "        [0.6271],\n",
      "        [0.7672],\n",
      "        [0.4278],\n",
      "        [1.0460],\n",
      "        [0.5536],\n",
      "        [0.5159],\n",
      "        [0.6995],\n",
      "        [1.8287],\n",
      "        [0.8605],\n",
      "        [0.8463],\n",
      "        [0.5274],\n",
      "        [0.4116],\n",
      "        [1.3545],\n",
      "        [0.6456],\n",
      "        [1.2065],\n",
      "        [1.4360],\n",
      "        [0.4308],\n",
      "        [2.8177],\n",
      "        [0.3266],\n",
      "        [0.7052],\n",
      "        [0.6954],\n",
      "        [1.6529],\n",
      "        [0.4343],\n",
      "        [0.5883],\n",
      "        [0.6505],\n",
      "        [0.2826],\n",
      "        [1.1271],\n",
      "        [0.7586],\n",
      "        [1.4445],\n",
      "        [1.5913],\n",
      "        [1.8536],\n",
      "        [1.7635],\n",
      "        [0.6053],\n",
      "        [0.6231],\n",
      "        [1.7550],\n",
      "        [1.9090],\n",
      "        [0.8389],\n",
      "        [1.5307],\n",
      "        [0.7505],\n",
      "        [0.8490],\n",
      "        [0.5298],\n",
      "        [0.4446],\n",
      "        [0.8849],\n",
      "        [0.7588],\n",
      "        [1.2674],\n",
      "        [0.7364],\n",
      "        [1.3560],\n",
      "        [1.6548],\n",
      "        [0.8347],\n",
      "        [1.3719],\n",
      "        [0.2320],\n",
      "        [1.4404],\n",
      "        [1.0056],\n",
      "        [1.2523],\n",
      "        [0.8056],\n",
      "        [1.6915],\n",
      "        [1.5238],\n",
      "        [0.7882],\n",
      "        [0.5915],\n",
      "        [0.8845],\n",
      "        [1.0932],\n",
      "        [0.5509],\n",
      "        [1.1023],\n",
      "        [1.4470],\n",
      "        [0.6111],\n",
      "        [1.8214],\n",
      "        [0.4898],\n",
      "        [1.2987],\n",
      "        [2.2433],\n",
      "        [2.0887],\n",
      "        [0.6875],\n",
      "        [0.6067],\n",
      "        [2.1761],\n",
      "        [1.3328],\n",
      "        [1.6230],\n",
      "        [1.1409],\n",
      "        [0.8134],\n",
      "        [3.8687],\n",
      "        [0.7826],\n",
      "        [1.1079],\n",
      "        [1.4270],\n",
      "        [1.0505],\n",
      "        [1.6545],\n",
      "        [0.6983],\n",
      "        [2.9411],\n",
      "        [0.6853],\n",
      "        [0.6800],\n",
      "        [0.9635],\n",
      "        [0.6754],\n",
      "        [0.5171],\n",
      "        [0.6869],\n",
      "        [0.5125],\n",
      "        [1.2104],\n",
      "        [1.4748],\n",
      "        [0.8840],\n",
      "        [0.9867],\n",
      "        [1.1967],\n",
      "        [0.5127],\n",
      "        [1.5548],\n",
      "        [0.3047],\n",
      "        [1.8522],\n",
      "        [0.7433],\n",
      "        [1.3954],\n",
      "        [0.6769],\n",
      "        [1.4751],\n",
      "        [0.4872],\n",
      "        [0.7558],\n",
      "        [0.9621],\n",
      "        [0.5644],\n",
      "        [1.1945],\n",
      "        [1.6254],\n",
      "        [0.4407],\n",
      "        [1.1459],\n",
      "        [0.4687],\n",
      "        [1.3332],\n",
      "        [0.8018],\n",
      "        [0.9328],\n",
      "        [0.5634],\n",
      "        [1.1415],\n",
      "        [1.3224],\n",
      "        [0.6209],\n",
      "        [0.9832],\n",
      "        [0.3190],\n",
      "        [0.7114],\n",
      "        [1.4245],\n",
      "        [1.3006],\n",
      "        [0.4835],\n",
      "        [1.1525],\n",
      "        [0.4019],\n",
      "        [2.0098],\n",
      "        [0.8051],\n",
      "        [0.6308],\n",
      "        [1.0638],\n",
      "        [0.9997],\n",
      "        [1.6964],\n",
      "        [2.0291],\n",
      "        [0.9989],\n",
      "        [1.0659],\n",
      "        [1.4651],\n",
      "        [0.7143],\n",
      "        [1.7322],\n",
      "        [2.2181],\n",
      "        [0.4212],\n",
      "        [1.1210],\n",
      "        [0.6370],\n",
      "        [0.5939],\n",
      "        [0.6127],\n",
      "        [0.6710],\n",
      "        [0.8894],\n",
      "        [1.7426],\n",
      "        [1.5147],\n",
      "        [0.6943],\n",
      "        [0.5343],\n",
      "        [0.8145],\n",
      "        [1.1602],\n",
      "        [2.2545],\n",
      "        [1.0162],\n",
      "        [0.5743],\n",
      "        [0.7098],\n",
      "        [0.4470],\n",
      "        [0.7144],\n",
      "        [0.9095],\n",
      "        [0.6309],\n",
      "        [0.5040],\n",
      "        [0.4298],\n",
      "        [1.7090],\n",
      "        [0.7172],\n",
      "        [1.1061],\n",
      "        [0.3444],\n",
      "        [1.1302],\n",
      "        [1.7076],\n",
      "        [1.1368],\n",
      "        [1.4587],\n",
      "        [0.6845],\n",
      "        [0.6935],\n",
      "        [0.6736],\n",
      "        [1.3746],\n",
      "        [0.6290],\n",
      "        [1.2171],\n",
      "        [0.5002],\n",
      "        [0.6421],\n",
      "        [1.1438],\n",
      "        [1.0502],\n",
      "        [0.4360],\n",
      "        [0.6830],\n",
      "        [0.9611],\n",
      "        [0.6376],\n",
      "        [1.3160],\n",
      "        [1.7224],\n",
      "        [0.8161],\n",
      "        [1.5462],\n",
      "        [0.5745],\n",
      "        [0.8963],\n",
      "        [1.0223],\n",
      "        [1.5215],\n",
      "        [0.8923],\n",
      "        [1.1730],\n",
      "        [0.5633],\n",
      "        [1.1354],\n",
      "        [0.6060],\n",
      "        [2.4962],\n",
      "        [2.5475],\n",
      "        [0.8657],\n",
      "        [1.1098],\n",
      "        [1.2990],\n",
      "        [1.1710],\n",
      "        [0.7024],\n",
      "        [1.1697],\n",
      "        [0.8390],\n",
      "        [0.6824],\n",
      "        [0.5212],\n",
      "        [0.1728],\n",
      "        [1.3735],\n",
      "        [1.3434],\n",
      "        [0.6538],\n",
      "        [0.7495],\n",
      "        [1.5491],\n",
      "        [0.5098],\n",
      "        [0.4781],\n",
      "        [0.4978],\n",
      "        [2.2430],\n",
      "        [0.7836],\n",
      "        [1.3137],\n",
      "        [1.0155],\n",
      "        [0.8467],\n",
      "        [0.6328],\n",
      "        [0.8519],\n",
      "        [0.6910],\n",
      "        [0.8928],\n",
      "        [0.5574],\n",
      "        [0.5152],\n",
      "        [0.5451],\n",
      "        [0.5899],\n",
      "        [0.4827],\n",
      "        [1.3287],\n",
      "        [0.7116],\n",
      "        [1.5004],\n",
      "        [0.4408],\n",
      "        [2.6889],\n",
      "        [1.0052],\n",
      "        [0.4729],\n",
      "        [1.9993],\n",
      "        [1.6486],\n",
      "        [1.7785],\n",
      "        [1.1592],\n",
      "        [0.5594],\n",
      "        [0.3970],\n",
      "        [0.7506],\n",
      "        [2.0320],\n",
      "        [0.8812],\n",
      "        [0.6744],\n",
      "        [0.7160],\n",
      "        [0.6995],\n",
      "        [1.2579],\n",
      "        [0.6974],\n",
      "        [1.6363],\n",
      "        [1.2246],\n",
      "        [1.1230]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(selckpt['model'])\n",
    "model2.to(device)\n",
    "for batch in test_dataloader:\n",
    "    model2.eval()\n",
    "    image = batch['image'].to(device)\n",
    "    pred=model2(image=image)\n",
    "    print(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [23]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m      3\u001B[0m image \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36mRegressionCNN.forward\u001B[1;34m(self, pd, image, cif, workcap, sel)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m image\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m---> 44\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1]' is invalid for input of size 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model(\u001B[43mtrainDataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36mCubePHDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcif\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcifs[index],\n\u001B[0;32m     64\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphImg[index],\n\u001B[1;32m---> 65\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworkcap\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproperty\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     66\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproperty[index][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m,),\n\u001B[0;32m     67\u001B[0m             }\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[1]' is invalid for input of size 2"
     ]
    }
   ],
   "source": [
    "model(trainDataset[0]['image'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [91]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     13\u001B[0m image \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m#y=batch['workcap'].to(device)\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m testPred[j\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworkcap\u001B[39m\u001B[38;5;124m'\u001B[39m]):(j\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworkcap\u001B[39m\u001B[38;5;124m'\u001B[39m])]\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mhstack(np\u001B[38;5;241m.\u001B[39marray(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworkcap\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()),\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     16\u001B[0m j\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mTypeError\u001B[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "selckpt = torch.load(f\"{inDir}/tThresh0_CNN_-8_23_{optVar}.pth\")# map_location=device)\n",
    "\n",
    "model = RegressionCNN(kernel_size=9)\n",
    "model=model.load_state_dict(selckpt['model'])\n",
    "device=torch.cuda.current_device()\n",
    "\n",
    "\n",
    "testPred=np.zeros((len(testDataset),2),dtype=np.float32)\n",
    "j: int =0\n",
    "#model.to(device)\n",
    "for batch in test_dataloader:\n",
    "    model.eval()\n",
    "    image = batch['image'].to(device)\n",
    "    #y=batch['workcap'].to(device)\n",
    "    testPred[j*len(batch['workcap']):(j+1)*len(batch['workcap'])]=np.hstack(np.array(batch['workcap'].numpy()),(model(image=image)).numpy())\n",
    "    j+=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_IncompatibleKeys' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [108]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m model\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mload_state_dict(selckpt[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      7\u001B[0m device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mcurrent_device()\n\u001B[1;32m----> 8\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m(device)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m test_dataloader:\n\u001B[0;32m     10\u001B[0m     image\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_IncompatibleKeys' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "selckpt = torch.load(f\"{inDir}/tThresh0_CNN_-8_23_{optVar}.pth\")# map_location=device)\n",
    "\n",
    "model = RegressionCNN(kernel_size=9)\n",
    "model=model.load_state_dict(selckpt['model'])\n",
    "\n",
    "\n",
    "device=torch.cuda.current_device()\n",
    "model.to(device)\n",
    "for batch in test_dataloader:\n",
    "    image=batch[\"image\"].to(device)\n",
    "    pred=model(image=image)\n",
    "    display(torch.column_stack((batch[optVar].to(device),pred)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_IncompatibleKeys' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [114]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: '_IncompatibleKeys' object is not callable"
     ]
    }
   ],
   "source": [
    "model.to"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "test_dataloader= torch.utils.data.DataLoader(testDataset, batch_size=2, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [102]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m image\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      7\u001B[0m testPred[j,\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32(batch[optVar])\n\u001B[1;32m----> 8\u001B[0m testPred[j,\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      9\u001B[0m j\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [82]\u001B[0m, in \u001B[0;36mRegressionCNN.forward\u001B[1;34m(self, pd, image, cif, workcap, sel)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m image\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m---> 44\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "testPred=np.zeros((len(testDataset),2),dtype=np.float32)\n",
    "j=0\n",
    "for batch in test_dataloader:\n",
    "    #print()\n",
    "    image=batch[\"image\"].to(device)\n",
    "\n",
    "    testPred[j,0]=np.float32(batch[optVar])\n",
    "    testPred[j,1]=np.float32(model(image))\n",
    "    j+=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[2.66994 , 0.      ],\n       [0.147288, 1.      ],\n       [1.143492, 2.      ],\n       [1.259065, 3.      ],\n       [2.320613, 4.      ],\n       [1.196761, 5.      ],\n       [1.843505, 6.      ],\n       [0.949611, 7.      ],\n       [2.302493, 8.      ],\n       [0.6985  , 9.      ]], dtype=float32)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPred[:10,1]=np.arange(10)\n",
    "testPred[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [105]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m test_dataloader:\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mfloat\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [82]\u001B[0m, in \u001B[0;36mRegressionCNN.forward\u001B[1;34m(self, pd, image, cif, workcap, sel)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m image\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m---> 44\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    print(float(model(batch['image'].to(device))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [100]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m test_dataloader:\n\u001B[0;32m      2\u001B[0m     image\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [82]\u001B[0m, in \u001B[0;36mRegressionCNN.forward\u001B[1;34m(self, pd, image, cif, workcap, sel)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m image\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m---> 44\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    image=batch[\"image\"].to(device)\n",
    "    print(model(image))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
