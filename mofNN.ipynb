{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import tcripser\n",
    "import gudhi,gudhi.hera,gudhi.wasserstein,persim\n",
    "import ase\n",
    "from ase.io import cube\n",
    "from ase.io import cif\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from typing import *\n",
    "import collections\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "inDir=\"Z:/data/diverse_metals\"\n",
    "df=pd.read_csv(f\"{inDir}/post-combustion-vsa-2-clean.csv\",index_col=0)\n",
    "phDF=pd.read_csv(f\"{inDir}/phDF_tThresh0_B1.csv\",index_col=0)\n",
    "imgIndexOrd=list(phDF.index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "newThresh=[-8,25]\n",
    "pBounds=[-25,50]\n",
    "\n",
    "truncCols=[i for i in phDF.columns[4:] if newThresh[0]<=int(i.split(\"_\")[0])<=newThresh[1] and newThresh[0]<=int(i.split(\"_\")[1])<=newThresh[1]]\n",
    "\n",
    "phDFSub=phDF[list(phDF.columns[:4])+list(truncCols)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CubePHDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, property_excel_dir: str, phDF_dir: str,cifs: List[str] = None ,newThresh: List[int]=None,elimDiag=True,normalize=False ,shiftStats: List[Tuple[torch.FloatTensor]]=None):\n",
    "\n",
    "        if property_excel_dir[-3:]==\"csv\": #better for match: case if python > 3.10\n",
    "            propertyDF: pd.DataFrame = pd.read_csv(property_excel_dir,index_col=0)\n",
    "        elif property_excel_dir[-4:]==\"xlsx\":\n",
    "            propertyDF: pd.DataFrame = pd.read_excel(property_excel_dir,index_col=0)\n",
    "        if phDF_dir[-3:]==\"csv\":\n",
    "            phDF: pd.DataFrame = pd.read_csv(phDF_dir,index_col=0) #indexIsMOFs\n",
    "        elif phDF_dir[-4:]==\"xlsx\":\n",
    "            phDF: pd.DataFrame = pd.read_excel(phDF_dir,index_col=0) #indexIsMOFs\n",
    "        if cifs: ## You can either provide the general phDF + property_excel files, and a list of subset cifs\n",
    "            assert False not in {c in (set(phDF.index) & set(propertyDF.index)) for c in cifs}\n",
    "            self.cifs=cifs\n",
    "        else: ### or you can leave cifs as none and just take all overlapping MOFS in both input data files\n",
    "            self.cifs=list(set(phDF.index) & set(propertyDF.index)) #\n",
    "        self.property=torch.from_numpy((propertyDF.loc[self.cifs]).to_numpy(dtype=np.float32))\n",
    "\n",
    "        ### ADD NORMALIZATION OF PROPERTY\n",
    "        self.propCols=list(propertyDF.columns)\n",
    "        if normalize:\n",
    "            self.propStats={'workcap':(torch.mean(self.property[:,0]),torch.std(self.property[:,0])),'sel':(torch.mean(self.property[:,1]),torch.std(self.property[:,1]))}\n",
    "            propNew=self.property.clone()\n",
    "            propNew[:,0]=(propNew[:,0]-self.propStats['workcap'][0])/self.propStats['workcap'][1]\n",
    "            propNew[:,1]=(propNew[:,1]-self.propStats['sel'][0])/self.propStats['sel'][1]\n",
    "\n",
    "            self.shiftFac=(propNew[:,0].min(),propNew[:,1].min())\n",
    "            propNew[:,0]-=self.shiftFac[0]\n",
    "            propNew[:,1]-=self.shiftFac[1]\n",
    "            self.property=propNew.detach().clone()\n",
    "        elif shiftStats: ### [(mean,std,shiftFac),()]\n",
    "            self.shiftFac=(shiftStats[0][2],shiftStats[1][2])\n",
    "            self.propStats={'workcap':(shiftStats[0][0],shiftStats[0][1]),'sel':(shiftStats[1][0],shiftStats[1][1])} # in this case propStats does not accurately reflect the stats of this dataset, but of the standardization statistics provided\n",
    "            propNew=self.property.clone()\n",
    "            propNew[:,0]=(propNew[:,0]-self.propStats['workcap'][0])/self.propStats['workcap'][1]\n",
    "            propNew[:,1]=(propNew[:,1]-self.propStats['sel'][0])/self.propStats['sel'][1]\n",
    "\n",
    "            self.shiftFac=(propNew[:,0].min(),propNew[:,1].min())\n",
    "            propNew[:,0]-=self.shiftFac[0]\n",
    "            propNew[:,1]-=self.shiftFac[1]\n",
    "            self.property=propNew.detach().clone()\n",
    "\n",
    "\n",
    "        phDF=phDF.loc[self.cifs]\n",
    "\n",
    "        phVals={int(s[:s.find(\"_\")]) for s in phDF.columns if \"_\" in s}# assuming integer resolution for now\n",
    "        # this requires the columns to have the format b_d\n",
    "        phBounds=[min(phVals),max(phVals)]\n",
    "        self.phThresh=phBounds\n",
    "        if newThresh and len(newThresh)==2 and newThresh[0]>=self.phThresh[0] and  newThresh[1]<=self.phThresh[1] :\n",
    "            self.subsetPHImg(phDF,newThresh)\n",
    "        else:\n",
    "            self.subsetPHImg(phDF,self.phThresh)\n",
    "\n",
    "        if elimDiag:\n",
    "            self.elimDiag()\n",
    "\n",
    "\n",
    "\n",
    "    def subsetPHImg(self,phDF,newThresh: List[int]=[-8, 25],offset=4):\n",
    "\n",
    "        truncCols=[i for i in phDF.columns[offset:] if newThresh[0]<=int(i.split(\"_\")[0])<=newThresh[1] and newThresh[0]<=int(i.split(\"_\")[1])<=newThresh[1]]\n",
    "        phDFSub=phDF[list(phDF.columns[:offset])+list(truncCols)]\n",
    "        life=newThresh[1]-newThresh[0]\n",
    "\n",
    "        phImg=np.zeros((len(self.cifs),life+1,life+1))#,dtype='uint8')\n",
    "\n",
    "        for m in range(len(self.cifs)):\n",
    "            for b in np.arange(newThresh[0],newThresh[1]+1):\n",
    "                for d in np.arange(b,newThresh[1]+1):\n",
    "                    phImg[m,newThresh[1]-d,b-newThresh[0]]=phDFSub[f\"{b}_{d}\"].loc[self.cifs[m]]\n",
    "        self.phThresh=newThresh\n",
    "        self.phImg=torch.from_numpy(phImg).type(torch.float32)\n",
    "\n",
    "\n",
    "    def elimDiag(self): #allow persist thresh in future but really fast implementation for one pixel\n",
    "        sqlen=len(self.phImg[0])\n",
    "        imgClone=self.phImg.clone()\n",
    "        yrange=torch.arange(sqlen)\n",
    "        imgClone[:,yrange.__reversed__(),yrange]=torch.tensor(0).to(imgClone)\n",
    "        self.phImg=imgClone\n",
    "\n",
    "\n",
    "    #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"cif\":self.cifs[index],\n",
    "                \"image\": self.phImg[index],\n",
    "                \"workcap\": self.property[index][0].view(1,),\n",
    "                \"sel\": self.property[index][1].view(1,),\n",
    "                }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [45]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mCubePHDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproperty_excel_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43minDir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/post-combustion-vsa-2-clean.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mphDF_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43minDir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/phDF_tThresh0_B1.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewThresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [44]\u001B[0m, in \u001B[0;36mCubePHDataset.__init__\u001B[1;34m(self, property_excel_dir, phDF_dir, cifs, newThresh, elimDiag, normalize)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphThresh\u001B[38;5;241m=\u001B[39mphBounds\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m newThresh \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(newThresh)\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m newThresh[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphThresh[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m  newThresh[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphThresh[\u001B[38;5;241m1\u001B[39m] :\n\u001B[1;32m---> 38\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubsetPHImg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mphDF\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnewThresh\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubsetPHImg(phDF,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphThresh)\n",
      "Input \u001B[1;32mIn [44]\u001B[0m, in \u001B[0;36mCubePHDataset.subsetPHImg\u001B[1;34m(self, phDF, newThresh, offset)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcifs)):\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39marange(newThresh[\u001B[38;5;241m0\u001B[39m],newThresh[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 57\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnewThresh\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     58\u001B[0m             phImg[m,newThresh[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m-\u001B[39md,b\u001B[38;5;241m-\u001B[39mnewThresh[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m=\u001B[39mphDFSub[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mb\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00md\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcifs[m]]\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphThresh\u001B[38;5;241m=\u001B[39mnewThresh\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#dataset = CubePHDataset(property_excel_dir=f\"{inDir}/post-combustion-vsa-2-clean.csv\",\n",
    "#                       phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,25])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "MLinDir=f\"{inDir}/dataSplitML\"\n",
    "optVar=\"sel\"\n",
    "\n",
    "trainDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/trainCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23],normalize=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainDataset.shiftFac[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "testDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/testCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23],\n",
    "                          shiftStats=[(trainDataset.propStats['workcap'][0],trainDataset.propStats['workcap'][1],trainDataset.shiftFac[0]),(trainDataset.propStats['sel'][0],trainDataset.propStats['sel'][1],trainDataset.shiftFac[1])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "validDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/valCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23],\n",
    "                          shiftStats=[(trainDataset.propStats['workcap'][0],trainDataset.propStats['workcap'][1],trainDataset.shiftFac[0]),(trainDataset.propStats['sel'][0],trainDataset.propStats['sel'][1],trainDataset.shiftFac[1])])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    #print(batch)\n",
    "    #pd, image, cif = batch['pd'], batch['image'], batch['cif']\n",
    "    elem = batch[0]\n",
    "    assert isinstance(elem, collections.abc.Mapping)\n",
    "    #print(type(batch))\n",
    "\n",
    "    mapped_dict = {key: torch.stack([d[key] for d in batch], dim=0) for key in elem if key in ['pd', 'image', 'workcap', 'sel'] }\n",
    "    mapped_dict.update({key: [d[key] for d in batch] for key in elem if key not in ['pd', 'image', 'workcap', 'sel']})\n",
    "    #print(mapped_dict.keys())\n",
    "\n",
    "    image, cif, workcap, sel = mapped_dict['image'], mapped_dict['cif'], mapped_dict['workcap'], mapped_dict['sel']\n",
    "    #torch.stack([img0, img1, img2], dim=1)\n",
    "    return dict(image=image[:,None,...], cif=cif, workcap=workcap, sel=sel)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "{'cif': 'DB0-m3_o11_o12_f0_pcu.sym.14',\n 'image': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n 'workcap': tensor([0.4308]),\n 'sel': tensor([0.1126])}"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataset[5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "{'cif': 'DB12-NIMWUD_freeONLY',\n 'image': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n 'workcap': tensor([-0.0537]),\n 'sel': tensor([2.0369])}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[5000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1a349f2a370>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainDataset[5000]['image'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 32])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[5000]['image'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "MLinDir=f\"{inDir}/dataSplitML\"\n",
    "optVar=\"sel\"\n",
    "\n",
    "#trainDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/trainCombustionDiverseMOF.xlsx\",\n",
    "#                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainDataset, batch_size=2048, pin_memory=True, num_workers=0, shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "#testDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/testCombustionDiverseMOF.xlsx\",\n",
    "#                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "test_dataloader= torch.utils.data.DataLoader(testDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n",
    "\n",
    "#validDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/valCombustionDiverseMOF.xlsx\",\n",
    "#                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "valid_dataloader= torch.utils.data.DataLoader(validDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'image': tensor([[[[0., 0., 0.,  ..., 0., 1., 0.],\n           [0., 0., 0.,  ..., 9., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n         [[[0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n         [[[0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n         ...,\n \n \n         [[[0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n         [[[0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]],\n \n \n         [[[0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.],\n           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n 'cif': ['DB12-HEMTEZ_clean',\n  'DB12-MIHLIZ_freeONLY',\n  'DB12-PIRYOF_clean',\n  'DB12-LORROB_clean',\n  'DB12-OVIZOJ_clean',\n  'DB12-NAXLII_clean',\n  'DB5-hypotheticalMOF_5065444_1_1_2_15_16_8',\n  'DB12-WECBOX_clean',\n  'DB0-m2_o22_o27_f0_pcu.sym.43',\n  'DB5-hypotheticalMOF_32798_0_0_2_0_17_7',\n  'DB5-hypotheticalMOF_22901_0_0_1_13_17_7',\n  'DB12-FUFREE_clean',\n  'DB0-m9_o1_o22_f0_sra.sym.72',\n  'DB0-m3_o24_o27_f0_pcu.sym.15',\n  'DB1-Zn2O8N2-irmof6_A-irmof7_A_No524',\n  'DB0-m2_o7_o24_f0_pcu.sym.6',\n  'DB0-m2_o16_o20_f0_pcu.sym.72',\n  'DB0-m1_o2_o6_f0_pcu',\n  'DB12-QOSJUF_clean',\n  'DB0-m3_o14_o27_f0_pcu.sym.28',\n  'DB0-m3_o16_o152_f0_fsc.sym.20',\n  'DB1-Zn2O8N2-BDC_A-irmof14_A_No68',\n  'DB0-m9_o27_o27_f0_sra.sym.36',\n  'DB0-m3_o520_o22_f0_fsc.sym.55',\n  'DB5-hypotheticalMOF_33157_0_0_2_0_1_2',\n  'DB0-m2_o8_o29_f0_pcu.sym.52',\n  'DB7-ddmof_15280',\n  'DB0-m3_o4_o19_f0_nbo.sym.16',\n  'DB12-LUMZEA_clean',\n  'DB0-m3_o14_o21_f0_pcu.sym.23',\n  'DB5-hypotheticalMOF_21654_0_0_1_11_2_7',\n  'DB12-EDADET_clean',\n  'DB5-hypotheticalMOF_19434_1_0_1_24_21_12',\n  'DB15-pcu_N37_E32_opt',\n  'DB0-m3_o11_o24_f0_pcu.sym.64',\n  'DB15-pcu_N65_E10_opt',\n  'DB12-LUJPUC_freeONLY',\n  'DB0-m3_o12_o29_f0_pcu.sym.96',\n  'DB12-LASQON_clean',\n  'DB7-ddmof_3878',\n  'DB0-m2_o12_o19_f0_pcu.sym.70',\n  'DB12-ja512437u_si_003_freeONLY',\n  'DB0-m2_o1_o21_f0_pcu.sym.62',\n  'DB12-YOQLAT_clean',\n  'DB12-GEGGUW_clean',\n  'DB0-m3_o22_o27_f0_pcu.sym.46',\n  'DB0-m2_o24_o29_f0_pcu.sym.7',\n  'DB12-QOTWON_clean',\n  'DB12-RORRIB_clean',\n  'DB5-hypotheticalMOF_5077197_0_0_2_27_14_7',\n  'DB12-HUFYIS_clean',\n  'DB12-JUJTAL_clean',\n  'DB0-m3_o12_o23_f0_pcu.sym.80',\n  'DB0-m3_o24_o28_f0_pcu.sym.63',\n  'DB0-m3_o490_o22_f0_fsc.sym.19',\n  'DB12-TUWPIL_freeONLY',\n  'DB0-m3_o7_o24_f0_pcu.sym.58',\n  'DB12-ZOHBUV_clean',\n  'DB0-m3_o81_o12_f0_fsc.sym.4',\n  'DB1-Zn2O8N2-AZO_A-fum_A_No77',\n  'DB12-COKQAW_freeONLY',\n  'DB0-m3_o8_o22_f0_pcu.sym.32',\n  'DB12-UTEXIB_freeONLY',\n  'DB0-m3_o155_o147_f0_fsc.sym.9',\n  'DB5-hypotheticalMOF_3002121_1_0_2_34_34_6',\n  'DB12-EDECOG_freeONLY',\n  'DB0-m3_o540_o14_f0_fsc.sym.25',\n  'DB0-m2_o22_o23_f0_pcu.sym.2',\n  'DB12-MAYYAN_clean',\n  'DB0-m2_o8_o17_f0_pcu.sym.34',\n  'DB0-m2_o16_o20_f0_pcu.sym.61',\n  'DB12-ABEXUC_freeONLY',\n  'DB0-m3_o16_o24_f0_pcu.sym.35',\n  'DB12-QOPZIF_clean',\n  'DB0-m3_o530_o159_f0_fsc.sym.12',\n  'DB5-hypotheticalMOF_6000570_0_0_3_12_13_8',\n  'DB5-hypotheticalMOF_5072202_0_0_2_19_11_4',\n  'DB0-m3_o5_o11_f0_nbo.sym.20',\n  'DB0-m3_o24_o1530_f0_fsc.sym.46',\n  'DB0-m9_o6_o23_f0_sra.sym.50',\n  'DB5-hypotheticalMOF_3001046_1_1_1_34_34_4',\n  'DB12-ZADDAJ_clean',\n  'DB12-SALRON_clean',\n  'DB0-m3_o10_o19_f0_pcu.sym.21',\n  'DB0-m3_o10_o29_f0_pcu.sym.34',\n  'DB12-ADAXIO_freeONLY',\n  'DB12-WALBOC_clean',\n  'DB6-cuf_2029',\n  'DB0-m3_o12_o23_f0_pcu.sym.156',\n  'DB0-m2_o13_o27_f0_pcu.sym.89',\n  'DB15-pcu_N123_E51_opt',\n  'DB0-m3_o43_o22_f0_fsc.sym.68',\n  'DB5-hypotheticalMOF_5047287_1_0_1_19_11_6',\n  'DB5-hypotheticalMOF_5027993_2_1_0_11_11_8',\n  'DB12-HOGJIX_freeONLY',\n  'DB0-m2_o14_o25_f0_pcu.sym.32',\n  'DB12-GAJYUN_freeONLY',\n  'DB15-lon_N76_E24_opt',\n  'DB12-EXOZEX_clean',\n  'DB5-hypotheticalMOF_5059898_1_0_2_12_10_2',\n  'DB15-kag_N108_E132_opt',\n  'DB0-m3_o150_o22_f0_fsc.sym.101',\n  'DB0-m3_o12_o22_f0_pcu.sym.147',\n  'DB0-m3_o13_o22_f0_pcu.sym.35',\n  'DB12-CODSUL_clean',\n  'DB0-m3_o22_o27_f0_pcu.sym.54',\n  'DB12-DICVIW_clean',\n  'DB12-TIXVON_clean',\n  'DB0-m1_o15_o24_f0_pcu',\n  'DB12-RIYWED_clean',\n  'DB0-m3_o8_o20_f0_pcu.sym.85',\n  'DB1-Zn2O8N2-ADC_A-irmof10_A_No558',\n  'DB12-WALCET_freeONLY',\n  'DB12-JUQTOG_clean',\n  'DB5-hypotheticalMOF_5033922_0_0_1_12_7_2',\n  'DB12-GADRAH_clean',\n  'DB12-LULWUL_clean',\n  'DB5-hypotheticalMOF_5059647_1_1_2_12_24_2',\n  'DB5-hypotheticalMOF_5052048_0_0_1_27_11_7',\n  'DB15-sow_N76_E166_unopt',\n  'DB12-DILXED_freeONLY',\n  'DB12-RESWOC_clean',\n  'DB12-QUJFUX_clean',\n  'DB0-m2_o16_o21_f0_pcu.sym.16',\n  'DB0-m2_o3_o14_f0_pcu.sym.89',\n  'DB0-m3_o24_o28_f0_pcu.sym.24',\n  'DB0-m2_o14_o19_f0_pcu.sym.26',\n  'DB12-QUFFED_clean',\n  'DB0-m2_o11_o11_f0_pcu.sym.82',\n  'DB12-IWOSAP_clean',\n  'DB0-m3_o12_o17_f0_pcu.sym.62',\n  'DB12-WAQFIG_clean',\n  'DB0-m3_o10_o13_f0_pcu.sym.55',\n  'DB12-GESDUF_freeONLY',\n  'DB12-jacs.5b12516_ja5b12516_si_002_clean',\n  'DB0-m5_o12_o22_f0_bcu.sym.12',\n  'DB12-DAMZEZ_clean',\n  'DB0-m2_o10_o11_f0_pcu.sym.25',\n  'DB5-hypotheticalMOF_5071074_0_0_2_3_3_11',\n  'DB12-NOSTUM_freeONLY',\n  'DB0-m3_o153_o22_f0_fsc.sym.73',\n  'DB12-PAMGIV_clean',\n  'DB0-m3_o22_o26_f0_pcu.sym.80',\n  'DB0-m2_o7_o25_f0_pcu.sym.22',\n  'DB0-m2_o3_o15_f0_pcu.sym.42',\n  'DB0-m2_o16_o21_f0_pcu.sym.5',\n  'DB12-AHUTIH01_freeONLY',\n  'DB12-EPEXII_freeONLY',\n  'DB1-Cu2O8N2-DPAC_A-irmof6_A_No529',\n  'DB0-m3_o150_o14_f0_fsc.sym.36',\n  'DB0-m3_o15_o16_f0_pcu.sym.36',\n  'DB0-m2_o14_o25_f0_pcu.sym.105',\n  'DB0-m3_o113_o22_f0_fsc.sym.6',\n  'DB12-TOLGOR_clean',\n  'DB12-COXHED_clean',\n  'DB0-m2_o10_o11_f0_pcu.sym.81',\n  'DB0-m2_o18_o29_f0_pcu.sym.15',\n  'DB0-m2_o12_o16_f0_pcu.sym.23',\n  'DB0-m3_o11_o17_f0_pcu.sym.20',\n  'DB5-hypotheticalMOF_5077352_1_0_2_27_22_5',\n  'DB1-Cu2O8N2-BDC_A-fum_A_No35',\n  'DB5-hypotheticalMOF_5056515_3_0_1_29_17_0',\n  'DB0-m3_o11_o12_f0_pcu.sym.26',\n  'DB0-m3_o8_o23_f0_pcu.sym.162',\n  'DB5-hypotheticalMOF_5073972_0_0_2_23_2_14',\n  'DB13-dia-Syn033060',\n  'DB0-m2_o24_o27_f0_pcu.sym.57',\n  'DB0-m2_o11_o25_f0_pcu.sym.23',\n  'DB0-m3_o13_o23_f0_pcu.sym.31',\n  'DB5-hypotheticalMOF_23203_2_0_1_13_21_5',\n  'DB0-m3_o6_o17_f0_pcu.sym.106',\n  'DB12-KIPKEB_freeONLY',\n  'DB0-m3_o22_o27_f0_pcu.sym.49',\n  'DB12-PEGPUO_clean',\n  'DB0-m3_o12_o21_f0_pcu.sym.22',\n  'DB12-LIQDAS_clean',\n  'DB12-HAPNOE_clean',\n  'DB12-RESSAK_clean',\n  'DB15-kag_N65_E151_opt',\n  'DB0-m23_o1_o58_f0_pcu',\n  'DB12-AQODOA_clean',\n  'DB0-m3_o16_o152_f0_fsc.sym.16',\n  'DB12-VIRVEY_clean',\n  'DB0-m2_o2_o25_f0_pcu.sym.107',\n  'DB0-m2_o10_o23_f0_pcu.sym.72',\n  'DB0-m3_o10_o25_f0_pcu.sym.64',\n  'DB12-RUPTED_freeONLY',\n  'DB7-ddmof_5612',\n  'DB12-IXAKAU_clean',\n  'DB0-m3_o510_o22_f0_fsc.sym.7',\n  'DB0-m2_o12_o17_f0_pcu.sym.15',\n  'DB1-Zn2O8N2-AZO_A-irmof14_A_No351',\n  'DB0-m15_o14_o146_f0_fsc',\n  'DB7-ddmof_21082',\n  'DB12-RUBHOM_freeONLY',\n  'DB12-BOLPAU_clean',\n  'DB12-XUYDIF_freeONLY',\n  'DB0-m2_o17_o25_f0_pcu.sym.15',\n  'DB0-m2_o2_o6_f0_pcu.sym.45',\n  'DB1-Cu2O8N2-ADC_A-irmof20_A_No679',\n  'DB6-cuf_4540',\n  'DB0-m3_o12_o25_f0_pcu.sym.29',\n  'DB0-m3_o14_o151_f0_fsc.sym.4',\n  'DB0-m2_o25_o25_f0_pcu.sym.18',\n  'DB12-SONTAR_manual',\n  'DB7-ddmof_21656',\n  'DB12-LAGNUE_clean',\n  'DB0-m3_o22_o47_f0_fsc.sym.35',\n  'DB15-pcu_N47_E174_opt',\n  'DB0-m3_o19_o24_f0_pcu',\n  'DB0-m2_o24_o27_f0_pcu.sym.29',\n  'DB0-m2_o4_o10_f0_pcu',\n  'DB0-m2_o22_o24_f0_pcu.sym.9',\n  'DB12-BUGSIG_clean',\n  'DB5-hypotheticalMOF_5061882_1_0_2_18_24_4',\n  'DB12-MASMIC_clean',\n  'DB12-LEMHAM_clean',\n  'DB7-ddmof_16243',\n  'DB0-m3_o12_o23_f0_pcu.sym.109',\n  'DB0-m2_o3_o10_f0_pcu.sym.4',\n  'DB0-m3_o24_o150_f0_fsc.sym.96',\n  'DB0-m23_o150_o153_f0_pcu',\n  'DB15-qtz-e_N65_E27_opt',\n  'DB12-EXEQAA_freeONLY',\n  'DB12-c6cc02494g_c6cc02494g5_clean',\n  'DB0-m9_o11_o11_f0_sra.sym.36',\n  'DB12-KAFYAT_clean',\n  'DB0-m2_o10_o21_f0_pcu.sym.21',\n  'DB0-m3_o96_o12_f0_fsc.sym.29',\n  'DB5-hypotheticalMOF_11699_1_1_0_5_2_7',\n  'DB0-m3_o12_o13_f0_pcu.sym.13',\n  'DB0-m9_o11_o24_f0_sra.sym.46',\n  'DB12-FOJLOH_clean',\n  'DB0-m3_o11_o12_f0_pcu.sym.30',\n  'DB0-m3_o10_o23_f0_pcu.sym.81',\n  'DB0-m3_o12_o19_f0_pcu.sym.72',\n  'DB0-m3_o43_o4_f0_fsc.sym.9',\n  'DB12-CUWNIT_clean',\n  'DB0-m3_o18_o146_f0_fsc.sym.67',\n  'DB12-MAKSUO_clean',\n  'DB0-m3_o11_o20_f0_pcu.sym.9',\n  'DB0-m2_o13_o21_f0_pcu.sym.9',\n  'DB15-pcu_N65_E137_opt',\n  'DB0-m3_o12_o24_f0_pcu.sym.77',\n  'DB5-hypotheticalMOF_3000997_1_1_1_34_34_2',\n  'DB0-m18_o22_o51_f0_fsc',\n  'DB12-HEBKEG_freeONLY',\n  'DB5-hypotheticalMOF_1000810_0_0_3_4_25_6',\n  'DB12-ZEDHIB_freeONLY',\n  'DB13-acs-Syn032864',\n  'DB5-hypotheticalMOF_5068391_0_0_2_20_11_8',\n  'DB0-m3_o10_o19_f0_pcu.sym.27',\n  'DB0-m3_o27_o27_f0_pcu.sym.107',\n  'DB7-ddmof_15425',\n  'DB12-CIGYAU_freeONLY',\n  'DB12-FOHQUO_clean',\n  'DB5-hypotheticalMOF_5055125_0_0_1_28_14_5',\n  'DB0-m3_o16_o25_f0_pcu.sym.49',\n  'DB6-cuf_8218',\n  'DB12-ROQDIM04_freeONLY',\n  'DB12-PENNUT_freeONLY',\n  'DB5-hypotheticalMOF_1002702_0_0_3_5_1_6',\n  'DB7-ddmof_4951',\n  'DB0-m2_o31_o28_ins',\n  'DB0-m23_o150_o58_f0_pcu',\n  'DB0-m3_o96_o11_f0_fsc.sym.47',\n  'DB0-m3_o12_o16_f0_pcu.sym.98',\n  'DB0-m3_o11_o17_f0_pcu.sym.90',\n  'DB0-m3_o14_o23_f0_pcu.sym.4',\n  'DB0-m2_o11_o27_f0_pcu.sym.82',\n  'DB0-m2_o12_o14_f0_pcu.sym.25',\n  'DB0-m2_o16_o28_f0_pcu.sym.43',\n  'DB0-m18_o22_o22_f0_pcu',\n  'DB12-HEBJAB_clean',\n  'DB15-pcu_N123_E201_opt',\n  'DB12-TARVOX_clean',\n  'DB0-m2_o24_o24_f0_pcu.sym.23',\n  'DB0-m3_o12_o23_f0_pcu.sym.129',\n  'DB0-m3_o9_o28_f0_pcu.sym.61',\n  'DB0-m2_o24_o26_f0_pcu.sym.105',\n  'DB12-AHUTIH01_clean',\n  'DB12-BINDAF_manual',\n  'DB12-FAHTUE_freeONLY',\n  'DB12-CAKYAQ_clean',\n  'DB0-m3_o160_o45_f0_fsc.sym.22',\n  'DB0-m2_o22_o25_f0_pcu.sym.63',\n  'DB0-m2_o12_o14_f0_pcu.sym.23',\n  'DB12-KIYQIU_clean',\n  'DB5-hypotheticalMOF_1001177_0_0_3_10_10_12',\n  'DB0-m3_o8_o22_f0_pcu.sym.27',\n  'DB5-hypotheticalMOF_11793_0_0_0_5_21_7',\n  'DB0-m3_o16_o17_f0_pcu.sym.25',\n  'DB5-hypotheticalMOF_5078093_1_0_2_27_3_7',\n  'DB0-m3_o3_o19_f0_pcu.sym.22',\n  'DB0-m2_o11_o11_f0_pcu.sym.30',\n  'DB12-BEXSII_clean',\n  'DB12-HURGEH_freeONLY',\n  'DB12-DOHYIJ01_freeONLY',\n  'DB0-m3_o3_o21_f0_pcu.sym.35',\n  'DB0-m23_o21_o151_f0_pcu',\n  'DB0-m3_o99_o22_f0_fsc.sym.24',\n  'DB0-m2_o8_o14_f0_pcu.sym.92',\n  'DB12-KOSJOT_clean',\n  'DB12-CAYSOL_clean',\n  'DB0-m2_o10_o19_f0_pcu.sym.22',\n  'DB12-OPENIH_freeONLY',\n  'DB0-m3_o1_o115_f0_fsc.sym.10',\n  'DB0-m2_o11_o25_f0_pcu.sym.32',\n  'DB0-m3_o3_o21_f0_pcu.sym.20',\n  'DB12-NOFQII_clean',\n  'DB0-m3_o22_o47_f0_fsc.sym.31',\n  'DB0-m2_o12_o25_f0_pcu.sym.70',\n  'DB12-OPENUT_clean',\n  'DB0-m18_o19_o22_f0_pcu',\n  'DB0-m2_o89_o155_fsc_0.1_SO3H',\n  'DB1-Zn2O8N2-BDC_A-irmof7_A_No8',\n  'DB15-smu_N123_E27_opt',\n  'DB0-m15_o15_o46_f0_fsc',\n  'DB0-m15_o23_o50_f0_fsc',\n  'DB12-CUBHOY_freeONLY',\n  'DB0-m2_o12_o13_f0_pcu.sym.17',\n  'DB0-m3_o146_o22_f0_fsc.sym.17',\n  'DB12-EQOCES01_freeONLY',\n  'DB0-m3_o24_o45_f0_fsc.sym.11',\n  'DB0-m3_o96_o11_f0_fsc.sym.19',\n  'DB0-m3_o156_o510_f0_fsc.sym.48',\n  'DB0-m3_o160_o83_f0_fsc.sym.12',\n  'DB12-ILOSOS_freeONLY',\n  'DB0-m3_o1490_o155_f0_fsc.sym.16',\n  'DB15-pcu_N108_E148_opt',\n  'DB6-pMOF_155',\n  'DB12-FAVGER_freeONLY',\n  'DB5-hypotheticalMOF_5014827_0_0_0_3_11_13',\n  'DB12-XOVYUD_freeONLY',\n  'DB0-m3_o112_o22_f0_fsc.sym.2',\n  'DB0-m23_o20_o58_f0_pcu',\n  'DB0-m3_o18_o22_f0_pcu.sym.72',\n  'DB0-m3_o1_o16_f0_pcu.sym.87',\n  'DB0-m3_o7_o19_f0_pcu.sym.79',\n  'DB0-m3_o10_o16_f0_pcu.sym.15',\n  'DB12-GEDSOZ_clean',\n  'DB0-m3_o4_o5_f0_pcu.sym.103',\n  'DB5-hypotheticalMOF_5044854_0_0_1_3_25_14',\n  'DB12-KIPKEB_clean',\n  'DB12-GIWNUV_clean',\n  'DB0-m3_o8_o19_f0_pcu.sym.130',\n  'DB1-Zn2O8N2-irmof10_A-irmof7_A_No547',\n  'DB7-ddmof_21132',\n  'DB12-cg200615s_si_002_clean',\n  'DB5-hypotheticalMOF_5038057_0_0_1_18_16_8',\n  'DB12-WOTSEF_clean',\n  'DB12-YARYUL_clean',\n  'DB0-m3_o520_o22_f0_fsc.sym.79',\n  'DB12-MOCJEU_clean',\n  'DB12-ZIDDIB_clean',\n  'DB0-m3_o18_o23_f0_pcu.sym.82',\n  'DB0-m3_o52_o14_f0_fsc.sym.64',\n  'DB0-m3_o11_o24_f0_pcu.sym.81',\n  'DB5-hypotheticalMOF_36378_0_0_2_16_2_11',\n  'DB0-m3_o18_o22_f0_pcu.sym.83',\n  'DB0-m2_o11_o21_f0_pcu.sym.96',\n  'DB5-hypotheticalMOF_5061813_1_1_2_18_24_2',\n  'DB0-m3_o10_o11_f0_pcu.sym.3',\n  'DB7-ddmof_3313',\n  'DB12-FOHCAH_clean',\n  'DB0-m3_o11_o13_f0_pcu.sym.87',\n  'DB0-m3_o16_o23_f0_pcu.sym.80',\n  'DB0-m3_o78_o22_f0_fsc.sym.13',\n  'DB0-m3_o11_o24_f0_pcu.sym.3',\n  'DB0-m3_o99_o22_f0_fsc.sym.43',\n  'DB0-m3_o24_o1530_f0_fsc.sym.19',\n  'DB0-m3_o10_o26_f0_pcu.sym.20',\n  'DB12-WOJZOM_clean',\n  'DB0-m2_o11_o22_f0_pcu.sym.61',\n  'DB5-hypotheticalMOF_5001367_1_0_0_12_1_7',\n  'DB5-hypotheticalMOF_5053564_1_1_1_27_19_6',\n  'DB12-WUXLEI_clean',\n  'DB12-OKABAE_clean',\n  'DB0-m2_o22_o25_f0_pcu.sym.95',\n  'DB12-KEDJAG26_clean',\n  'DB0-m2_o24_o29_f0_pcu.sym.4',\n  'DB12-HUSJUC_clean',\n  'DB0-m2_o12_o20_f0_pcu.sym.70',\n  'DB0-m3_o105_o22_f0_fsc.sym.23',\n  'DB12-ATEYED_freeONLY',\n  'DB5-hypotheticalMOF_5014344_1_0_0_3_25_4',\n  'DB12-CAKYIY_clean',\n  'DB5-hypotheticalMOF_5046139_0_0_1_3_3_13',\n  'DB0-m2_o12_o28_f0_pcu.sym.49',\n  'DB0-m3_o24_o500_f0_fsc.sym.47',\n  'DB12-MAWTUC_clean',\n  'DB13-dia-Syn038606',\n  'DB0-m3_o24_o29_f0_pcu.sym.80',\n  'DB12-RUVBOB_freeONLY',\n  'DB13-lvt-Syn034313',\n  'DB12-XESKEN_SL',\n  'DB5-hypotheticalMOF_5034948_1_0_1_11_11_5',\n  'DB12-PAPVAF_clean',\n  'DB0-m2_o18_o19_f0_pcu.sym.31',\n  'DB0-m2_o8_o24_f0_pcu.sym.63',\n  'DB1-Cu2O8N2-AZO_A-DPAC_A_No256',\n  'DB0-m3_o8_o23_f0_pcu.sym.29',\n  'DB0-m3_o11_o20_f0_pcu.sym.3',\n  'DB5-hypotheticalMOF_5039238_1_1_1_15_25_4',\n  'DB5-hypotheticalMOF_5061714_1_0_2_18_25_5',\n  'DB12-TUNJET_clean',\n  'DB0-m3_o1490_o22_f0_fsc.sym.19',\n  'DB0-m3_o19_jsd',\n  'DB12-SARTIR_clean',\n  'DB5-hypotheticalMOF_5043022_1_0_1_20_25_3',\n  'DB0-m23_o19_o153_f0_pcu',\n  'DB0-m2_o22_o25_f0_pcu.sym.33',\n  'DB0-m3_o10_o25_f0_pcu.sym.16',\n  'DB12-QOXNIB_freeONLY',\n  'DB12-SOJGOO_clean',\n  'DB12-WECSAZ_clean',\n  'DB1-Cu2O8N2-AZO_A-irmof14_A_No65',\n  'DB12-EKARUE_clean',\n  'DB12-RACZEC_clean',\n  'DB0-m2_o22_o25_f0_pcu.sym.76',\n  'DB5-hypotheticalMOF_5047698_1_1_1_19_14_0',\n  'DB12-SERJOP_clean',\n  'DB0-m2_o11_o12_f0_pcu.sym.87',\n  'DB0-m1_o24_o25_f0_pcu',\n  'DB12-ASUWUI_freeONLY',\n  'DB5-hypotheticalMOF_28225_2_1_2_21_9_3',\n  'DB5-hypotheticalMOF_5054102_0_0_1_28_7_5',\n  'DB15-qtz_N76_E48_opt',\n  'DB12-WEMBUN_freeONLY',\n  'DB0-m3_o153_o22_f0_fsc.sym.27',\n  'DB12-PUWCUF_clean',\n  'DB15-pcu_N144_E10_opt',\n  'DB12-BEXSII_freeONLY',\n  'DB5-hypotheticalMOF_5059964_1_0_2_12_10_6',\n  'DB0-m3_o540_o22_f0_fsc.sym.43',\n  'DB12-SUTJIC_clean',\n  'DB12-NEYYUN_clean',\n  'DB0-m3_o52_o22_f0_fsc.sym.10',\n  'DB12-OFUDUQ_clean',\n  'DB12-DUXKUD_clean',\n  'DB5-hypotheticalMOF_5060622_0_0_2_12_14_10',\n  'DB0-m3_o11_o18_f0_pcu.sym.73',\n  'DB12-ncomms7350-s2_freeONLY',\n  'DB12-HETZOW_clean',\n  'DB0-m3_o470_o22_f0_fsc.sym.16',\n  'DB15-pcu_N123_E141_opt',\n  'DB12-ZAVQAQ_clean',\n  'DB0-m3_o24_o153_f0_fsc.sym.88',\n  'DB0-m3_o2_o24_f0_pcu.sym.51',\n  'DB0-m3_o13_o149_f0_fsc.sym.21',\n  'DB6-cuf_10095',\n  'DB0-m3_o12_o17_f0_pcu.sym.85',\n  'DB0-m3_o16_o1530_f0_fsc.sym.18',\n  'DB0-m2_o12_o21_f0_pcu.sym.22',\n  'DB12-ETESIG_clean',\n  'DB0-m3_o152_o155_f0_fsc.sym.9',\n  'DB0-m3_o7_o19_f0_pcu.sym.4',\n  'DB12-WESYAV_clean',\n  'DB12-GIMSUS_freeONLY',\n  'DB0-m3_o17_o93_fsc_0.214285714286_SO3H',\n  'DB0-m3_o12_o14_f0_pcu.sym.13',\n  'DB15-pcu_N123_E36_opt',\n  'DB15-qtz-x_N65_E204_opt',\n  'DB12-NIJVAF_freeONLY',\n  'DB0-m18_o25_o42_f0_fsc',\n  'DB0-m9_o13_o14_f0_sra.sym.56',\n  'DB13-pcu-Syn033538',\n  'DB12-HGCNCO_clean',\n  'DB15-pcu_N139_E162_opt',\n  'DB0-m3_o146_o156_f0_fsc.sym.26',\n  'DB12-XEJXER_clean',\n  'DB0-m3_o24_o150_f0_pcu',\n  'DB0-m3_o11_o22_f0_pcu.sym.5',\n  'DB12-jz4002345_si_006_freeONLY',\n  'DB0-m2_o12_o21_f0_pcu.sym.8',\n  'DB0-m3_o8_o23_f0_pcu.sym.27',\n  'DB0-m2_o10_o27_f0_pcu.sym.37',\n  'DB0-m1_o4_o23_f0_pcu',\n  'DB12-GEVKID_clean',\n  'DB0-m3_o24_o520_f0_fsc.sym.29',\n  'DB0-m3_o2_o2_f0_pcu.sym.26',\n  'DB5-hypotheticalMOF_5064426_1_0_2_15_24_3',\n  'DB0-m3_o146_o22_f0_fsc.sym.15',\n  'DB5-hypotheticalMOF_5059987_1_1_2_12_10_11',\n  'DB0-m2_o8_o16_f0_pcu.sym.71',\n  'DB12-NAQREE_clean',\n  'DB0-m3_o12_o22_f0_pcu.sym.3',\n  'DB0-m2_o3_o3_f0_pcu.sym.57',\n  'DB0-m3_o10_o20_f0_pcu.sym.12',\n  'DB5-hypotheticalMOF_5076650_0_0_2_27_1_12',\n  'DB0-m3_o16_o25_f0_pcu.sym.60',\n  'DB0-m3_o10_o23_f0_pcu.sym.42',\n  'DB12-ILUJIJ_freeONLY',\n  'DB12-KEGZOL02_freeONLY',\n  'DB5-hypotheticalMOF_5064676_0_0_2_15_10_10',\n  'DB0-m2_o14_o23_f0_pcu.sym.80',\n  'DB1-Zn2O8N2-irmof16_A-irmof8_A_No330',\n  'DB12-RUFMUA01_clean',\n  'DB0-m2_o2_o13_f0_pcu.sym.13',\n  'DB5-hypotheticalMOF_5015522_3_2_0_3_12_6',\n  'DB0-m2_o22_o23_f0_pcu.sym.15',\n  'DB5-hypotheticalMOF_5041776_0_0_1_8_10_11',\n  'DB0-m3_o8_o11_f0_pcu.sym.39',\n  'DB0-m3_o3_o11_f0_pcu.sym.99',\n  'DB12-DUTKAG_clean',\n  'DB12-BEPNUH_freeONLY',\n  'DB0-m2_o14_o19_f0_pcu.sym.27',\n  'DB5-hypotheticalMOF_5062412_0_0_2_18_11_8',\n  'DB12-GIYTAK_clean',\n  'DB1-Cu2O8N2-irmof20_A-irmof6_A_No283',\n  'DB12-SOCKOM_clean',\n  'DB12-PUJJEL_clean',\n  'DB5-hypotheticalMOF_5037216_1_0_1_18_11_10',\n  'DB5-hypotheticalMOF_5001651_2_2_0_12_10_8',\n  'DB12-RUFRIV_freeONLY',\n  'DB12-LOXWAY_freeONLY',\n  'DB0-m2_o16_o25_f0_pcu.sym.83',\n  'DB0-m2_o13_o14_f0_pcu.sym.102',\n  'DB0-m3_o150_o22_f0_fsc.sym.7',\n  'DB12-TETVIZ_clean',\n  'DB0-m3_o160_o96_f0_fsc.sym.61',\n  'DB7-ddmof_21823',\n  'DB12-SAJLOH_clean',\n  'DB0-m9_o1_o4_f0_sra.sym.55',\n  'DB12-YEGTEK_clean',\n  'DB7-ddmof_21815',\n  'DB0-m2_o26_o26_f0_pcu.sym.47',\n  'DB5-hypotheticalMOF_5072650_0_0_2_19_14_8',\n  'DB12-GUPREQ_clean',\n  'DB0-m3_o21_brl',\n  'DB0-m2_o23_o42_f0_fsc',\n  'DB12-VIWKIX_clean',\n  'DB0-m23_o22_o153_f0_pcu',\n  'DB0-m2_o17_o24_f0_pcu.sym.64',\n  'DB0-m2_o22_o26_f0_pcu.sym.50',\n  'DB12-FOQPAE_freeONLY',\n  'DB0-m3_o11_o25_f0_pcu.sym.87',\n  'DB0-m3_o11_o28_f0_pcu.sym.81',\n  'DB0-m3_o12_o23_f0_pcu.sym.163',\n  'DB12-SOZRUW_clean',\n  'DB0-m3_o8_o18_f0_pcu.sym.113',\n  'DB12-DAHNOQ_freeONLY',\n  'DB12-BEXPAX_clean',\n  'DB12-QUXREI_clean',\n  'DB13-nbo-Syn031501',\n  'DB12-VONBIK_clean',\n  'DB0-m3_o13_o24_f0_pcu.sym.40',\n  'DB12-UTOTUT01_freeONLY',\n  'DB0-m2_o22_o22_f0_pcu.sym.36',\n  'DB5-hypotheticalMOF_24818_0_0_1_5_11_12',\n  'DB12-MAJHUC_clean',\n  'DB0-m3_o22_o58_dme',\n  'DB5-hypotheticalMOF_5076814_0_0_2_27_11_5',\n  'DB0-m2_o13_o21_f0_pcu.sym.8',\n  'DB12-XUGDOU_clean',\n  'DB5-hypotheticalMOF_37598_0_0_2_22_13_6',\n  'DB0-m3_o11_o13_f0_pcu.sym.34',\n  'DB12-TUGFAE_clean',\n  'DB0-m3_o8_o8_f0_pcu.sym.18',\n  'DB0-m2_o12_o25_f0_pcu.sym.6',\n  'DB0-m3_o13_o23_f0_pcu.sym.19',\n  'DB7-ddmof_2867',\n  'DB12-OWIHUZ_clean',\n  'DB12-DADLIG_freeONLY',\n  'DB0-m1_o24_o31_ith-d',\n  'DB12-WOXKOL_clean',\n  'DB6-cuf_2063',\n  'DB0-m2_o8_o8_f0_pcu.sym.28',\n  'DB0-m9_o2_o2_f0_sra.sym.26',\n  'DB0-m3_o24_o29_f0_pcu.sym.46',\n  'DB0-m18_o22_o77_f0_fsc',\n  'DB5-hypotheticalMOF_5039647_0_0_1_15_10_11',\n  'DB0-m3_o11_o26_f0_pcu.sym.63',\n  'DB5-hypotheticalMOF_5176_3_1_0_1_21_4',\n  'DB1-Cu2O8N2-BDC_A-irmof20_A_No374',\n  'DB1-Zn2O8N2-irmof8_A-TePM_No469',\n  'DB0-m2_o33_o22_jea',\n  'DB12-DUBKAO_freeONLY',\n  'DB0-m3_o24_o430_f0_fsc.sym.25',\n  'DB7-ddmof_22118',\n  'DB5-hypotheticalMOF_5037270_2_1_1_18_11_6',\n  'DB5-hypotheticalMOF_5023242_1_0_0_27_2_6',\n  'DB12-NARNOM_clean',\n  'DB0-m3_o440_o17_f0_fsc.sym.8',\n  'DB12-DITTEH_clean',\n  'DB12-KIYBIF_freeONLY',\n  'DB12-AFUPEX_clean',\n  'DB12-VASKOR_freeONLY',\n  'DB0-m3_o10_o19_f0_pcu.sym.7',\n  'DB5-hypotheticalMOF_5047004_2_1_1_19_24_4',\n  'DB12-XOKHAH_freeONLY',\n  'DB0-m3_o7_o21_f0_pcu.sym.98',\n  'DB12-PEGQID_clean',\n  'DB0-m2_o24_o25_f0_pcu.sym.4',\n  'DB0-m2_o10_o10_f0_pcu.sym.13',\n  'DB0-m3_o4_o25_f0_pcu.sym.33',\n  'DB12-HOBGAH_freeONLY',\n  'DB0-m3_o150_o6_f0_fsc.sym.25',\n  'DB1-ZIFZn-BDC_A-DPAC_A_No521',\n  'DB0-m3_o12_o27_f0_pcu.sym.6',\n  'DB12-MUNDOQ_freeONLY',\n  'DB0-m3_o12_o18_f0_pcu.sym.6',\n  'DB0-m2_o12_o24_f0_pcu.sym.32',\n  'DB0-m3_o96_o11_f0_fsc.sym.13',\n  'DB0-m15_o2_o30_jea',\n  'DB5-hypotheticalMOF_5070377_0_0_2_3_14_0',\n  'DB0-m3_o460_o13_f0_fsc.sym.41',\n  'DB0-m2_o7_o19_f0_pcu.sym.56',\n  'DB12-UVAROZ_freeONLY',\n  'DB15-snp_N123_E57_opt',\n  'DB0-m2_o11_o26_f0_pcu.sym.76',\n  'DB0-m3_o11_o12_f0_pcu.sym.10',\n  'DB0-m3_o22_o26_f0_pcu.sym.4',\n  'DB12-RUYVAK_clean',\n  'DB0-m23_o27_o151_f0_pcu',\n  'DB12-ESIFET_clean',\n  'DB12-MUFJAZ01_clean',\n  'DB0-m3_o16_o25_f0_pcu.sym.57',\n  'DB12-CAZFOA_clean',\n  'DB12-APAYUN_clean',\n  'DB0-m18_o4_o20_f0_pcu',\n  'DB12-MIXYID_freeONLY',\n  'DB0-m2_o1_o5_f0_pcu.sym.23',\n  'DB0-m2_o13_o22_f0_pcu.sym.10',\n  'DB0-m3_o24_o147_f0_fsc.sym.29',\n  'DB12-HELDOT_freeONLY',\n  'DB12-IKETOI_clean',\n  'DB5-hypotheticalMOF_5039722_0_0_1_15_11_7',\n  'DB5-hypotheticalMOF_28050_0_0_2_2_17_12',\n  'DB0-m2_o9_o11_f0_pcu.sym.12',\n  'DB0-m2_o11_o27_f0_pcu.sym.5',\n  'DB0-m5_o37_o37_flu',\n  'DB0-m2_o13_o17_f0_pcu.sym.23',\n  'DB0-m3_o11_o21_f0_pcu.sym.58',\n  'DB0-m2_o14_o27_f0_pcu.sym.30',\n  'DB0-m3_o16_o500_f0_fsc.sym.16',\n  'DB5-hypotheticalMOF_5045956_1_1_1_3_15_6',\n  'DB5-hypotheticalMOF_36791_0_0_2_16_11_7',\n  'DB12-ZISYAD_freeONLY',\n  'DB0-m3_o16_o22_f0_pcu.sym.19',\n  'DB0-m2_o1_o7_f0_pcu.sym.21',\n  'DB15-pcu_N65_E101_opt',\n  'DB12-AROFET_clean',\n  'DB0-m23_o8_o11_f0_pcu',\n  'DB0-m3_o16_o19_f0_pcu.sym.53',\n  'DB12-FIJDUY_clean',\n  'DB12-GAMXAV_clean',\n  'DB0-m3_o24_o540_f0_fsc.sym.32',\n  'DB0-m2_o24_o24_f0_pcu.sym.22',\n  'DB0-m2_o10_o19_f0_pcu.sym.2',\n  'DB12-DUKYOZ_freeONLY',\n  'DB0-m2_o12_o20_f0_pcu.sym.7',\n  'DB0-m2_o24_o26_f0_pcu.sym.102',\n  'DB0-m3_o150_o23_f0_fsc.sym.128',\n  'DB12-XEDPIH_freeONLY',\n  'DB12-KUVMIZ_clean',\n  'DB12-TOVHUJ_clean',\n  'DB0-m3_o4_o22_f0_pcu.sym.62',\n  'DB0-m3_o12_o29_f0_pcu.sym.81',\n  'DB7-ddmof_22617',\n  'DB12-UMIZEX_clean',\n  'DB5-hypotheticalMOF_5069340_0_0_2_3_9_10',\n  'DB5-hypotheticalMOF_5053396_0_0_1_27_3_10',\n  'DB0-m3_o24_o149_f0_fsc.sym.32',\n  'DB7-ddmof_23799',\n  'DB0-m3_o11_o12_f0_pcu.sym.114',\n  'DB12-TARVUD_clean',\n  'DB12-XALDIY_clean',\n  'DB0-m3_o1480_o4_f0_fsc.sym.4',\n  'DB0-m2_o12_o22_f0_pcu.sym.29',\n  'DB0-m3_o12_o16_f0_pcu.sym.99',\n  'DB0-m3_o17_o25_f0_pcu.sym.106',\n  'DB6-cuf_589',\n  'DB0-m23_o20_o21_f0_pcu',\n  'DB0-m3_o18_o150_f0_fsc.sym.19',\n  'DB5-hypotheticalMOF_5040121_2_0_1_15_13_8',\n  'DB0-m3_o12_o27_f0_pcu.sym.19',\n  'DB0-m3_o16_o151_f0_fsc.sym.53',\n  'DB0-m3_o1470_o159_f0_fsc.sym.34',\n  'DB12-RUSSAA_clean',\n  'DB0-m3_o12_o22_f0_pcu.sym.17',\n  'DB12-CAVREY_freeONLY',\n  'DB0-m3_o11_o21_f0_pcu',\n  'DB0-m2_o16_o28_f0_pcu.sym.39',\n  'DB7-ddmof_22821',\n  'DB12-YUZRES_clean',\n  'DB0-m2_o17_o29_f0_pcu.sym.49',\n  'DB0-m9_o2_o20_f0_sra.sym.92',\n  'DB0-m3_o10_o19_f0_pcu.sym.13',\n  'DB0-m3_o24_o520_f0_fsc.sym.28',\n  'DB5-hypotheticalMOF_5071341_1_0_2_19_26_7',\n  'DB5-hypotheticalMOF_1959_1_1_0_21_2_11',\n  'DB5-hypotheticalMOF_6002484_0_0_3_20_24_11',\n  'DB12-QOVWOP_freeONLY',\n  'DB7-ddmof_23668',\n  'DB0-m3_o520_o22_f0_fsc.sym.32',\n  'DB15-qzd_N87_E162_opt',\n  'DB0-m2_o16_o16_f0_pcu.sym.22',\n  'DB7-ddmof_232',\n  'DB5-hypotheticalMOF_5046465_0_0_1_19_7_3',\n  'DB0-m3_o2_o12_f0_pcu.sym.46',\n  'DB0-m3_o22_o27_f0_pcu.sym.27',\n  'DB1-Cu2O8N2-fum_A-irmof10_A_No31',\n  'DB0-m3_o11_o17_f0_pcu.sym.93',\n  'DB0-m2_o8_o25_f0_pcu.sym.143',\n  'DB0-m3_o11_o24_f0_pcu.sym.17',\n  'DB0-m23_o25_o29_f0_pcu',\n  'DB0-m3_o51_o156_f0_fsc.sym.20',\n  'DB0-m2_o22_o25_f0_pcu.sym.87',\n  'DB6-cuf_10172',\n  'DB5-hypotheticalMOF_5068268_0_0_2_20_1_8',\n  'DB0-m3_o16_o19_f0_pcu.sym.9',\n  'DB5-hypotheticalMOF_5038523_0_0_1_15_9_8',\n  'DB0-m3_o24_o112_f0_fsc.sym.49',\n  'DB15-qtz-x_N65_E132_opt',\n  'DB12-HEKTAU_clean',\n  'DB13-cds-Syn036446',\n  'DB0-m2_o10_o23_f0_pcu.sym.80',\n  'DB0-m2_o1_o6_f0_pcu.sym.17',\n  'DB0-m2_o11_o24_f0_pcu.sym.79',\n  'DB12-QADBII_clean',\n  'DB0-m23_o23_o149_f0_pcu',\n  'DB12-CIYZUG_clean',\n  'DB12-QUSSII_freeONLY',\n  'DB6-cuf_9063',\n  'DB7-ddmof_4854',\n  'DB0-m3_o12_o23_f0_pcu.sym.118',\n  'DB0-m3_o24_o28_f0_pcu.sym.21',\n  'DB7-ddmof_15764',\n  'DB5-hypotheticalMOF_3629_1_1_0_24_2_10',\n  'DB0-m3_o11_o14_f0_pcu.sym.50',\n  'DB0-m3_o2_o26_f0_pcu.sym.19',\n  'DB12-NEVVAM02_clean',\n  'DB12-AWAKEQ_clean',\n  'DB5-hypotheticalMOF_5067800_0_0_2_20_7_6',\n  'DB12-WOBHEB_freeONLY',\n  'DB0-m2_o16_o25_f0_pcu.sym.72',\n  'DB0-m2_o11_o19_f0_pcu.sym.70',\n  'DB0-m2_o14_o16_f0_pcu.sym.29',\n  'DB0-m2_o10_o11_f0_pcu.sym.68',\n  'DB12-FOFTIF_clean',\n  'DB0-m2_o10_o16_f0_pcu.sym.11',\n  'DB5-hypotheticalMOF_5070951_0_0_2_3_8_5',\n  'DB0-m3_o160_o152_f0_fsc.sym.20',\n  'DB0-m2_o11_o24_f0_pcu.sym.28',\n  'DB0-m9_o2_o14_f0_sra.sym.11',\n  'DB5-hypotheticalMOF_5044964_1_0_1_3_24_6',\n  'DB0-m2_o12_o26_f0_pcu.sym.44',\n  'DB0-m3_o48_o13_f0_fsc.sym.44',\n  'DB0-m3_o530_o158_f0_fsc.sym.50',\n  'DB5-hypotheticalMOF_5045794_0_0_1_3_18_4',\n  'DB5-hypotheticalMOF_5035972_1_0_1_18_26_5',\n  'DB0-m15_o3_o105_f0_fsc',\n  'DB15-pcu_N123_E23_opt',\n  'DB6-cuf_1304',\n  'DB5-hypotheticalMOF_5047475_0_0_1_19_13_2',\n  'DB15-sxc_N65_E92_opt',\n  'DB0-m3_o109_o22_f0_fsc.sym.14',\n  'DB0-m3_o51_o22_f0_fsc.sym.60',\n  'DB6-cuf_3578',\n  'DB0-m2_o12_o12_f0_pcu.sym.9',\n  'DB0-m3_o10_o29_f0_pcu.sym.32',\n  'DB12-ETEKAQ_clean',\n  'DB7-ddmof_4653',\n  'DB1-Cu2O8N2-irmof20_A-TED_A_No5',\n  'DB15-kag_N123_E38_opt',\n  'DB12-NIYZIG_freeONLY',\n  'DB7-ddmof_23529',\n  'DB12-EVUQUI_clean',\n  'DB5-hypotheticalMOF_5074463_0_0_2_23_10_6',\n  'DB12-QUGNOV_freeONLY',\n  'DB5-hypotheticalMOF_5072772_0_0_2_19_22_5',\n  'DB0-m3_o152_o22_f0_fsc.sym.47',\n  'DB12-YOZRUC_clean',\n  'DB0-m3_o10_o27_f0_pcu.sym.49',\n  'DB5-hypotheticalMOF_5042466_0_0_1_8_8_6',\n  'DB1-Cu2O8N2-DPAC_A-fum_A_No297',\n  'DB0-m3_o147_o4_f0_fsc.sym.13',\n  'DB0-m3_o16_o26_f0_pcu.sym.85',\n  'DB0-m15_o24_o97_f0_fsc',\n  'DB0-m15_o4_o146_f0_fsc',\n  'DB0-m2_o10_o10_f0_pcu.sym.1',\n  'DB12-YEGDAR_clean',\n  'DB12-IXUQOJ_clean',\n  'DB14-lufnac05_P1_H',\n  'DB6-cuf_10514',\n  'DB5-hypotheticalMOF_5072103_0_0_2_19_10_14',\n  'DB12-YUMYUC_clean',\n  'DB12-GEGGIK_clean',\n  'DB0-m3_o8_o24_f0_pcu.sym.129',\n  'DB5-hypotheticalMOF_5038591_1_1_1_15_17_5',\n  'DB0-m2_o3_o29_f0_pcu.sym.69',\n  'DB1-Cu2O8N2-irmof20_A-irmof7_A_No67',\n  'DB0-m2_o2_o9_f0_pcu.sym.60',\n  'DB0-m2_o24_o24_f0_pcu.sym.6',\n  'DB0-m3_o24_o153_f0_fsc.sym.72',\n  'DB15-pcu_N37_E223_opt',\n  'DB0-m2_o24_o26_f0_pcu.sym.78',\n  'DB12-XEKCAT01_clean',\n  'DB0-m18_o12_o74_f0_fsc',\n  'DB5-hypotheticalMOF_35138_1_0_2_5_25_5',\n  'DB0-m2_o10_o23_f0_pcu.sym.112',\n  'DB5-hypotheticalMOF_5064705_2_1_2_15_10_6',\n  'DB12-DEZKAW_freeONLY',\n  'DB0-m9_o12_o2_f0_sra.sym.25',\n  'DB0-m3_o48_o156_f0_fsc.sym.11',\n  'DB5-hypotheticalMOF_5039752_0_0_1_15_11_14',\n  'DB0-m9_o1_o9_f0_sra.sym.24',\n  'DB12-ECEDAU_clean',\n  'DB1-Zn2O8N2-irmof16_A-irmof7_A_No222',\n  'DB0-m3_o4_o25_f0_pcu.sym.82',\n  'DB0-m2_o14_o28_f0_pcu.sym.68',\n  'DB0-m3_o22_o27_f0_pcu.sym.18',\n  'DB0-m2_o11_o17_f0_pcu.sym.101',\n  'DB0-m2_o10_o20_f0_pcu.sym.31',\n  'DB0-m3_o24_o95_f0_fsc',\n  'DB7-ddmof_22242',\n  'DB12-BETDIP_clean',\n  'DB0-m2_o12_o28_f0_pcu.sym.10',\n  'DB0-m2_o1_o23_f0_pcu.sym.79',\n  'DB6-cuf_614',\n  'DB12-RIXBIK_freeONLY',\n  'DB12-PUPXII_clean',\n  'DB15-acs_N134_E73_unopt',\n  'DB5-hypotheticalMOF_5044952_1_1_1_3_24_4',\n  'DB5-hypotheticalMOF_20861_0_0_1_10_2_10',\n  'DB0-m3_o16_o20_f0_pcu.sym.84',\n  'DB0-m3_o7_o25_f0_pcu.sym.9',\n  'DB12-BIYVEM_clean',\n  'DB0-m2_o12_o141_f0_fsc',\n  'DB0-m2_o28_o28_f0_pcu.sym.40',\n  'DB12-WEMFIF_clean',\n  'DB0-m2_o3_o16_f0_pcu.sym.4',\n  'DB5-hypotheticalMOF_3000988_1_0_1_34_34_2',\n  'DB0-m2_o14_o19_f0_pcu.sym.43',\n  'DB0-m1_o32_o1_rna',\n  'DB5-hypotheticalMOF_5074684_1_1_2_23_13_6',\n  'DB0-m3_o22_o143_f0_fsc.sym.37',\n  'DB0-m23_o25_o27_f0_pcu',\n  'DB0-m3_o22_o47_f0_fsc.sym.46',\n  'DB7-ddmof_19658',\n  'DB0-m3_o7_o21_f0_pcu.sym.80',\n  'DB5-hypotheticalMOF_5039765_0_0_1_15_11_6',\n  'DB5-hypotheticalMOF_36509_1_1_2_16_25_2',\n  'DB12-EVUPOC_clean',\n  'DB5-hypotheticalMOF_5037053_3_0_1_18_10_7',\n  'DB12-GOBXAY_clean',\n  'DB0-m9_o11_o11_f0_sra.sym.73',\n  'DB0-m2_o22_o25_f0_pcu.sym.121',\n  'DB15-pcu_N37_E6_opt',\n  'DB0-m2_o24_o26_f0_pcu.sym.25',\n  'DB0-m3_o24_o27_f0_pcu.sym.29',\n  'DB0-m3_o12_o19_f0_pcu.sym.76',\n  'DB5-hypotheticalMOF_27075_0_0_1_22_11_4',\n  'DB5-hypotheticalMOF_5038038_1_0_1_18_16_6',\n  'DB0-m3_o109_o22_f0_fsc.sym.18',\n  'DB12-HAKCIG_clean',\n  'DB12-XOVXAI_freeONLY',\n  'DB15-cds_N87_E110_opt',\n  'DB7-ddmof_21913',\n  'DB0-m2_o11_o11_f0_pcu.sym.25',\n  'DB12-ABEFUL_clean',\n  'DB12-KAVNIH_clean',\n  'DB12-JORSEQ_clean',\n  'DB0-m3_o10_o21_f0_pcu.sym.24',\n  'DB5-hypotheticalMOF_5046198_0_0_1_19_9_14',\n  'DB12-LOGBEO_freeONLY',\n  'DB0-m3_o11_o28_f0_pcu.sym.27',\n  'DB0-m3_o146_o4_f0_fsc.sym.25',\n  'DB12-ZIYQAB_clean',\n  'DB5-hypotheticalMOF_5037849_0_0_1_18_5_5',\n  'DB0-m3_o14_o26_f0_pcu.sym.19',\n  'DB12-NOHHID_freeONLY',\n  'DB12-GAQKOB_clean',\n  'DB5-hypotheticalMOF_5041659_1_0_1_8_24_4',\n  'DB5-hypotheticalMOF_5073350_0_0_2_19_8_8',\n  'DB12-GOTDIE_clean',\n  'DB0-m2_o12_o27_f0_pcu.sym.25',\n  'DB7-ddmof_21770',\n  'DB0-m2_o8_o8_f0_pcu.sym.80',\n  'DB0-m3_o24_o47_f0_fsc.sym.20',\n  'DB0-m2_o11_o21_f0_pcu.sym.102',\n  'DB12-LIBZIG_clean',\n  'DB0-m2_o22_o27_f0_pcu.sym.15',\n  'DB0-m3_o13_o21_f0_pcu.sym.21',\n  'DB0-m3_o12_o28_f0_pcu.sym.102',\n  'DB0-m2_o10_o16_f0_pcu.sym.31',\n  'DB0-m3_o12_o28_f0_pcu.sym.6',\n  'DB0-m2_o22_o27_f0_pcu.sym.35',\n  'DB0-m3_o4_o1510_f0_fsc.sym.31',\n  'DB0-m2_o10_o11_f0_pcu.sym.12',\n  'DB0-m3_o12_o24_f0_pcu.sym.12',\n  'DB12-OCUMIL_clean',\n  'DB0-m3_o19_o24_f0_nbo.sym.113',\n  'DB12-ZEZPAX01_clean',\n  'DB12-IWOKUC_freeONLY',\n  'DB0-m3_o24_o29_f0_pcu.sym.71',\n  'DB12-SONTUL_neutral_b',\n  'DB12-SIVGIP_clean',\n  'DB12-QOFLAA_freeONLY',\n  'DB12-BACGAQ_clean',\n  'DB0-m3_o10_o23_f0_pcu.sym.63',\n  'DB7-ddmof_6329',\n  'DB7-ddmof_21624',\n  'DB15-pcu_N77_E137_unopt',\n  'DB12-GAMTAQ_clean',\n  'DB5-hypotheticalMOF_5066311_0_0_2_8_17_4',\n  'DB1-ZIFZn-BDC_A-irmof6_A_No215',\n  'DB5-hypotheticalMOF_1001932_0_0_3_0_25_7',\n  'DB0-m2_o11_o17_f0_pcu.sym.116',\n  'DB5-hypotheticalMOF_5052311_1_1_1_27_13_4',\n  'DB5-hypotheticalMOF_3001004_1_0_1_34_34_3',\n  'DB5-hypotheticalMOF_5059394_2_1_2_12_21_3',\n  'DB5-hypotheticalMOF_32188_1_1_2_11_2_4',\n  'DB1-Cu2O8N2-DPAC_A-irmof14_A_No288',\n  'DB1-Cu2O8N2-irmof6_A_No10',\n  'DB12-WARFAZ_clean',\n  'DB5-hypotheticalMOF_5039183_2_0_1_15_25_3',\n  'DB7-ddmof_22195',\n  'DB12-ROGLOP_clean',\n  'DB0-m3_o4_o7_f0_pcu.sym.124',\n  'DB0-m2_o24_o25_f0_pcu.sym.15',\n  'DB12-KANMIX_clean',\n  'DB0-m3_o22_o22_f0_pcu.sym.15',\n  'DB12-BARZUR_freeONLY',\n  'DB5-hypotheticalMOF_5052106_1_0_1_27_11_8',\n  'DB0-m3_o16_o146_f0_fsc.sym.28',\n  'DB15-pcu_N139_E225_opt',\n  'DB12-GURPIT_clean',\n  'DB6-cuf_783',\n  'DB0-m2_o10_o12_f0_pcu.sym.25',\n  'DB12-KOJTOU_clean',\n  'DB0-m3_o540_o22_f0_fsc.sym.41',\n  'DB5-hypotheticalMOF_5064728_2_0_2_15_10_8',\n  'DB12-LAGNIS_clean',\n  'DB7-ddmof_3855',\n  'DB5-hypotheticalMOF_5068718_1_1_2_20_16_2',\n  'DB0-m3_o10_o20_f0_pcu.sym.34',\n  'DB12-PUQHIT_SL',\n  'DB0-m3_o24_o27_f0_pcu.sym.13',\n  'DB0-m23_o27_o58_f0_pcu',\n  'DB5-hypotheticalMOF_3001042_0_0_1_34_34_14',\n  'DB5-hypotheticalMOF_1844_2_2_0_21_2_3',\n  'DB12-REJLEY_freeONLY',\n  'DB0-m3_o11_o20_f0_pcu.sym.61',\n  'DB7-ddmof_4520',\n  'DB12-IKUTOZ_clean',\n  'DB1-Cu2O8N2-irmof6_A-irmof7_A_No14',\n  'DB5-hypotheticalMOF_5043220_0_0_1_20_24_8',\n  'DB5-hypotheticalMOF_37649_0_0_2_22_14_7',\n  'DB7-ddmof_20995',\n  'DB6-cuf_4501',\n  'DB7-ddmof_5178',\n  'DB0-m2_o11_o11_f0_pcu.sym.86',\n  'DB0-m3_o16_o25_f0_pcu',\n  'DB0-m2_o16_o16_f0_pcu.sym.47',\n  'DB1-Cu2O8N2-ADC_A-irmof6_A_No253',\n  'DB0-m3_o16_o450_f0_fsc.sym.68',\n  'DB0-m2_o24_o29_f0_pcu.sym.41',\n  'DB0-m2_o22_o25_f0_pcu.sym.1',\n  'DB12-WOPSOL_clean',\n  'DB15-pcu_N123_E42_opt',\n  'DB12-QOJTUE_clean',\n  'DB12-BOFCEG_clean',\n  'DB0-m3_o90_o22_f0_fsc.sym.21',\n  'DB0-m3_o11_o12_f0_pcu.sym.12',\n  'DB5-hypotheticalMOF_29101_1_0_2_25_7_2',\n  'DB5-hypotheticalMOF_5035932_1_0_1_18_26_2',\n  'DB5-hypotheticalMOF_5047036_2_0_1_19_24_8',\n  'DB0-m2_o10_o23_f0_pcu.sym.30',\n  'DB0-m3_o540_o14_f0_fsc.sym.29',\n  'DB7-ddmof_22030',\n  'DB12-IKUMAE_clean',\n  'DB1-Cu2O8N2-DPAC_A-irmof6_A_No354',\n  'DB0-m3_o11_o23_f0_pcu.sym.131',\n  'DB0-m9_o28_o28_f0_sra.sym.31',\n  'DB0-m3_o24_o42_f0_fsc.sym.29',\n  'DB12-ja308995t_si_002_clean',\n  'DB12-EZOBUR02_freeONLY',\n  'DB0-m3_o22_o25_f0_pcu.sym.29',\n  'DB12-FIMXUW_freeONLY',\n  'DB0-m2_o13_o24_f0_pcu.sym.23',\n  'DB0-m2_o22_o23_f0_pcu.sym.23',\n  'DB12-PORGAE_clean',\n  'DB12-EDABOC_freeONLY',\n  'DB0-m3_o13_o20_f0_pcu.sym.114',\n  'DB12-RETBEZ_clean',\n  'DB0-m3_o10_o20_f0_pcu.sym.3',\n  'DB12-CAVPIA_manual',\n  'DB0-m2_o12_o13_f0_pcu.sym.55',\n  'DB0-m3_o14_o19_f0_pcu.sym.23',\n  'DB12-GOSDID_clean',\n  'DB0-m3_o8_o9_f0_pcu.sym.114',\n  'DB1-Zn2O8N2-AZO_A-BDC_A_No487',\n  'DB0-m3_o96_o13_f0_fsc.sym.43',\n  'DB15-pcu_N65_E185_opt',\n  'DB15-bsn_N123_E146_opt',\n  'DB12-PIQMUZ_clean',\n  'DB12-GAYGAQ_clean_h',\n  'DB12-ZIGFIG_clean',\n  ...],\n 'workcap': tensor([[2.3939],\n         [1.1778],\n         [0.3650],\n         ...,\n         [1.6135],\n         [0.3594],\n         [1.4085]]),\n 'sel': tensor([[1.5499],\n         [2.3062],\n         [0.1288],\n         ...,\n         [1.1326],\n         [0.0847],\n         [0.6993]])}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "def collate_fn(batch):\n",
    "    elem = batch[0]\n",
    "    assert isinstance(elem, collections.abc.Mapping)\n",
    "\n",
    "    return {}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class RegressionCNN(torch.nn.Module):\n",
    "  def __init__(self, input_size=1, output_size=1, image_size=32, kernel_size=3):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size #(input channel e.g. RBG)\n",
    "    self.output_size = output_size #(num_classes)\n",
    "    self.image_size = image_size #(height, width)\n",
    "    self.kernel_size = kernel_size #(filter size)\n",
    "    self.model = self.build_model()\n",
    "\n",
    "  @property\n",
    "  def computeRF(self, ):\n",
    "    #(h = np.floor(H + 2p - d*(k-1) - 1) / s)\n",
    "    return np.floor(self.image_size - (self.kernel_size - 1) - 1)\n",
    "\n",
    "  def build_model(self):\n",
    "    # add convolutional layers\n",
    "    layers = [\n",
    "        torch.nn.Conv2d(self.input_size, 32, kernel_size=self.kernel_size), #input: (1,128,128) -> output: (32, 125, 125)\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=self.kernel_size), #input: (32, 125, 125) -> output: (64, 122, 122)\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "\n",
    "        #torch.nn.MaxPool2d(kernel_size=(2,2)), #input: (64, 122, 122) -> output: (64, 61, 61)\n",
    "        torch.nn.AdaptiveMaxPool2d(10), #input: (64, 122, 122)  -> output: (64, 10, 10)\n",
    "        #torch.nn.Dropout(0.10),\n",
    "\n",
    "        # add dense layers\n",
    "        torch.nn.Flatten(start_dim=1, end_dim=-1),\n",
    "        torch.nn.Linear(64*10*10, 128),\n",
    "        torch.nn.ReLU(inplace=True),\n",
    "        #torch.nn.Dropout(0.25),\n",
    "        torch.nn.Linear(128, self.output_size),\n",
    "        #torch.nn.LogSoftmax(128,self.output_size),\n",
    "        torch.nn.ReLU(inplace=True)\n",
    "        ]\n",
    "    #model = torch.nn.Sequential(*layers)\n",
    "    model = torch.nn.ModuleList(layers)\n",
    "\n",
    "    # compile the model\n",
    "    return model\n",
    "\n",
    "  def forward(self, pd=None, image=None, cif=None, workcap=None, sel=None):\n",
    "    x = image\n",
    "    for layer in self.model:\n",
    "      x = layer(x)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model = RegressionCNN(kernel_size=9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1453],\n        [0.2619],\n        [0.0278],\n        [0.0426],\n        [0.0747],\n        [0.0345],\n        [0.0845],\n        [0.0720],\n        [0.0669],\n        [0.0391],\n        [0.0506],\n        [0.0577],\n        [0.0654],\n        [0.2548],\n        [0.0556],\n        [0.1280],\n        [0.0420],\n        [0.0323],\n        [0.1588],\n        [0.0515],\n        [0.0789],\n        [0.0831],\n        [0.0629],\n        [0.0592],\n        [0.0659],\n        [0.0945],\n        [0.0676],\n        [0.0727],\n        [0.1047],\n        [0.0638],\n        [0.0652],\n        [0.0601],\n        [0.0494],\n        [0.2752],\n        [0.0631],\n        [0.0764],\n        [0.0542],\n        [0.0587],\n        [0.0648],\n        [0.1011],\n        [0.0549],\n        [0.0652],\n        [0.0937],\n        [0.1097],\n        [0.0629],\n        [0.0300],\n        [0.0791],\n        [0.0550],\n        [0.0548],\n        [0.0777],\n        [0.1531],\n        [0.0134],\n        [0.0545],\n        [0.1579],\n        [0.0654],\n        [0.0546],\n        [0.0627],\n        [0.0582],\n        [0.0833],\n        [0.0953],\n        [0.0878],\n        [0.0743],\n        [0.0874],\n        [0.1039],\n        [0.0456],\n        [0.0536],\n        [0.0433],\n        [0.0695],\n        [0.0816],\n        [0.0000],\n        [0.0735],\n        [0.0813],\n        [0.0876],\n        [0.0262],\n        [0.0789],\n        [0.1226],\n        [0.0406],\n        [0.0546],\n        [0.0703],\n        [0.0602],\n        [0.0687],\n        [0.0356],\n        [0.0559],\n        [0.0440],\n        [0.0726],\n        [0.0562],\n        [0.0470],\n        [0.0461],\n        [0.0456],\n        [0.0783],\n        [0.0565],\n        [0.0995],\n        [0.1404],\n        [0.0390],\n        [0.0679],\n        [0.0519],\n        [0.0568],\n        [0.0791],\n        [0.0503],\n        [0.1038],\n        [0.0133],\n        [0.0613],\n        [0.1936],\n        [0.0923],\n        [0.0957],\n        [0.0958],\n        [0.0943],\n        [0.0523],\n        [0.0740],\n        [0.0625],\n        [0.0653],\n        [0.0700],\n        [0.0843],\n        [0.1539],\n        [0.0670],\n        [0.0673],\n        [0.0748],\n        [0.0626],\n        [0.0592],\n        [0.1169],\n        [0.0539],\n        [0.0938],\n        [0.0679],\n        [0.0997],\n        [0.0345],\n        [0.0666],\n        [0.0171],\n        [0.0767],\n        [0.0009],\n        [0.0750],\n        [0.0000],\n        [0.0000],\n        [0.0429],\n        [0.0437],\n        [0.0659],\n        [0.0626],\n        [0.0691],\n        [0.0731],\n        [0.0642],\n        [0.0668],\n        [0.0771],\n        [0.0604],\n        [0.0679],\n        [0.0314],\n        [0.0584],\n        [0.0708],\n        [0.0594],\n        [0.0691],\n        [0.0482],\n        [0.0655],\n        [0.2175],\n        [0.0582],\n        [0.0381],\n        [0.0623],\n        [0.0455],\n        [0.0551],\n        [0.0588],\n        [0.0723],\n        [0.0617],\n        [0.0576],\n        [0.0629],\n        [0.0000],\n        [0.4028],\n        [0.0624],\n        [0.0582],\n        [0.0263],\n        [0.0977],\n        [0.1601],\n        [0.0241],\n        [0.3712],\n        [0.0528],\n        [0.0733],\n        [0.0414],\n        [0.0676],\n        [0.0139],\n        [0.0277],\n        [0.0624],\n        [0.0471],\n        [0.1010],\n        [0.0611],\n        [0.0623],\n        [0.0607],\n        [0.0295],\n        [0.0665],\n        [0.0501],\n        [0.0823],\n        [0.0665],\n        [0.0791],\n        [0.0816],\n        [0.0353],\n        [0.0299],\n        [0.0970],\n        [0.0441],\n        [0.1090],\n        [0.0804],\n        [0.0651],\n        [0.0508],\n        [0.1058],\n        [0.0714],\n        [0.0444],\n        [0.0740],\n        [0.2794],\n        [0.0707],\n        [0.0254],\n        [0.0862],\n        [0.1253],\n        [0.1161],\n        [0.0547],\n        [0.0753],\n        [0.0429],\n        [0.0655],\n        [0.0799],\n        [0.0333],\n        [0.0642],\n        [0.0396],\n        [0.0397],\n        [0.0684],\n        [0.2071],\n        [0.0384],\n        [0.0727],\n        [0.0570],\n        [0.0842],\n        [0.0486],\n        [0.0334],\n        [0.0962],\n        [0.0648],\n        [0.0581],\n        [0.1280],\n        [0.0022],\n        [0.0651],\n        [0.0657],\n        [0.0550],\n        [0.0710],\n        [0.0555],\n        [0.0526],\n        [0.0681],\n        [0.1058],\n        [0.0964],\n        [0.0503],\n        [0.0081],\n        [0.0575],\n        [0.0686],\n        [0.0851],\n        [0.0694],\n        [0.0864],\n        [0.0153],\n        [0.0519],\n        [0.0553],\n        [0.0544],\n        [0.0495],\n        [0.0000],\n        [0.0431],\n        [0.0699],\n        [0.0732],\n        [0.0623],\n        [0.0695],\n        [0.0875],\n        [0.0845],\n        [0.0310],\n        [0.1070],\n        [0.0555],\n        [0.0964],\n        [0.0347],\n        [0.0550],\n        [0.0446],\n        [0.0481],\n        [0.1003],\n        [0.1534],\n        [0.0712],\n        [0.0000],\n        [0.0063],\n        [0.0730],\n        [0.0604],\n        [0.0517],\n        [0.0541],\n        [0.0483],\n        [0.0487],\n        [0.2272],\n        [0.0649],\n        [0.1191],\n        [0.0488],\n        [0.0870],\n        [0.0867],\n        [0.0062],\n        [0.1606],\n        [0.0147],\n        [0.0000],\n        [0.0645],\n        [0.0277],\n        [0.0767],\n        [0.0438],\n        [0.0430],\n        [0.0996],\n        [0.0566],\n        [0.0742],\n        [0.0477],\n        [0.1567],\n        [0.0607],\n        [0.0681],\n        [0.0926],\n        [0.0633],\n        [0.0407],\n        [0.0939],\n        [0.0572],\n        [0.2281],\n        [0.0739],\n        [0.0723],\n        [0.0396],\n        [0.0716],\n        [0.0541],\n        [0.0683],\n        [0.0599],\n        [0.0794],\n        [0.0336],\n        [0.0762],\n        [0.0728],\n        [0.0530],\n        [0.0681],\n        [0.0519],\n        [0.0577],\n        [0.0847],\n        [0.0595],\n        [0.0538],\n        [0.0638],\n        [0.0637],\n        [0.0735],\n        [0.0745],\n        [0.0000],\n        [0.0825],\n        [0.0542],\n        [0.0457],\n        [0.0650],\n        [0.0352],\n        [0.1348],\n        [0.1744],\n        [0.0991],\n        [0.1162],\n        [0.0790],\n        [0.0557],\n        [0.0758],\n        [0.1795],\n        [0.0616],\n        [0.0871],\n        [0.0540],\n        [0.0980],\n        [0.0369],\n        [0.1185],\n        [0.0663],\n        [0.0569],\n        [0.0675],\n        [0.1339],\n        [0.0543],\n        [0.1249],\n        [0.1166],\n        [0.0535],\n        [0.0509],\n        [0.0785],\n        [0.0755],\n        [0.0579],\n        [0.0554],\n        [0.0458],\n        [0.0841],\n        [0.0649],\n        [0.0539],\n        [0.0649],\n        [0.0050],\n        [0.0507],\n        [0.0000],\n        [0.0591],\n        [0.0390],\n        [0.0923],\n        [0.0934],\n        [0.0661],\n        [0.1668],\n        [0.0725],\n        [0.1112],\n        [0.0693],\n        [0.0253],\n        [0.0547],\n        [0.1825],\n        [0.0437],\n        [0.1292],\n        [0.0432],\n        [0.0000],\n        [0.0723],\n        [0.0549],\n        [0.1462],\n        [0.0795],\n        [0.1520],\n        [0.0235],\n        [0.0491],\n        [0.1431],\n        [0.0622],\n        [0.0559],\n        [0.1722],\n        [0.0596],\n        [0.0655],\n        [0.0750],\n        [0.1025],\n        [0.0583],\n        [0.0677],\n        [0.0604],\n        [0.0688],\n        [0.0396],\n        [0.0454],\n        [0.0927],\n        [0.0864],\n        [0.0779],\n        [0.0663],\n        [0.0420],\n        [0.1015],\n        [0.0560],\n        [0.0847],\n        [0.0551],\n        [0.0000],\n        [0.0909],\n        [0.0249],\n        [0.0588],\n        [0.0956],\n        [0.1084],\n        [0.0757],\n        [0.0550],\n        [0.0356],\n        [0.0587],\n        [0.0660],\n        [0.0628],\n        [0.0817],\n        [0.0315],\n        [0.0712],\n        [0.0250],\n        [0.0931],\n        [0.0833],\n        [0.0132],\n        [0.0792],\n        [0.0407],\n        [0.0617],\n        [0.0485],\n        [0.0567],\n        [0.0590],\n        [0.1611],\n        [0.0890],\n        [0.0628],\n        [0.0964],\n        [0.0741],\n        [0.0615],\n        [0.0853],\n        [0.0000],\n        [0.0416],\n        [0.1888],\n        [0.0513],\n        [0.0760],\n        [0.0535],\n        [0.0403],\n        [0.0646],\n        [0.0498],\n        [0.0398],\n        [0.0000],\n        [0.0483],\n        [0.0646],\n        [0.0731],\n        [0.0633],\n        [0.0311],\n        [0.1379],\n        [0.1238],\n        [0.0953],\n        [0.0698],\n        [0.0607],\n        [0.0766],\n        [0.0815],\n        [0.0304],\n        [0.1270],\n        [0.0538],\n        [0.0578],\n        [0.0173],\n        [0.0600],\n        [0.0492],\n        [0.0306],\n        [0.1314],\n        [0.0628],\n        [0.0598],\n        [0.0661],\n        [0.0922],\n        [0.0542],\n        [0.0603],\n        [0.0000],\n        [0.1007],\n        [0.0612],\n        [0.0518],\n        [0.0558],\n        [0.0656],\n        [0.0577],\n        [0.0316],\n        [0.0706],\n        [0.0749],\n        [0.0517],\n        [0.0559],\n        [0.0562],\n        [0.0458],\n        [0.0508],\n        [0.0000],\n        [0.0442],\n        [0.1322],\n        [0.0571],\n        [0.0712],\n        [0.0504],\n        [0.0589],\n        [0.0862],\n        [0.1114],\n        [0.0812],\n        [0.1058],\n        [0.0477],\n        [0.0277]], grad_fn=<ReluBackward0>)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**next(iter(train_dataloader)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "MLinDir=f\"{inDir}/dataSplitML\"\n",
    "optVar=\"workcap\"\n",
    "\n",
    "if optVar==\"sel\":\n",
    "    newThresh=[-25,50]\n",
    "else:\n",
    "    newThresh=[-8,23]\n",
    "\n",
    "\n",
    "\n",
    "trainDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/trainCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=newThresh,normalize=True)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainDataset, batch_size=2048, pin_memory=True, num_workers=0, shuffle=True,collate_fn=collate_fn)\n",
    "\n",
    "testDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/testCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=newThresh,\n",
    "                          shiftStats=[(trainDataset.propStats['workcap'][0],trainDataset.propStats['workcap'][1],trainDataset.shiftFac[0]),(trainDataset.propStats['sel'][0],trainDataset.propStats['sel'][1],trainDataset.shiftFac[1])])\n",
    "\n",
    "test_dataloader= torch.utils.data.DataLoader(testDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n",
    "\n",
    "validDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/valCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=newThresh,\n",
    "                          shiftStats=[(trainDataset.propStats['workcap'][0],trainDataset.propStats['workcap'][1],trainDataset.shiftFac[0]),(trainDataset.propStats['sel'][0],trainDataset.propStats['sel'][1],trainDataset.shiftFac[1])])\n",
    "\n",
    "valid_dataloader= torch.utils.data.DataLoader(validDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 350 : loss 1.1445904970169067 \n"
     ]
    }
   ],
   "source": [
    "model = RegressionCNN(kernel_size=9)\n",
    "#optVar=\"workcap\"\n",
    "def main(args):\n",
    "    epochs = 1000\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    device = torch.device(0) #torch.cuda.current_device()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters()) #model.parameters() # extract Ws from the model\n",
    "    best_loss = np.inf\n",
    "    best_epoch=0\n",
    "    for e in range(epochs):\n",
    "      for batch in train_dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad(set_to_none=True) #dL/dWs = 0 vs None\n",
    "        image = batch['image'].to(device)\n",
    "        pred = model(image=image) #output (batch, 1)\n",
    "        y = batch[optVar].to(device) #output (batch, 1)\n",
    "        loss = loss_fn(pred, y) #scalar (Tensor)\n",
    "        loss.backward() #gradient dL/dWs\n",
    "        optimizer.step() #W <- W - lr * dL/dWs\n",
    "      val_loss = 0.\n",
    "      for batch in valid_dataloader:\n",
    "        model.eval()\n",
    "        image = batch['image'].to(device)\n",
    "        pred = model(image=image) #output (batch, 1)\n",
    "        y = batch[optVar].to(device) #output (batch, 1)\n",
    "        loss = loss_fn(pred, y) #scalar (Tensor)\n",
    "        val_loss += loss.item()\n",
    "      val_loss /= len(valid_dataloader)\n",
    "      if val_loss < best_loss:\n",
    "        torch.save({\"model\":model.state_dict()}, f\"{inDir}/tThresh0_CNN_-8_23_{optVar}_noDrop.pth\")\n",
    "        best_loss = val_loss\n",
    "        best_epoch = e\n",
    "    print(f\"At epoch {best_epoch} : loss {val_loss} \")\n",
    "    return model\n",
    "args=None\n",
    "model=main(args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "MLinDir=f\"{inDir}/dataSplitML\"\n",
    "optVar=\"workcap\"\n",
    "\n",
    "testDataset=CubePHDataset(property_excel_dir=f\"{MLinDir}/{optVar}/testCombustionDiverseMOF.xlsx\",\n",
    "                        phDF_dir=f\"{inDir}/phDF_tThresh0_B1.csv\", newThresh=[-8,23])\n",
    "test_dataloader= torch.utils.data.DataLoader(testDataset, batch_size=1024, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optVar=\"workcap\"\n",
    "device = torch.device(0)\n",
    "selckpt = torch.load(f\"{inDir}/tThresh0_CNN_-8_23_{optVar}_noDrop.pth\", map_location=device)\n",
    "model2=RegressionCNN(kernel_size=9)\n",
    "model2.load_state_dict(selckpt['model'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3232],\n",
      "        [0.3344],\n",
      "        [0.6673],\n",
      "        ...,\n",
      "        [0.6955],\n",
      "        [0.7379],\n",
      "        [1.4353]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5829],\n",
      "        [2.0133],\n",
      "        [1.2532],\n",
      "        [1.0975],\n",
      "        [1.4140],\n",
      "        [1.1568],\n",
      "        [0.1965],\n",
      "        [1.6433],\n",
      "        [1.3014],\n",
      "        [2.1104],\n",
      "        [0.5968],\n",
      "        [1.1356],\n",
      "        [1.4469],\n",
      "        [0.6128],\n",
      "        [2.1286],\n",
      "        [0.1823],\n",
      "        [0.3082],\n",
      "        [0.6195],\n",
      "        [0.9868],\n",
      "        [0.5953],\n",
      "        [0.3692],\n",
      "        [0.4519],\n",
      "        [1.3207],\n",
      "        [2.0661],\n",
      "        [1.5463],\n",
      "        [1.9954],\n",
      "        [1.9208],\n",
      "        [1.9506],\n",
      "        [1.3805],\n",
      "        [0.5793],\n",
      "        [0.9624],\n",
      "        [1.5213],\n",
      "        [0.3581],\n",
      "        [0.9019],\n",
      "        [0.4151],\n",
      "        [1.0556],\n",
      "        [1.7689],\n",
      "        [1.5101],\n",
      "        [1.7253],\n",
      "        [1.4098],\n",
      "        [0.8306],\n",
      "        [1.5128],\n",
      "        [2.0101],\n",
      "        [1.9041],\n",
      "        [1.4690],\n",
      "        [0.9738],\n",
      "        [2.2628],\n",
      "        [0.6088],\n",
      "        [0.5428],\n",
      "        [2.6883],\n",
      "        [1.1872],\n",
      "        [0.8023],\n",
      "        [0.4135],\n",
      "        [1.2281],\n",
      "        [0.7695],\n",
      "        [0.6335],\n",
      "        [1.8595],\n",
      "        [0.9960],\n",
      "        [0.4306],\n",
      "        [0.6032],\n",
      "        [0.3514],\n",
      "        [1.3037],\n",
      "        [0.3431],\n",
      "        [1.4435],\n",
      "        [2.1954],\n",
      "        [0.7289],\n",
      "        [1.4259],\n",
      "        [0.9558],\n",
      "        [1.9289],\n",
      "        [1.1036],\n",
      "        [0.7245],\n",
      "        [0.5996],\n",
      "        [2.1342],\n",
      "        [0.8003],\n",
      "        [1.5794],\n",
      "        [2.8881],\n",
      "        [0.7650],\n",
      "        [0.7711],\n",
      "        [2.8908],\n",
      "        [1.2089],\n",
      "        [1.9837],\n",
      "        [0.9355],\n",
      "        [1.0925],\n",
      "        [0.6186],\n",
      "        [1.8711],\n",
      "        [0.6747],\n",
      "        [1.1637],\n",
      "        [0.9373],\n",
      "        [0.9121],\n",
      "        [1.5307],\n",
      "        [0.8182],\n",
      "        [1.2374],\n",
      "        [2.1868],\n",
      "        [0.6637],\n",
      "        [1.1949],\n",
      "        [0.3401],\n",
      "        [0.7267],\n",
      "        [0.4918],\n",
      "        [2.5032],\n",
      "        [1.7799],\n",
      "        [2.2360],\n",
      "        [0.9163],\n",
      "        [1.0310],\n",
      "        [0.5449],\n",
      "        [1.4696],\n",
      "        [2.0615],\n",
      "        [0.6966],\n",
      "        [0.2668],\n",
      "        [0.9250],\n",
      "        [1.0108],\n",
      "        [0.7419],\n",
      "        [0.7328],\n",
      "        [1.4616],\n",
      "        [1.1504],\n",
      "        [0.2602],\n",
      "        [0.9582],\n",
      "        [0.8401],\n",
      "        [0.5818],\n",
      "        [0.2955],\n",
      "        [0.4914],\n",
      "        [2.0964],\n",
      "        [0.8799],\n",
      "        [2.1838],\n",
      "        [1.3015],\n",
      "        [0.6461],\n",
      "        [1.6691],\n",
      "        [0.2098],\n",
      "        [0.5855],\n",
      "        [2.3121],\n",
      "        [0.8909],\n",
      "        [0.6955],\n",
      "        [0.8707],\n",
      "        [1.8993],\n",
      "        [0.8187],\n",
      "        [0.7621],\n",
      "        [0.8407],\n",
      "        [0.2994],\n",
      "        [2.4283],\n",
      "        [1.5788],\n",
      "        [0.9640],\n",
      "        [2.1082],\n",
      "        [1.9624],\n",
      "        [0.6250],\n",
      "        [1.2287],\n",
      "        [1.3354],\n",
      "        [1.0110],\n",
      "        [0.6062],\n",
      "        [0.9460],\n",
      "        [0.7122],\n",
      "        [0.3547],\n",
      "        [0.8081],\n",
      "        [0.7554],\n",
      "        [0.5394],\n",
      "        [1.0577],\n",
      "        [2.6643],\n",
      "        [0.9037],\n",
      "        [1.1943],\n",
      "        [0.6754],\n",
      "        [0.4935],\n",
      "        [0.7029],\n",
      "        [0.9505],\n",
      "        [0.8306],\n",
      "        [0.5578],\n",
      "        [0.7035],\n",
      "        [0.2288],\n",
      "        [0.2303],\n",
      "        [0.7642],\n",
      "        [0.3354],\n",
      "        [0.9467],\n",
      "        [1.5138],\n",
      "        [1.2044],\n",
      "        [0.7418],\n",
      "        [1.6510],\n",
      "        [1.8590],\n",
      "        [2.3392],\n",
      "        [1.1287],\n",
      "        [0.7485],\n",
      "        [0.2165],\n",
      "        [1.0387],\n",
      "        [0.5314],\n",
      "        [0.7056],\n",
      "        [0.6334],\n",
      "        [1.2911],\n",
      "        [0.6197],\n",
      "        [0.8096],\n",
      "        [0.5730],\n",
      "        [1.0637],\n",
      "        [1.3018],\n",
      "        [0.5757],\n",
      "        [0.9009],\n",
      "        [1.3972],\n",
      "        [0.5485],\n",
      "        [0.4456],\n",
      "        [1.2984],\n",
      "        [1.8843],\n",
      "        [2.3723],\n",
      "        [0.2398],\n",
      "        [2.0599],\n",
      "        [0.2820],\n",
      "        [0.4039],\n",
      "        [0.3325],\n",
      "        [1.8356],\n",
      "        [1.8719],\n",
      "        [0.6511],\n",
      "        [1.5898],\n",
      "        [0.9386],\n",
      "        [0.6385],\n",
      "        [0.4815],\n",
      "        [1.3598],\n",
      "        [2.4320],\n",
      "        [0.9973],\n",
      "        [0.3998],\n",
      "        [0.7504],\n",
      "        [1.1867],\n",
      "        [0.8505],\n",
      "        [0.4310],\n",
      "        [1.0103],\n",
      "        [2.1709],\n",
      "        [0.1310],\n",
      "        [1.8875],\n",
      "        [1.0724],\n",
      "        [0.5546],\n",
      "        [0.4862],\n",
      "        [0.7299],\n",
      "        [0.6882],\n",
      "        [1.0252],\n",
      "        [1.8753],\n",
      "        [0.5208],\n",
      "        [1.4524],\n",
      "        [1.4239],\n",
      "        [1.2030],\n",
      "        [2.2687],\n",
      "        [1.8262],\n",
      "        [1.8652],\n",
      "        [1.1038],\n",
      "        [0.9451],\n",
      "        [1.5844],\n",
      "        [1.4832],\n",
      "        [2.0510],\n",
      "        [1.0971],\n",
      "        [0.3882],\n",
      "        [0.4652],\n",
      "        [2.8101],\n",
      "        [0.7608],\n",
      "        [1.0971],\n",
      "        [1.0395],\n",
      "        [0.3768],\n",
      "        [1.0163],\n",
      "        [0.2633],\n",
      "        [1.4539],\n",
      "        [2.6687],\n",
      "        [0.8732],\n",
      "        [2.1701],\n",
      "        [1.5921],\n",
      "        [0.2998],\n",
      "        [0.6119],\n",
      "        [0.9837],\n",
      "        [0.8422],\n",
      "        [1.2925],\n",
      "        [0.2830],\n",
      "        [1.0257],\n",
      "        [0.4661],\n",
      "        [0.1290],\n",
      "        [1.5205],\n",
      "        [0.4052],\n",
      "        [0.8354],\n",
      "        [0.8852],\n",
      "        [0.6653],\n",
      "        [0.9122],\n",
      "        [0.9556],\n",
      "        [1.3797],\n",
      "        [0.8178],\n",
      "        [0.7329],\n",
      "        [0.8739],\n",
      "        [1.2368],\n",
      "        [1.0923],\n",
      "        [0.3246],\n",
      "        [1.9820],\n",
      "        [0.9703],\n",
      "        [0.3573],\n",
      "        [1.1935],\n",
      "        [0.9977],\n",
      "        [0.2342],\n",
      "        [0.9187],\n",
      "        [0.2526],\n",
      "        [0.4140],\n",
      "        [1.7321],\n",
      "        [1.9453],\n",
      "        [0.2697],\n",
      "        [0.6410],\n",
      "        [1.0017],\n",
      "        [1.7330],\n",
      "        [0.5563],\n",
      "        [0.8563],\n",
      "        [1.8445],\n",
      "        [0.9416],\n",
      "        [0.3931],\n",
      "        [0.6920],\n",
      "        [0.6256],\n",
      "        [0.9748],\n",
      "        [0.7895],\n",
      "        [2.5574],\n",
      "        [0.7077],\n",
      "        [0.3553],\n",
      "        [1.4554],\n",
      "        [0.1098],\n",
      "        [1.3441],\n",
      "        [0.7065],\n",
      "        [1.8285],\n",
      "        [1.2355],\n",
      "        [3.1389],\n",
      "        [0.6398],\n",
      "        [0.7919],\n",
      "        [0.7998],\n",
      "        [0.3378],\n",
      "        [1.0075],\n",
      "        [0.9886],\n",
      "        [0.5484],\n",
      "        [1.1133],\n",
      "        [1.2549],\n",
      "        [1.8837],\n",
      "        [1.9065],\n",
      "        [1.2163],\n",
      "        [1.2069],\n",
      "        [0.2534],\n",
      "        [1.4171],\n",
      "        [2.0892],\n",
      "        [0.8134],\n",
      "        [1.6874],\n",
      "        [0.5194],\n",
      "        [0.3462],\n",
      "        [1.0809],\n",
      "        [1.4847],\n",
      "        [1.0609],\n",
      "        [0.8736],\n",
      "        [0.5326],\n",
      "        [1.2342],\n",
      "        [1.3198],\n",
      "        [0.6562],\n",
      "        [1.6463],\n",
      "        [0.7773],\n",
      "        [1.9010],\n",
      "        [0.4293],\n",
      "        [2.7581],\n",
      "        [0.6592],\n",
      "        [1.1711],\n",
      "        [0.7696],\n",
      "        [0.4913],\n",
      "        [1.9780],\n",
      "        [1.2100],\n",
      "        [0.7795],\n",
      "        [0.7589],\n",
      "        [0.7692],\n",
      "        [2.0476],\n",
      "        [2.2261],\n",
      "        [1.1964],\n",
      "        [0.4939],\n",
      "        [0.7088],\n",
      "        [0.5905],\n",
      "        [0.7936],\n",
      "        [1.0498],\n",
      "        [1.0928],\n",
      "        [1.0028],\n",
      "        [0.5089],\n",
      "        [0.5149],\n",
      "        [0.6514],\n",
      "        [1.0317],\n",
      "        [0.3227],\n",
      "        [0.2196],\n",
      "        [0.2817],\n",
      "        [1.4262],\n",
      "        [1.4379],\n",
      "        [1.1051],\n",
      "        [1.0079],\n",
      "        [0.5152],\n",
      "        [1.1845],\n",
      "        [1.0975],\n",
      "        [1.8718],\n",
      "        [2.4131],\n",
      "        [1.4214],\n",
      "        [1.1891],\n",
      "        [0.7538],\n",
      "        [0.6412],\n",
      "        [0.3753],\n",
      "        [0.4059],\n",
      "        [0.8491],\n",
      "        [0.8154],\n",
      "        [0.4691],\n",
      "        [1.2354],\n",
      "        [1.8584],\n",
      "        [0.7909],\n",
      "        [2.2513],\n",
      "        [1.1452],\n",
      "        [0.6082],\n",
      "        [0.3484],\n",
      "        [0.0954],\n",
      "        [0.7708],\n",
      "        [1.7391],\n",
      "        [0.5121],\n",
      "        [1.7545],\n",
      "        [0.4107],\n",
      "        [1.9494],\n",
      "        [1.9712],\n",
      "        [1.2472],\n",
      "        [2.6386],\n",
      "        [1.3891],\n",
      "        [1.5969],\n",
      "        [1.8135],\n",
      "        [0.4401],\n",
      "        [1.5125],\n",
      "        [1.2068],\n",
      "        [2.0314],\n",
      "        [1.3590],\n",
      "        [1.3526],\n",
      "        [0.5330],\n",
      "        [0.3839],\n",
      "        [0.5258],\n",
      "        [0.4388],\n",
      "        [1.0761],\n",
      "        [1.0000],\n",
      "        [1.5228],\n",
      "        [0.4431],\n",
      "        [1.3505],\n",
      "        [0.2794],\n",
      "        [1.6397],\n",
      "        [1.1922],\n",
      "        [1.6606],\n",
      "        [0.5854],\n",
      "        [1.0033],\n",
      "        [2.4431],\n",
      "        [1.2568],\n",
      "        [0.3106],\n",
      "        [1.8095],\n",
      "        [0.5807],\n",
      "        [0.5253],\n",
      "        [0.6376],\n",
      "        [0.9517],\n",
      "        [0.6072],\n",
      "        [0.1965],\n",
      "        [1.1548],\n",
      "        [1.8054],\n",
      "        [0.8076],\n",
      "        [1.2246],\n",
      "        [1.1230],\n",
      "        [1.8185],\n",
      "        [0.6096],\n",
      "        [0.8596],\n",
      "        [2.0827],\n",
      "        [1.2318],\n",
      "        [0.7642],\n",
      "        [0.7128],\n",
      "        [1.9337],\n",
      "        [0.6126],\n",
      "        [0.6253],\n",
      "        [2.1898],\n",
      "        [0.8835],\n",
      "        [0.5642],\n",
      "        [0.7645],\n",
      "        [0.2087],\n",
      "        [0.8531],\n",
      "        [0.7865],\n",
      "        [1.1285],\n",
      "        [2.4419],\n",
      "        [1.1984],\n",
      "        [1.7026],\n",
      "        [0.6287],\n",
      "        [1.0395],\n",
      "        [1.0515],\n",
      "        [1.7600],\n",
      "        [0.3386],\n",
      "        [0.4400],\n",
      "        [0.5169],\n",
      "        [1.9222],\n",
      "        [1.2974],\n",
      "        [1.3366],\n",
      "        [0.8628],\n",
      "        [0.7433],\n",
      "        [0.2866],\n",
      "        [1.4785],\n",
      "        [0.6221],\n",
      "        [1.9103],\n",
      "        [1.3129],\n",
      "        [1.8548],\n",
      "        [1.4776],\n",
      "        [2.0750],\n",
      "        [2.6466],\n",
      "        [1.4464],\n",
      "        [0.7872],\n",
      "        [1.1643],\n",
      "        [2.7967],\n",
      "        [0.6489],\n",
      "        [0.3768],\n",
      "        [0.8385],\n",
      "        [0.6034],\n",
      "        [1.0601],\n",
      "        [1.1897],\n",
      "        [0.7870],\n",
      "        [0.8461],\n",
      "        [0.6017],\n",
      "        [0.2564],\n",
      "        [1.1792],\n",
      "        [1.4522],\n",
      "        [0.7516],\n",
      "        [1.5024],\n",
      "        [0.5130],\n",
      "        [0.9387],\n",
      "        [0.7539],\n",
      "        [2.7624],\n",
      "        [0.5573],\n",
      "        [0.4317],\n",
      "        [2.7864],\n",
      "        [2.9345],\n",
      "        [2.1126],\n",
      "        [0.6141],\n",
      "        [0.6701],\n",
      "        [1.0087],\n",
      "        [0.9181],\n",
      "        [0.4801],\n",
      "        [0.8513],\n",
      "        [0.3349],\n",
      "        [1.0869],\n",
      "        [0.3363],\n",
      "        [2.2674],\n",
      "        [0.7078],\n",
      "        [0.8021],\n",
      "        [0.0000],\n",
      "        [1.4965],\n",
      "        [0.7381],\n",
      "        [1.0582],\n",
      "        [1.2428],\n",
      "        [0.3771],\n",
      "        [1.7683],\n",
      "        [0.4961],\n",
      "        [1.4277],\n",
      "        [2.3622],\n",
      "        [1.2273],\n",
      "        [0.8494],\n",
      "        [1.2791],\n",
      "        [2.0264],\n",
      "        [1.5853],\n",
      "        [1.0227],\n",
      "        [0.3530],\n",
      "        [0.8687],\n",
      "        [1.7571],\n",
      "        [0.2856],\n",
      "        [0.9909],\n",
      "        [2.0899],\n",
      "        [2.2195],\n",
      "        [0.5127],\n",
      "        [0.0000],\n",
      "        [2.7694],\n",
      "        [0.8332],\n",
      "        [1.3602],\n",
      "        [0.5461],\n",
      "        [0.2650],\n",
      "        [1.3886],\n",
      "        [0.9904],\n",
      "        [0.5148],\n",
      "        [1.4473],\n",
      "        [1.0008],\n",
      "        [1.0727],\n",
      "        [1.0131],\n",
      "        [1.0320],\n",
      "        [1.7837],\n",
      "        [1.5410],\n",
      "        [0.6723],\n",
      "        [1.2748],\n",
      "        [0.4323],\n",
      "        [1.7518],\n",
      "        [1.7770],\n",
      "        [1.3277],\n",
      "        [0.3332],\n",
      "        [0.4564],\n",
      "        [0.6055],\n",
      "        [0.4440],\n",
      "        [1.7998],\n",
      "        [2.3022],\n",
      "        [0.4479],\n",
      "        [1.8644],\n",
      "        [0.8829],\n",
      "        [0.9281],\n",
      "        [2.8346],\n",
      "        [0.8994],\n",
      "        [2.0341],\n",
      "        [0.5747],\n",
      "        [1.3139],\n",
      "        [1.1710],\n",
      "        [0.8597],\n",
      "        [0.5250],\n",
      "        [1.7708],\n",
      "        [0.6457],\n",
      "        [2.7874],\n",
      "        [0.6075],\n",
      "        [0.3922],\n",
      "        [1.2405],\n",
      "        [0.7546],\n",
      "        [2.0165],\n",
      "        [0.2839],\n",
      "        [1.1180],\n",
      "        [1.1248],\n",
      "        [0.7716],\n",
      "        [0.5961],\n",
      "        [1.4253],\n",
      "        [1.4181],\n",
      "        [0.9618],\n",
      "        [1.5364],\n",
      "        [0.3664],\n",
      "        [1.1965],\n",
      "        [0.6456],\n",
      "        [1.4290],\n",
      "        [0.9062],\n",
      "        [1.0580],\n",
      "        [1.8723],\n",
      "        [2.2632],\n",
      "        [1.7546],\n",
      "        [0.4423],\n",
      "        [1.3227],\n",
      "        [1.0127],\n",
      "        [1.8264],\n",
      "        [0.4606],\n",
      "        [1.1354],\n",
      "        [0.4561],\n",
      "        [0.6584],\n",
      "        [2.1078],\n",
      "        [1.0364],\n",
      "        [0.6098],\n",
      "        [2.5272],\n",
      "        [0.5910],\n",
      "        [1.6053],\n",
      "        [0.0000],\n",
      "        [0.5857],\n",
      "        [0.8852],\n",
      "        [0.2644],\n",
      "        [3.6523],\n",
      "        [0.3782],\n",
      "        [2.2249],\n",
      "        [0.6670],\n",
      "        [2.7053],\n",
      "        [0.7095],\n",
      "        [2.6975],\n",
      "        [1.0716],\n",
      "        [0.9561],\n",
      "        [0.5122],\n",
      "        [0.4351],\n",
      "        [0.4220],\n",
      "        [1.0229],\n",
      "        [0.1758],\n",
      "        [1.6062],\n",
      "        [0.8254],\n",
      "        [1.1274],\n",
      "        [1.6634],\n",
      "        [1.8520],\n",
      "        [0.1493],\n",
      "        [1.0817],\n",
      "        [1.1685],\n",
      "        [2.8178],\n",
      "        [0.4015],\n",
      "        [0.0000],\n",
      "        [0.7241],\n",
      "        [1.2877],\n",
      "        [0.2794],\n",
      "        [0.0675],\n",
      "        [1.0254],\n",
      "        [1.2659],\n",
      "        [0.4184],\n",
      "        [1.0346],\n",
      "        [1.0372],\n",
      "        [1.3568],\n",
      "        [0.3190],\n",
      "        [0.2463],\n",
      "        [2.2705],\n",
      "        [1.6741],\n",
      "        [0.5569],\n",
      "        [0.5382],\n",
      "        [0.0000],\n",
      "        [0.6800],\n",
      "        [1.0894],\n",
      "        [0.6352],\n",
      "        [1.1837],\n",
      "        [2.7426],\n",
      "        [0.4317],\n",
      "        [0.4881],\n",
      "        [2.9011],\n",
      "        [0.4653],\n",
      "        [1.2639],\n",
      "        [0.6307],\n",
      "        [0.4274],\n",
      "        [0.6638],\n",
      "        [0.2386],\n",
      "        [1.4349],\n",
      "        [1.5672],\n",
      "        [1.1682],\n",
      "        [1.2390],\n",
      "        [0.5427],\n",
      "        [0.9708],\n",
      "        [0.6751],\n",
      "        [0.8935],\n",
      "        [0.6830],\n",
      "        [0.4497],\n",
      "        [0.5717],\n",
      "        [1.9159],\n",
      "        [1.5607],\n",
      "        [0.6748],\n",
      "        [1.4469],\n",
      "        [0.9335],\n",
      "        [0.5808],\n",
      "        [0.3016],\n",
      "        [0.4951],\n",
      "        [0.4037],\n",
      "        [0.2836],\n",
      "        [0.4109],\n",
      "        [0.7511],\n",
      "        [0.4371],\n",
      "        [0.9893],\n",
      "        [1.6317],\n",
      "        [1.4635],\n",
      "        [2.1365],\n",
      "        [0.3229],\n",
      "        [1.6942],\n",
      "        [0.8315],\n",
      "        [0.5515],\n",
      "        [1.5502],\n",
      "        [0.9204],\n",
      "        [1.3132],\n",
      "        [0.6208],\n",
      "        [0.7828],\n",
      "        [1.3752],\n",
      "        [0.8940],\n",
      "        [1.0169],\n",
      "        [0.5889],\n",
      "        [0.3759],\n",
      "        [2.2847],\n",
      "        [1.0697],\n",
      "        [0.7978],\n",
      "        [1.5521],\n",
      "        [0.7126],\n",
      "        [0.5002],\n",
      "        [0.3822],\n",
      "        [0.9577],\n",
      "        [1.3022],\n",
      "        [1.0684],\n",
      "        [2.2595],\n",
      "        [1.3550],\n",
      "        [0.8878],\n",
      "        [0.7237],\n",
      "        [1.5229],\n",
      "        [1.0232],\n",
      "        [2.1169],\n",
      "        [0.8078],\n",
      "        [0.3178],\n",
      "        [0.7437],\n",
      "        [1.4914],\n",
      "        [0.2572],\n",
      "        [0.8643],\n",
      "        [1.5016],\n",
      "        [1.1868],\n",
      "        [0.2478],\n",
      "        [1.4814],\n",
      "        [0.9011],\n",
      "        [0.4198],\n",
      "        [0.8459],\n",
      "        [1.3299],\n",
      "        [2.0192],\n",
      "        [0.2360],\n",
      "        [1.8974],\n",
      "        [0.4882],\n",
      "        [1.5207],\n",
      "        [1.4615],\n",
      "        [2.5603],\n",
      "        [0.8167],\n",
      "        [0.4160],\n",
      "        [0.8169],\n",
      "        [0.8794],\n",
      "        [0.6107],\n",
      "        [1.3786],\n",
      "        [0.6743],\n",
      "        [0.7839],\n",
      "        [2.2077],\n",
      "        [0.7347],\n",
      "        [0.3317],\n",
      "        [0.5774],\n",
      "        [0.5225],\n",
      "        [2.1019],\n",
      "        [1.2650],\n",
      "        [0.5291],\n",
      "        [0.4110],\n",
      "        [1.1057],\n",
      "        [1.4970],\n",
      "        [0.7181],\n",
      "        [1.7202],\n",
      "        [0.8257],\n",
      "        [1.2493],\n",
      "        [2.6118],\n",
      "        [0.8767],\n",
      "        [1.3494],\n",
      "        [0.1884],\n",
      "        [0.3630],\n",
      "        [0.8022],\n",
      "        [0.6561],\n",
      "        [0.3157],\n",
      "        [0.9613],\n",
      "        [0.4573],\n",
      "        [2.0081],\n",
      "        [0.8559],\n",
      "        [0.5350],\n",
      "        [0.2485],\n",
      "        [0.4132]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(selckpt['model'])\n",
    "model2.to(device)\n",
    "for batch in test_dataloader:\n",
    "    model2.eval()\n",
    "    image = batch['image'].to(device)\n",
    "    pred=model2(image=image)\n",
    "    print(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.5477173e-01 -3.5387278e-04 -1.7893094e-01 ...  3.3440506e-01\n",
      "  5.0361812e-02  1.8266216e-01]\n",
      "-1.2699321508407593 0.9669976830482483\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.lines.Line2D at 0x261ffa6f220>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.to(device)\n",
    "losses=[]\n",
    "indices = []\n",
    "\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    model2.eval()\n",
    "    image = batch['image'].to(device)\n",
    "    pred=model2(image=image)\n",
    "    y=batch[optVar].to(device)\n",
    "    losses.append((pred-y).detach().cpu().numpy())\n",
    "    indices.append((y >= 4).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    #print(f\"TrueVal {y}\\t\\t predVal {pred}\\t\\t loss: {pred-y}\")\n",
    "indices = np.concatenate(indices, axis=0).reshape(-1)\n",
    "losses=np.concatenate(losses, axis=0).reshape(-1)\n",
    "print(losses)\n",
    "plt.hist(losses,bins=100)\n",
    "print(np.percentile(losses,5),np.percentile(losses,95))\n",
    "plt.axvline(np.percentile(losses, 5),color=\"red\")\n",
    "plt.axvline(np.percentile(losses, 95),color=\"red\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape==losses.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.5913773 , -1.7335777 , -1.745687  , -2.7537985 , -1.7446122 ,\n       -1.12219   , -0.96230483, -1.521143  , -1.6458395 , -2.8156705 ,\n       -2.5878696 , -2.2201188 , -3.9775953 , -3.2923198 , -3.400732  ,\n       -1.441184  , -2.242352  , -1.688174  , -4.475149  , -2.3090086 ,\n       -2.3313124 ], dtype=float32)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[indices]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241m.\u001B[39mdataset\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader.dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1]' is invalid for input of size 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model(\u001B[43mtrainDataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36mCubePHDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcif\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcifs[index],\n\u001B[0;32m     64\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mphImg[index],\n\u001B[1;32m---> 65\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworkcap\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproperty\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     66\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msel\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproperty[index][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m,),\n\u001B[0;32m     67\u001B[0m             }\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[1]' is invalid for input of size 2"
     ]
    }
   ],
   "source": [
    "model(trainDataset[0]['image'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [91]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     13\u001B[0m image \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m#y=batch['workcap'].to(device)\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m testPred[j\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworkcap\u001B[39m\u001B[38;5;124m'\u001B[39m]):(j\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworkcap\u001B[39m\u001B[38;5;124m'\u001B[39m])]\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mhstack(np\u001B[38;5;241m.\u001B[39marray(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworkcap\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()),\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     16\u001B[0m j\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mTypeError\u001B[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "selckpt = torch.load(f\"{inDir}/tThresh0_CNN_-8_23_{optVar}.pth\")# map_location=device)\n",
    "\n",
    "model = RegressionCNN(kernel_size=9)\n",
    "model=model.load_state_dict(selckpt['model'])\n",
    "device=torch.cuda.current_device()\n",
    "\n",
    "\n",
    "testPred=np.zeros((len(testDataset),2),dtype=np.float32)\n",
    "j: int =0\n",
    "#model.to(device)\n",
    "for batch in test_dataloader:\n",
    "    model.eval()\n",
    "    image = batch['image'].to(device)\n",
    "    #y=batch['workcap'].to(device)\n",
    "    testPred[j*len(batch['workcap']):(j+1)*len(batch['workcap'])]=np.hstack(np.array(batch['workcap'].numpy()),(model(image=image)).numpy())\n",
    "    j+=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_IncompatibleKeys' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [108]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m model\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mload_state_dict(selckpt[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      7\u001B[0m device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mcurrent_device()\n\u001B[1;32m----> 8\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m(device)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m test_dataloader:\n\u001B[0;32m     10\u001B[0m     image\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_IncompatibleKeys' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "selckpt = torch.load(f\"{inDir}/tThresh0_CNN_-8_23_{optVar}.pth\")# map_location=device)\n",
    "\n",
    "model = RegressionCNN(kernel_size=9)\n",
    "model=model.load_state_dict(selckpt['model'])\n",
    "\n",
    "\n",
    "device=torch.cuda.current_device()\n",
    "model.to(device)\n",
    "for batch in test_dataloader:\n",
    "    image=batch[\"image\"].to(device)\n",
    "    pred=model(image=image)\n",
    "    display(torch.column_stack((batch[optVar].to(device),pred)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_IncompatibleKeys' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [114]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: '_IncompatibleKeys' object is not callable"
     ]
    }
   ],
   "source": [
    "model.to"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "test_dataloader= torch.utils.data.DataLoader(testDataset, batch_size=2, pin_memory=True, num_workers=0, shuffle=False,collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [102]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m image\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      7\u001B[0m testPred[j,\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32(batch[optVar])\n\u001B[1;32m----> 8\u001B[0m testPred[j,\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      9\u001B[0m j\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [82]\u001B[0m, in \u001B[0;36mRegressionCNN.forward\u001B[1;34m(self, pd, image, cif, workcap, sel)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m image\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m---> 44\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "testPred=np.zeros((len(testDataset),2),dtype=np.float32)\n",
    "j=0\n",
    "for batch in test_dataloader:\n",
    "    #print()\n",
    "    image=batch[\"image\"].to(device)\n",
    "\n",
    "    testPred[j,0]=np.float32(batch[optVar])\n",
    "    testPred[j,1]=np.float32(model(image))\n",
    "    j+=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[2.66994 , 0.      ],\n       [0.147288, 1.      ],\n       [1.143492, 2.      ],\n       [1.259065, 3.      ],\n       [2.320613, 4.      ],\n       [1.196761, 5.      ],\n       [1.843505, 6.      ],\n       [0.949611, 7.      ],\n       [2.302493, 8.      ],\n       [0.6985  , 9.      ]], dtype=float32)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPred[:10,1]=np.arange(10)\n",
    "testPred[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [105]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m test_dataloader:\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mfloat\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m))\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [82]\u001B[0m, in \u001B[0;36mRegressionCNN.forward\u001B[1;34m(self, pd, image, cif, workcap, sel)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m image\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m---> 44\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    print(float(model(batch['image'].to(device))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [100]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m test_dataloader:\n\u001B[0;32m      2\u001B[0m     image\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [82]\u001B[0m, in \u001B[0;36mRegressionCNN.forward\u001B[1;34m(self, pd, image, cif, workcap, sel)\u001B[0m\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m image\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n\u001B[1;32m---> 44\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (NoneType, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!NoneType!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    image=batch[\"image\"].to(device)\n",
    "    print(model(image))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
